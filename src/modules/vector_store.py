"""
ë²¡í„° ì €ì¥ì†Œ ëª¨ë“ˆ
BGE-m3 ê¸°ë°˜ Qdrant í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Dense + Sparse)
"""

from typing import List, Dict, Any, Optional
import uuid
import json
import os
import re
from pathlib import Path
import numpy as np

from qdrant_client import QdrantClient
from qdrant_client import AsyncQdrantClient
from qdrant_client.models import (
    Distance, VectorParams, SparseVectorParams, SparseIndexParams, Filter, FieldCondition, 
    MatchValue, Query, NamedSparseVector, Prefetch, SparseVector as QdrantSparseVector,
    PointStruct, FusionQuery, Fusion
)
from FlagEmbedding import BGEM3FlagModel

from src.utils.logger import get_logger
from src.utils.config import get_qdrant_config, get_embedding_config
from src.modules.document_processor import DocumentChunk
from src.modules.langchain_retrievers import LangChainRetrievalManager
from src.modules.kiwipiepy_preprocessor import KiwipiepyPreprocessor


class QdrantVectorStore:
    """BGE-m3 ê¸°ë°˜ Qdrant í•˜ì´ë¸Œë¦¬ë“œ ë²¡í„° ì €ì¥ì†Œ (Dense + Sparse)"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, bge_model: Optional[BGEM3FlagModel] = None):
        """
        Args:
            config: Qdrant ì„¤ì •
            bge_model: ê¸°ì¡´ BGE-m3 ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì , ì¤‘ë³µ ë¡œë“œ ë°©ì§€ìš©)
        """
        self.logger = get_logger()
        
        if config is None:
            config = get_qdrant_config()
        
        self.collection_name = config.collection_name
        self.vector_size = config.vector_size  # BGE-m3ëŠ” 1024ì°¨ì›
        self.distance_metric = Distance.COSINE if config.distance_metric.lower() == 'cosine' else Distance.EUCLIDEAN
        self.storage_path = config.storage_path
        self.use_local_storage = config.use_local_storage
        
        # Sparse ë²¡í„° ì„¤ì •
        self.sparse_enabled = getattr(config, 'sparse_enabled', True)
        self.sparse_vector_name = getattr(config, 'sparse_vector_name', 'sparse')
        self.dense_vector_name = 'dense'  # BGE-m3 dense ë²¡í„° ì´ë¦„
        self.hybrid_search_dense_weight = getattr(config, 'hybrid_search_dense_weight', 0.7)
        self.hybrid_search_sparse_weight = getattr(config, 'hybrid_search_sparse_weight', 0.3)
        # Pydantic ëª¨ë¸ì—ì„œëŠ” ì§ì ‘ ì†ì„± ì ‘ê·¼ (getattrëŠ” ì†ì„±ì´ ì—†ì„ ë•Œë§Œ ê¸°ë³¸ê°’ ì‚¬ìš©)
        self.use_kiwipiepy_preprocessing = config.sparse_use_kiwipiepy if hasattr(config, 'sparse_use_kiwipiepy') else True
        self.kiwipiepy_dictionary_path = config.kiwipiepy_dictionary_path if hasattr(config, 'kiwipiepy_dictionary_path') else None
        self.logger.info(f"Kiwipiepy ì„¤ì • ë¡œë“œ: use_kiwipiepy_preprocessing={self.use_kiwipiepy_preprocessing}, dictionary_path={self.kiwipiepy_dictionary_path}")
        
        # KIWIPIEPY_AVAILABLE ìƒíƒœ í™•ì¸
        from src.modules.kiwipiepy_preprocessor import KIWIPIEPY_AVAILABLE
        self.logger.info(f"Kiwipiepy ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: KIWIPIEPY_AVAILABLE={KIWIPIEPY_AVAILABLE}")
        
        self.kiwipiepy_preprocessor: Optional[KiwipiepyPreprocessor] = None
        if self.use_kiwipiepy_preprocessing:
            try:
                self.kiwipiepy_preprocessor = KiwipiepyPreprocessor(
                    use_kiwipiepy=True,
                    dictionary_path=self.kiwipiepy_dictionary_path,
                )
                self.logger.info(f"KiwipiepyPreprocessor ìƒì„± ì™„ë£Œ: use_kiwipiepy={self.kiwipiepy_preprocessor.use_kiwipiepy}, kiwi={self.kiwipiepy_preprocessor.kiwi is not None}")
                if not self.kiwipiepy_preprocessor.use_kiwipiepy:
                    self.logger.warning("Kiwipiepy ì „ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤. (Kiwipiepyê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šê±°ë‚˜ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤)")
                    self.use_kiwipiepy_preprocessing = False
                else:
                    self.logger.info("Kiwipiepy í˜•íƒœì†Œ ì „ì²˜ë¦¬ê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as exc:
                self.logger.warning(f"Kiwipiepy ì „ì²˜ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {exc}")
                import traceback
                self.logger.error(f"Kiwipiepy ì´ˆê¸°í™” ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                self.use_kiwipiepy_preprocessing = False
        
        # ê²€ìƒ‰ ê¸°ë³¸ê°’ ì„¤ì •
        self.default_limit = config.default_limit
        self.max_scroll_limit = config.max_scroll_limit
        self.connection_timeout = config.connection_timeout
        self.request_timeout = config.request_timeout
        
        # ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ì‚¬ìš©
        if self.use_local_storage:
            from pathlib import Path
            storage_path = Path(self.storage_path)
            storage_path.mkdir(parents=True, exist_ok=True)
            
            self.client = QdrantClient(path=str(storage_path))
            # ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ëŠ” ì§€ì—° ì´ˆê¸°í™” (ë¡œì»¬ ì €ì¥ì†Œ ë™ì‹œ ì ‘ê·¼ ë¬¸ì œ ë°©ì§€)
            self.async_client = None
            self._async_client_path = str(storage_path)
            self._async_client_host = None
            self._async_client_port = None
            self.logger.info(f"Qdrant ë¡œì»¬ ì €ì¥ì†Œ ì´ˆê¸°í™”: {storage_path}")
        else:
            self.client = QdrantClient(host=config.host, port=config.port)
            # ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ëŠ” ì§€ì—° ì´ˆê¸°í™”
            self.async_client = None
            self._async_client_path = None
            self._async_client_host = config.host
            self._async_client_port = config.port
            self.logger.info(f"Qdrant ì„œë²„ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”: {config.host}:{config.port}")
        
        # BGE-m3 ëª¨ë¸ ì´ˆê¸°í™”
        if bge_model is not None:
            self.bge_model = bge_model
            self.logger.info("ê¸°ì¡´ BGE-m3 ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš© (ì¤‘ë³µ ë¡œë“œ ë°©ì§€)")
        else:
            from src.utils.config import get_embedding_config
            embedding_config = get_embedding_config()
            
            # BGE-m3 ëª¨ë¸ ê²½ë¡œ í™•ì¸
            model_path = embedding_config.model_path or embedding_config.name
            if not model_path:
                raise ValueError(
                    "BGE-m3 ëª¨ë¸ ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                    "config.yamlì˜ model.embedding.model_pathë¥¼ BGE-m3 ëª¨ë¸ ê²½ë¡œë¡œ ì„¤ì •í•˜ì„¸ìš”. "
                    "ì˜ˆ: C:\\Users\\a003219048\\Desktop\\models\\BGE-m3-ko"
                )
            
            # ëª¨ë¸ ê²½ë¡œ ì¡´ì¬ í™•ì¸
            if not os.path.exists(model_path):
                self.logger.warning(f"BGE-m3 ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
                self.logger.warning("ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê±°ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                self.logger.warning("BGE-m3 ëª¨ë¸ì€ FlagEmbedding ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ìë™ ë‹¤ìš´ë¡œë“œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                # ëª¨ë¸ ë””ë ‰í† ë¦¬ ë‚´ë¶€ íŒŒì¼ í™•ì¸ (config.json, tokenizer ë“±)
                config_file = os.path.join(model_path, "config.json")
                if not os.path.exists(config_file):
                    self.logger.warning(f"BGE-m3 ëª¨ë¸ ë””ë ‰í† ë¦¬ ë‚´ config.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
                    self.logger.warning("ëª¨ë¸ì´ ì™„ì „íˆ ë‹¤ìš´ë¡œë“œë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                else:
                    self.logger.info(f"BGE-m3 ëª¨ë¸ ê²½ë¡œ ê²€ì¦ ì™„ë£Œ: {model_path}")
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            use_fp16 = True
            try:
                import torch
                if not torch.cuda.is_available():
                    use_fp16 = False
                    self.logger.info("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ FP16ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            except ImportError:
                use_fp16 = False
                self.logger.info("PyTorchë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ FP16ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            
            self.logger.info(f"BGE-m3 ëª¨ë¸ ì´ˆê¸°í™” ì¤‘: {model_path} (FP16: {use_fp16})")
            try:
                # transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ê³  ì–µì œ (XLMRobertaTokenizerFast ê´€ë ¨)
                import warnings
                with warnings.catch_warnings():
                    warnings.filterwarnings("ignore", message=".*XLMRobertaTokenizerFast.*")
                    warnings.filterwarnings("ignore", message=".*fast tokenizer.*")
                    # transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ê²½ê³ ë„ ì–µì œ
                    import transformers
                    transformers.logging.set_verbosity_error()
                    
                    self.bge_model = BGEM3FlagModel(model_path, use_fp16=use_fp16)
                
                self.logger.info("BGE-m3 ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.error(f"BGE-m3 ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                raise
    
    def _get_async_client(self):
        """ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ì§€ì—° ì´ˆê¸°í™” (ë¡œì»¬ ì €ì¥ì†Œ ë™ì‹œ ì ‘ê·¼ ë¬¸ì œ ë°©ì§€)"""
        if self.async_client is None:
            if self.use_local_storage:
                # ë¡œì»¬ ì €ì¥ì†Œì˜ ê²½ìš° AsyncQdrantLocalì€ ë™ê¸° í´ë¼ì´ì–¸íŠ¸ì™€ ë™ì‹œ ì ‘ê·¼ ë¶ˆê°€
                # ë”°ë¼ì„œ Noneì„ ë°˜í™˜í•˜ì—¬ asyncio.to_thread ì‚¬ìš©ì„ ìœ ë„
                return None
            else:
                # ì„œë²„ ëª¨ë“œì˜ ê²½ìš° ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                self.async_client = AsyncQdrantClient(host=self._async_client_host, port=self._async_client_port)
        return self.async_client
    
    def _check_connection(self) -> bool:
        """ì—°ê²° ìƒíƒœ í™•ì¸"""
        try:
            collections = self.client.get_collections()
            self.logger.debug(f"Qdrant ì—°ê²° í™•ì¸: {len(collections.collections)}ê°œ ì»¬ë ‰ì…˜")
            return True
        except Exception as e:
            self.logger.error(f"Qdrant ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _get_metadata(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í—¬í¼ ë©”ì„œë“œ (ì¤‘ë³µ ì œê±°)"""
        if 'metadata' in payload:
            return payload['metadata']
        return payload
    
    def create_collection(self, force_recreate: bool = False) -> bool:
        """ì»¬ë ‰ì…˜ ìƒì„± (BGE-m3 ê¸°ë°˜ Dense + Sparse ë²¡í„°)"""
        try:
            self.logger.info(f"ì»¬ë ‰ì…˜ ìƒì„± ì‹œì‘: {self.collection_name}")
            
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸
            self.logger.info("ê¸°ì¡´ ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ ì¤‘...")
            collections = self.client.get_collections()
            self.logger.info(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ ìˆ˜: {len(collections.collections)}")
            
            collection_exists = any(col.name == self.collection_name for col in collections.collections)
            self.logger.info(f"ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€: {collection_exists}")
            
            if collection_exists:
                if force_recreate:
                    self.logger.info(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ: {self.collection_name}")
                    self.client.delete_collection(self.collection_name)
                else:
                    self.logger.info(f"ì»¬ë ‰ì…˜ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {self.collection_name}")
                    return True
            
            # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
            self.logger.info(f"ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì¤‘: {self.collection_name}, ë²¡í„° í¬ê¸°: {self.vector_size}")
            
            # Dense ë²¡í„° ì„¤ì • (BGE-m3ëŠ” 1024ì°¨ì›)
            vectors_config = {
                self.dense_vector_name: VectorParams(
                    size=self.vector_size,  # 1024
                    distance=self.distance_metric
                )
            }
            
            # Sparse ë²¡í„° ì„¤ì • (sparse_enabledì¼ ë•Œë§Œ)
            sparse_vectors_config = None
            if self.sparse_enabled:
                sparse_vectors_config = {
                    self.sparse_vector_name: SparseVectorParams(
                        index=SparseIndexParams()
                    )
                }
                self.logger.info(f"Sparse ë²¡í„° ì„¤ì • ì¶”ê°€: {self.sparse_vector_name}")
            
            # ì»¬ë ‰ì…˜ ìƒì„±
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=vectors_config,
                sparse_vectors_config=sparse_vectors_config
            )
            
            self.logger.info(f"ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ: {self.collection_name} (Dense: {self.dense_vector_name}, Sparse: {self.sparse_vector_name if self.sparse_enabled else 'N/A'})")
            
            return True
            
        except Exception as e:
            self.logger.error(f"ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False
    
    def add_documents(self, documents: List[DocumentChunk], force_update: bool = False) -> bool:
        """ë¬¸ì„œ ì¶”ê°€ (BGE-m3ë¡œ dense+sparse ì„ë² ë”© ìƒì„± í›„ Qdrantì— ì§ì ‘ ì €ì¥)"""
        if not self._check_connection():
            return False
        
        try:
            # ì¤‘ë³µ ì²­í¬ ë°©ì§€
            seen_chunks = set()
            valid_documents = []
            
            for doc in documents:
                # source_file ê²€ì¦
                if not doc.source_file or doc.source_file.strip() == '':
                    self.logger.warning(f"source_fileì´ ë¹„ì–´ìˆëŠ” ì²­í¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤. chunk_id: {doc.chunk_id}")
                    continue
                
                # ì²­í¬ ê³ ìœ  ì‹ë³„ì ìƒì„± (íŒŒì¼ëª… + ì²­í¬ ì¸ë±ìŠ¤)
                chunk_key = f"{doc.source_file}:{doc.chunk_index}"
                    
                if chunk_key in seen_chunks:
                    self.logger.warning(f"ì¤‘ë³µ ì²­í¬ ê±´ë„ˆë›°ê¸°: {chunk_key}")
                    continue
                
                seen_chunks.add(chunk_key)
                valid_documents.append(doc)
            
            if not valid_documents:
                self.logger.warning("ì¶”ê°€í•  ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # BGE-m3ë¡œ ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
            self.logger.info(f"BGE-m3ë¡œ dense + sparse ì„ë² ë”© ìƒì„± ì¤‘: {len(valid_documents)}ê°œ ë¬¸ì„œ")
            texts = [doc.content for doc in valid_documents]
            # ì „ì²˜ë¦¬ ì—†ì´ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            embedding_texts = texts
            
            # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            batch_size = getattr(self, '_bge_batch_size', 32)
            all_embeddings = {'dense': [], 'lexical_weights': []}
            
            for i in range(0, len(texts), batch_size):
                batch_texts = embedding_texts[i:i+batch_size]
                batch_embeddings = self.bge_model.encode(
                    batch_texts,
                    return_dense=True,
                    return_sparse=True,
                    return_colbert_vecs=False
                )
                
                # ë°˜í™˜ í‚¤ í™•ì¸ ë° ì²˜ë¦¬
                if 'dense_vecs' in batch_embeddings:
                    dense_key = 'dense_vecs'
                elif 'dense' in batch_embeddings:
                    dense_key = 'dense'
                else:
                    self.logger.error(f"Available keys: {batch_embeddings.keys()}")
                    raise KeyError("Cannot find dense embeddings in BGE-m3 output")
                
                all_embeddings['dense'].extend(batch_embeddings[dense_key])
                
                # lexical_weights í™•ì¸ ë° ë¡œê¹…
                batch_lexical_weights = batch_embeddings.get('lexical_weights', [])
                if not batch_lexical_weights:
                    self.logger.warning(f"ë°°ì¹˜ {i//batch_size + 1}ì—ì„œ lexical_weightsê°€ ì—†ìŠµë‹ˆë‹¤. BGE-m3 ë°˜í™˜ê°’ í‚¤: {list(batch_embeddings.keys())}")
                else:
                    # ë¹ˆ sparse ë²¡í„° ê°œìˆ˜ í™•ì¸
                    empty_count = sum(1 for w in batch_lexical_weights if not w or (isinstance(w, dict) and len(w) == 0))
                    if empty_count > 0:
                        self.logger.warning(f"ë°°ì¹˜ {i//batch_size + 1}ì—ì„œ {empty_count}/{len(batch_lexical_weights)}ê°œ ì²­í¬ì˜ sparse ë²¡í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                
                all_embeddings['lexical_weights'].extend(batch_lexical_weights)
                self.logger.debug(f"  ì§„í–‰: {min(i+batch_size, len(texts))}/{len(texts)}")
            
            self.logger.info("BGE-m3 ì„ë² ë”© ìƒì„± ì™„ë£Œ")
            
            # Qdrantì— ì—…ë¡œë“œ
            self.logger.info("ë¬¸ì„œë¥¼ Qdrantì— ì—…ë¡œë“œ ì¤‘...")
            points = []
            
            for idx, (doc, dense_vec, sparse_dict) in enumerate(zip(valid_documents, all_embeddings['dense'], all_embeddings['lexical_weights'])):
                # Dense ë²¡í„° ë³€í™˜
                if hasattr(dense_vec, 'tolist'):
                    dense_vector = dense_vec.tolist()
                elif isinstance(dense_vec, np.ndarray):
                    dense_vector = dense_vec.tolist()
                else:
                    dense_vector = list(dense_vec)
                
                # Sparse ë²¡í„° ë³€í™˜ (lexical_weightsëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)
                sparse_vector = None
                if self.sparse_enabled:
                    if sparse_dict:
                        if isinstance(sparse_dict, dict):
                            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ: {token_id: weight, ...}
                            indices = list(sparse_dict.keys())
                            values = list(sparse_dict.values())
                        else:
                            # ë‹¤ë¥¸ í˜•íƒœì¸ ê²½ìš° ì²˜ë¦¬
                            self.logger.warning(f"ì²­í¬ {doc.chunk_id} ì˜ˆìƒì¹˜ ëª»í•œ sparse_dict í˜•íƒœ: {type(sparse_dict)}")
                            indices = []
                            values = []
                        
                        if indices and values:
                            sparse_vector = QdrantSparseVector(
                                indices=indices,
                                values=values
                            )
                            self.logger.debug(f"ì²­í¬ {doc.chunk_id} sparse ë²¡í„° ìƒì„±: {len(indices)}ê°œ í† í°")
                        else:
                            # ë¹ˆ sparse ë²¡í„°ëŠ” ì •ìƒì¼ ìˆ˜ ìˆìŒ (ì§§ì€ í…ìŠ¤íŠ¸, íŠ¹ìˆ˜ ë¬¸ìë§Œ ìˆëŠ” ê²½ìš° ë“±)
                            self.logger.debug(f"ì²­í¬ {doc.chunk_id} sparse ë²¡í„°ê°€ ë¹„ì–´ìˆìŒ: indices={len(indices) if indices else 0}, values={len(values) if values else 0}, sparse_dict íƒ€ì…={type(sparse_dict)}")
                    else:
                        # sparse_dictê°€ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš° (ì •ìƒì¼ ìˆ˜ ìˆìŒ)
                        self.logger.debug(f"ì²­í¬ {doc.chunk_id} sparse_dictê°€ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆìŒ (sparse_enabled={self.sparse_enabled})")
                
                # PointStruct ìƒì„±
                vector_dict = {
                    self.dense_vector_name: dense_vector
                }
                
                if sparse_vector:
                    vector_dict[self.sparse_vector_name] = sparse_vector
                    self.logger.debug(f"ì²­í¬ {doc.chunk_id} vector_dictì— sparse ë²¡í„° ì¶”ê°€: {list(vector_dict.keys())}")
                else:
                    # sparse ë²¡í„°ê°€ ì—†ëŠ” ê²½ìš°ëŠ” ì •ìƒì¼ ìˆ˜ ìˆìŒ (ì§§ì€ í…ìŠ¤íŠ¸ ë“±)
                    # ê²½ê³  ëŒ€ì‹  DEBUG ë ˆë²¨ë¡œ ë³€ê²½í•˜ì—¬ ë¡œê·¸ ë…¸ì´ì¦ˆ ê°ì†Œ
                    self.logger.debug(f"ì²­í¬ {doc.chunk_id} sparse ë²¡í„°ê°€ ì—†ì–´ vector_dictì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ì •ìƒì¼ ìˆ˜ ìˆìŒ)")
                
                # Point IDë¥¼ UUID ë¬¸ìì—´ë¡œ ë³€í™˜ (Qdrant ë¡œì»¬ ì €ì¥ì†ŒëŠ” UUID ë¬¸ìì—´ì„ ìš”êµ¬)
                # chunk_idë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¼ê´€ëœ UUID ìƒì„± (uuid5 ì‚¬ìš©)
                if doc.chunk_id:
                    # chunk_idë¥¼ ê¸°ë°˜ìœ¼ë¡œ UUID ìƒì„± (ì¼ê´€ì„± ìœ ì§€)
                    point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.chunk_id))
                else:
                    # chunk_idê°€ ì—†ìœ¼ë©´ ìƒˆ UUID ìƒì„±
                    point_id = str(uuid.uuid4())
                
                point = PointStruct(
                    id=point_id,
                    vector=vector_dict,
                    payload={
                        'page_content': doc.content,
                        'source_file': doc.source_file,
                        'chunk_id': doc.chunk_id,  # ì›ë³¸ chunk_idëŠ” payloadì— ì €ì¥
                        'chunk_index': doc.chunk_index,
                        'doc_id': doc.doc_id,
                        'section_id': doc.section_id,
                        'chunk_type': doc.chunk_type,
                        'heading_path': doc.heading_path,
                        'page_start': doc.page_start,
                        'page_end': doc.page_end,
                        'language': doc.language,
                        'domain': doc.domain,
                        'embedding_version': doc.embedding_version,
                        'document': doc.doc_metadata.to_dict() if doc.doc_metadata else {},
                        'metadata': {
                            **doc.metadata,
                            'chunk_id': doc.chunk_id,
                            'chunk_index': doc.chunk_index,
                            'doc_id': doc.doc_id,
                            'section_id': doc.section_id,
                            'chunk_type': doc.chunk_type,
                            'heading_path': doc.heading_path,
                            'page_start': doc.page_start,
                            'page_end': doc.page_end,
                            'language': doc.language,
                            'domain': doc.domain,
                            'embedding_version': doc.embedding_version,
                            'document': doc.doc_metadata.to_dict() if doc.doc_metadata else {},
                        },
                        **doc.metadata,
                    }
                )
                points.append(point)
            
            # ë°°ì¹˜ë¡œ ì—…ë¡œë“œ
            upload_batch_size = 100
            for i in range(0, len(points), upload_batch_size):
                self.client.upsert(
                    collection_name=self.collection_name,
                    points=points[i:i+upload_batch_size]
                )
                self.logger.debug(f"  ì—…ë¡œë“œ: {min(i+upload_batch_size, len(points))}/{len(points)}")
            
            self.logger.info(f"ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ: {len(valid_documents)}ê°œ (ì¤‘ë³µ ì œê±°: {len(documents) - len(valid_documents)}ê°œ)")
            if self.sparse_enabled:
                self.logger.info("Dense + Sparse ë²¡í„°ê°€ í•¨ê»˜ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
            return True
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False
    
    def replace_document_vectors(self, file_path: str, new_chunks: List[DocumentChunk]) -> bool:
        """íŠ¹ì • íŒŒì¼ì˜ ë²¡í„°ë¥¼ ì™„ì „íˆ êµì²´"""
        try:
            self.logger.info(f"íŒŒì¼ ë²¡í„° êµì²´ ì‹œì‘: {file_path}")
            
            # 1. ê¸°ì¡´ ë²¡í„° ì‚­ì œ
            delete_success = self._delete_document_vectors(file_path)
            if not delete_success:
                self.logger.warning(f"ê¸°ì¡´ ë²¡í„° ì‚­ì œ ì‹¤íŒ¨, ìƒˆ ë²¡í„°ë§Œ ì¶”ê°€: {file_path}")
            
            # 2. ìƒˆ ë²¡í„° ì¶”ê°€
            add_success = self.add_documents(new_chunks, force_update=False)
            
            if add_success:
                self.logger.info(f"íŒŒì¼ ë²¡í„° êµì²´ ì™„ë£Œ: {file_path}, ì²­í¬ ìˆ˜: {len(new_chunks)}")
                return True
            else:
                self.logger.error(f"íŒŒì¼ ë²¡í„° êµì²´ ì‹¤íŒ¨: {file_path}")
                return False
                
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ë²¡í„° êµì²´ ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {str(e)}")
            return False
    
    def _delete_document_vectors(self, file_path: str) -> tuple:
        """
        íŠ¹ì • íŒŒì¼ì˜ ëª¨ë“  ë²¡í„° ì‚­ì œ
        
        Args:
            file_path: ì‚­ì œí•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            (ì„±ê³µ ì—¬ë¶€, ì‚­ì œëœ ì²­í¬ ìˆ˜) íŠœí”Œ
        """
        try:
            # ì‚­ì œ ì „ ì²­í¬ ìˆ˜ í™•ì¸
            scroll_result = self.client.scroll(
                collection_name=self.collection_name,
                limit=10000,  # ì¶©ë¶„íˆ í° ìˆ˜
                scroll_filter=Filter(
                    must=[
                        FieldCondition(
                            key="source_file",
                            match=MatchValue(value=file_path)
                        )
                    ]
                ),
                with_payload=False,
                with_vectors=False
            )
            
            points_to_delete = scroll_result[0]
            chunk_count = len(points_to_delete)
            
            if chunk_count == 0:
                self.logger.warning(f"ì‚­ì œí•  í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                return True, 0
            
            # Qdrantì—ì„œ í•´ë‹¹ íŒŒì¼ì˜ ëª¨ë“  í¬ì¸íŠ¸ ì‚­ì œ
            # source_file í•„ë“œë¡œ í•„í„°ë§ (payloadì— ì €ì¥ëœ í•„ë“œ)
            result = self.client.delete(
                collection_name=self.collection_name,
                points_selector=Filter(
                    must=[
                        FieldCondition(
                            key="source_file",
                            match=MatchValue(value=file_path)
                        )
                    ]
                )
            )
            
            self.logger.info(f"íŒŒì¼ ë²¡í„° ì‚­ì œ ì™„ë£Œ: {file_path}, {chunk_count}ê°œ ì²­í¬ ì‚­ì œë¨")
            return True, chunk_count
            
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ë²¡í„° ì‚­ì œ ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False, 0
    
    # ========== ë¹„ë™ê¸° ë©”ì„œë“œ (Phase 2: ë²¡í„° ê²€ìƒ‰ ë¹„ë™ê¸°í™”) ==========
    
    async def search_with_table_filter_async(self, 
                                            query: str, 
                                            table_title: Optional[str] = None,
                                            is_table_data: Optional[bool] = None,
                                            limit: Optional[int] = None,
                                            score_threshold: Optional[float] = None,
                                            dense_weight: Optional[float] = None,
                                            sparse_weight: Optional[float] = None,
                                            keywords: Optional[List[str]] = None,
                                            entities: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """ë¹„ë™ê¸° í‘œ ê´€ë ¨ í•„í„°ì™€ í•¨ê»˜ ê²€ìƒ‰ (BGE-m3 ê¸°ë°˜ Prefetch + RRF ë˜ëŠ” ê°€ì¤‘ì¹˜ ê¸°ë°˜)"""
        if not self._check_connection():
            return []
        
        # ê¸°ë³¸ê°’ ì ìš©
        limit = limit if limit is not None else self.default_limit
        if score_threshold is None:
            from src.utils.config import get_rag_config
            rag_config = get_rag_config()
            score_threshold = rag_config.score_threshold
        
        # í•„í„° ì¡°ê±´ êµ¬ì„±
        filter_conditions = None
        if table_title or is_table_data is not None:
            must_conditions = []
            
            if table_title:
                must_conditions.append({
                    "key": "table_title",
                    "match": {"value": table_title}
                })
            
            if is_table_data is not None:
                must_conditions.append({
                    "key": "is_table_data",
                    "match": {"value": is_table_data}
                })
            
            if must_conditions:
                filter_conditions = {"must": must_conditions}
        
        # ê°€ì¤‘ì¹˜ê°€ ì œê³µë˜ë©´ ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš©
        if dense_weight is not None or sparse_weight is not None:
            # ê¸°ë³¸ê°’ ì ìš©
            from src.utils.config import get_qdrant_config
            qdrant_config = get_qdrant_config()
            effective_dense_weight = dense_weight if dense_weight is not None else getattr(qdrant_config, 'hybrid_search_dense_weight', 0.7)
            effective_sparse_weight = sparse_weight if sparse_weight is not None else getattr(qdrant_config, 'hybrid_search_sparse_weight', 0.3)
            
            self.logger.info(f"ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ ì‚¬ìš© (í‘œ í•„í„° í¬í•¨): dense={effective_dense_weight:.2f}, sparse={effective_sparse_weight:.2f}")
            
            # ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í˜¸ì¶œ
            docs_with_scores = await self._hybrid_search_with_weights(
                query=query,
                limit=limit,
                filter_conditions=filter_conditions,
                dense_weight=effective_dense_weight,
                sparse_weight=effective_sparse_weight,
                keywords=keywords,
                entities=entities
            )
            
            # Document í˜•ì‹ì—ì„œ Dict í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            results = []
            for doc, score in docs_with_scores:
                # score_threshold í•„í„°ë§
                if score_threshold is not None and score < score_threshold:
                    continue
                
                metadata = doc.metadata
                results.append({
                    'content': doc.page_content,
                    'score': score,
                    'metadata': metadata,
                    'source_file': metadata.get('source_file', ''),
                    'chunk_index': metadata.get('chunk_index', 0),
                    'chunk_id': metadata.get('chunk_id', ''),
                    'table_title': metadata.get('table_title', ''),
                    'is_table_data': metadata.get('is_table_data', False)
                })
            
            return results
        
        # ê°€ì¤‘ì¹˜ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ RRF ë°©ì‹ ì‚¬ìš©
        try:
            # í•„í„° ì¡°ê±´ êµ¬ì„± (RRF ë°©ì‹ìš©)
            qdrant_filter = None
            if table_title or is_table_data is not None:
                must_conditions = []
                
                if table_title:
                    must_conditions.append(
                        FieldCondition(key="table_title", match=MatchValue(value=table_title))
                    )
                
                if is_table_data is not None:
                    must_conditions.append(
                        FieldCondition(key="is_table_data", match=MatchValue(value=is_table_data))
                    )
                
                if must_conditions:
                    qdrant_filter = Filter(must=must_conditions)
            
            self.logger.info(f"=== BGE-m3 ë¹„ë™ê¸° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘ ===")
            # ì „ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ì¿¼ë¦¬ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            query_for_embedding = query
            self.logger.info(f"ì¿¼ë¦¬: {query[:100]}...")
            self.logger.info(f"Sparse ë²¡í„° í™œì„±í™”: {self.sparse_enabled}")
            
            # BGE-m3ë¡œ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ë¹„ë™ê¸°)
            import asyncio
            query_embeddings = await asyncio.to_thread(
                self.bge_model.encode,
                [query_for_embedding],
                return_dense=True,
                return_sparse=True,
                return_colbert_vecs=False
            )
            
            # í‚¤ ì´ë¦„ í™•ì¸
            dense_key = 'dense_vecs' if 'dense_vecs' in query_embeddings else 'dense'
            dense_vec = query_embeddings[dense_key][0]
            sparse_weights = query_embeddings['lexical_weights'][0]
            
            # Dense ë²¡í„° ë³€í™˜
            if hasattr(dense_vec, 'tolist'):
                dense_vector = dense_vec.tolist()
            elif isinstance(dense_vec, np.ndarray):
                dense_vector = dense_vec.tolist()
            else:
                dense_vector = list(dense_vec)
            
            # Sparse ë²¡í„° ë³€í™˜
            sparse_vector = None
            if self.sparse_enabled and sparse_weights:
                if isinstance(sparse_weights, dict):
                    indices = list(sparse_weights.keys())
                    values = list(sparse_weights.values())
                else:
                    self.logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ sparse_weights í˜•íƒœ: {type(sparse_weights)}")
                    indices = []
                    values = []
                
                if indices and values:
                    sparse_vector = QdrantSparseVector(
                        indices=indices,
                        values=values
                    )
            
            # Prefetch + RRF í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë¹„ë™ê¸°)
            if self.sparse_enabled and sparse_vector:
                self.logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ëª¨ë“œ: Dense + Sparse ë²¡í„° (RRF)")
                
                prefetch_list = [
                    Prefetch(
                        query=dense_vector,
                        using=self.dense_vector_name,
                        limit=limit * 2,
                        filter=qdrant_filter
                    ),
                    Prefetch(
                        query=sparse_vector,
                        using=self.sparse_vector_name,
                        limit=limit * 2,
                        filter=qdrant_filter
                    )
                ]
                
                if self.use_local_storage:
                    results = await asyncio.to_thread(
                        self.client.query_points,
                        collection_name=self.collection_name,
                        prefetch=prefetch_list,
                        query=FusionQuery(fusion=Fusion.RRF),
                        limit=limit,
                        with_payload=True,
                        with_vectors=False
                    )
                else:
                    if not hasattr(self, '_async_client') or self._async_client is None:
                        self._async_client = AsyncQdrantClient(host=self._async_client_host, port=self._async_client_port)
                    results = await self._async_client.query_points(
                        collection_name=self.collection_name,
                        prefetch=prefetch_list,
                        query=FusionQuery(fusion=Fusion.RRF),
                        limit=limit,
                        with_payload=True,
                        with_vectors=False
                    )
            else:
                # Denseë§Œ ì‚¬ìš©
                self.logger.info(f"â„¹ï¸  Dense ë²¡í„°ë§Œ ì‚¬ìš©")
                if self.use_local_storage:
                    results = await asyncio.to_thread(
                        self.client.query_points,
                        collection_name=self.collection_name,
                        query=dense_vector,
                        using=self.dense_vector_name,
                        limit=limit,
                        query_filter=qdrant_filter,
                        with_payload=True,
                        with_vectors=False
                    )
                else:
                    if not hasattr(self, '_async_client') or self._async_client is None:
                        self._async_client = AsyncQdrantClient(host=self._async_client_host, port=self._async_client_port)
                    results = await self._async_client.query_points(
                        collection_name=self.collection_name,
                        query=dense_vector,
                        using=self.dense_vector_name,
                        limit=limit,
                        query_filter=qdrant_filter,
                        with_payload=True,
                        with_vectors=False
                    )
            
            # ê²°ê³¼ ë³€í™˜
            search_results = []
            for point in results.points:
                payload = point.payload or {}
                
                # ì ìˆ˜ ì²˜ë¦¬ (RRF ì ìˆ˜ëŠ” ì´ë¯¸ ìœ ì‚¬ë„ í˜•íƒœ)
                score = point.score if hasattr(point, 'score') else 0.0
                similarity_score = float(score)
                
                # ì ìˆ˜ ë²”ìœ„ ê²€ì¦ (0-1 ë²”ìœ„ í™•ì¸)
                if similarity_score < 0.0 or similarity_score > 1.0:
                    self.logger.warning(
                        f"ì ìˆ˜ê°€ 0-1 ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨: {similarity_score:.4f}. "
                        f"ìë™ í´ë¦¬í•‘ ì ìš©"
                    )
                    similarity_score = max(0.0, min(1.0, similarity_score))
                
                # source_file ì¶”ì¶œ
                source_file = (
                    payload.get('source_file') or 
                    payload.get('file_path') or 
                    payload.get('file_name') or 
                    ''
                )
                
                # chunk_index ì¶”ì¶œ
                chunk_index = payload.get('chunk_index', 0)
                try:
                    chunk_index = int(chunk_index) if chunk_index is not None else 0
                except (ValueError, TypeError):
                    chunk_index = 0
                
                # chunk_id ì¶”ì¶œ
                chunk_id = payload.get('chunk_id', '')
                
                # chunk_indexê°€ ì—†ìœ¼ë©´ chunk_idì—ì„œ ì¶”ì¶œ ì‹œë„
                if chunk_index == 0 and chunk_id:
                    match = re.search(r'_(\d+)$', chunk_id)
                    if match:
                        try:
                            chunk_index = int(match.group(1))
                        except (ValueError, TypeError):
                            chunk_index = 0
                
                # content ì¶”ì¶œ
                content = payload.get('page_content', '') or payload.get('content', '')
                
                search_results.append({
                    'content': content,
                    'score': similarity_score,
                    'metadata': payload,
                    'source_file': source_file,
                    'chunk_index': chunk_index,
                    'table_title': payload.get('table_title', ''),
                    'is_table_data': payload.get('is_table_data', False)
                })
            
            # ì ìˆ˜ ì„ê³„ê°’ í•„í„°ë§
            if score_threshold > 0:
                before_filter_count = len(search_results)
                search_results = [r for r in search_results if r['score'] >= score_threshold]
                filtered_out = before_filter_count - len(search_results)
                if filtered_out > 0:
                    self.logger.info(
                        f"ë¹„ë™ê¸° ì ìˆ˜ ì„ê³„ê°’({score_threshold:.3f}) í•„í„°ë§: "
                        f"{before_filter_count}ê°œ â†’ {len(search_results)}ê°œ (ì œì™¸: {filtered_out}ê°œ)"
                    )
            
            self.logger.info(f"=== BGE-m3 ë¹„ë™ê¸° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ ===")
            self.logger.info(f"ìµœì¢… ê²°ê³¼: {len(search_results)}ê°œ (ì ìˆ˜ ì„ê³„ê°’ í•„í„°ë§ í›„)")
            if self.sparse_enabled and sparse_vector:
                self.logger.info(f"âœ… RRF í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: Dense + Sparse ë²¡í„° í†µí•© ê²°ê³¼")
            
            return search_results
            
        except Exception as e:
            self.logger.error(f"ë¹„ë™ê¸° í•„í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return []
    
    async def search_similar_async(self, 
                                  query: str, 
                                  limit: Optional[int] = None,
                                  score_threshold: Optional[float] = None,
                                  filter_conditions: Optional[Dict[str, Any]] = None,
                                  dense_weight: Optional[float] = None,
                                  sparse_weight: Optional[float] = None,
                                  keywords: Optional[List[str]] = None,
                                  entities: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """
        ë¹„ë™ê¸° ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (search_with_table_filter_asyncì˜ ë˜í¼)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            limit: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            score_threshold: ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’
            filter_conditions: í•„í„° ì¡°ê±´ (í˜„ì¬ ë¯¸ì‚¬ìš©, í˜¸í™˜ì„± ìœ ì§€)
            dense_weight: Dense ë²¡í„° ê°€ì¤‘ì¹˜ (Noneì´ë©´ config ê¸°ë³¸ê°’ ì‚¬ìš©)
            sparse_weight: Sparse ë²¡í„° ê°€ì¤‘ì¹˜ (Noneì´ë©´ config ê¸°ë³¸ê°’ ì‚¬ìš©)
            keywords: query_refinerì—ì„œ ì¶”ì¶œí•œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            entities: query_refinerì—ì„œ ì¶”ì¶œí•œ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
        """
        # search_with_table_filter_asyncì— ëª¨ë“  ë¡œì§ ìœ„ì„ (ê°€ì¤‘ì¹˜ ì²´í¬ í¬í•¨)
        return await self.search_with_table_filter_async(
            query=query,
            table_title=None,
            is_table_data=None,
            limit=limit,
            score_threshold=score_threshold,
            dense_weight=dense_weight,
            sparse_weight=sparse_weight,
            keywords=keywords,
            entities=entities
        )
    
    
    def _distance_to_similarity(self, distance: float, vector_type: str = 'dense') -> float:
        """
        ê±°ë¦¬(distance)ë¥¼ ìœ ì‚¬ë„(similarity)ë¡œ ë³€í™˜
        
        Args:
            distance: Qdrantì—ì„œ ë°˜í™˜í•œ ê±°ë¦¬ ê°’
            vector_type: ë²¡í„° íƒ€ì… ('dense' ë˜ëŠ” 'sparse')
            
        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ (0.0 ~ 1.0)
        """
        if distance is None or distance < 0:
            return 0.0
        
        # Sparse ë²¡í„°ëŠ” BM25 ê¸°ë°˜ì´ë¯€ë¡œ ì´ë¯¸ ìœ ì‚¬ë„ ì ìˆ˜ì¼ ìˆ˜ ìˆìŒ
        # í•˜ì§€ë§Œ QdrantëŠ” ê±°ë¦¬ë¡œ ë°˜í™˜í•˜ë¯€ë¡œ ë³€í™˜ í•„ìš”
        if vector_type == 'sparse':
            # Sparse ë²¡í„°ëŠ” ë³´í†µ BM25 ì ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, Qdrantì—ì„œëŠ” ê±°ë¦¬ë¡œ ë°˜í™˜
            # ê±°ë¦¬ ê°’ì´ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬í•˜ë¯€ë¡œ, COSINEê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
            # ë‹¨, Sparse ë²¡í„°ì˜ ê²½ìš° ì ìˆ˜ ë²”ìœ„ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜
            if self.distance_metric == Distance.COSINE:
                similarity = 1.0 - (distance / 2.0)
            else:
                # ê¸°ë³¸ê°’: COSINEìœ¼ë¡œ ì²˜ë¦¬
                similarity = 1.0 - (distance / 2.0)
        else:
            # Dense ë²¡í„°: Distance metricì— ë”°ë¼ ë³€í™˜
            if self.distance_metric == Distance.COSINE:
                # COSINE distance: 0(ê°™ìŒ) ~ 2(ë‹¤ë¦„)
                # ìœ ì‚¬ë„: 1 - (distance / 2)
                similarity = 1.0 - (distance / 2.0)
            elif self.distance_metric == Distance.EUCLIDEAN:
                # EUCLIDEAN distance: 0(ê°™ìŒ) ~ ë¬´í•œëŒ€(ë‹¤ë¦„)
                # ìœ ì‚¬ë„: 1 / (1 + distance)
                similarity = 1.0 / (1.0 + distance)
            else:
                # ê¸°ë³¸ê°’: COSINEìœ¼ë¡œ ì²˜ë¦¬
                similarity = 1.0 - (distance / 2.0)
        
        # 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
        similarity = max(0.0, min(1.0, similarity))
        
        return similarity
    
    async def _hybrid_search_with_weights(self, 
                                         query: str, 
                                         limit: int,
                                         filter_conditions: Optional[Dict[str, Any]] = None,
                                         dense_weight: float = 0.7,
                                         sparse_weight: float = 0.3,
                                         keywords: Optional[List[str]] = None,
                                         entities: Optional[List[str]] = None) -> List[tuple]:
        """
        BGE-m3 ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Prefetch + ìˆ˜ë™ ê°€ì¤‘ì¹˜ ê²°í•©)
        
        Returns:
            List[tuple]: (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        import asyncio
        from langchain_core.documents import Document
        
        try:
            # ì „ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ì¿¼ë¦¬ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            query_for_embedding = query
            # BGE-m3ë¡œ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ë¹„ë™ê¸°)
            query_embeddings = await asyncio.to_thread(
                self.bge_model.encode,
                [query_for_embedding],
                return_dense=True,
                return_sparse=True,
                return_colbert_vecs=False
            )
            
            # í‚¤ ì´ë¦„ í™•ì¸
            dense_key = 'dense_vecs' if 'dense_vecs' in query_embeddings else 'dense'
            dense_vec = query_embeddings[dense_key][0]
            sparse_weights = query_embeddings['lexical_weights'][0]
            
            # Dense ë²¡í„° ë³€í™˜
            if hasattr(dense_vec, 'tolist'):
                dense_vector = dense_vec.tolist()
            elif isinstance(dense_vec, np.ndarray):
                dense_vector = dense_vec.tolist()
            else:
                dense_vector = list(dense_vec)
            
            # Sparse ë²¡í„° ë³€í™˜
            sparse_vector = None
            if self.sparse_enabled and sparse_weights:
                if isinstance(sparse_weights, dict):
                    indices = list(sparse_weights.keys())
                    values = list(sparse_weights.values())
                else:
                    self.logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ sparse_weights í˜•íƒœ: {type(sparse_weights)}")
                    indices = []
                    values = []
                
                if indices and values:
                    sparse_vector = QdrantSparseVector(
                        indices=indices,
                        values=values
                    )
            
            # í•„í„° êµ¬ì„±
            qdrant_filter = None
            if filter_conditions:
                must_conditions = []
                for condition in filter_conditions.get("must", []):
                    key = condition.get("key")
                    match_value = condition.get("match", {}).get("value")
                    if key and match_value is not None:
                        must_conditions.append(
                            FieldCondition(key=key, match=MatchValue(value=match_value))
                        )
                if must_conditions:
                    qdrant_filter = Filter(must=must_conditions)
            
            self.logger.info(f"ğŸ”„ ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (Dense={dense_weight}, Sparse={sparse_weight})")
            
            # Denseì™€ Sparse ê²€ìƒ‰ ê°ê° ìˆ˜í–‰ (ê°€ì¤‘ì¹˜ ê²°í•©ì„ ìœ„í•´)
            dense_results = None
            sparse_results = None
            
            # Dense ë²¡í„° ê²€ìƒ‰
            if dense_vector:
                if self.use_local_storage:
                    dense_results = await asyncio.to_thread(
                        self.client.query_points,
                        collection_name=self.collection_name,
                        query=dense_vector,
                        using=self.dense_vector_name,
                        limit=limit * 2,
                        query_filter=qdrant_filter,
                        with_payload=True,
                        with_vectors=False
                    )
                else:
                    if not hasattr(self, '_async_client') or self._async_client is None:
                        self._async_client = AsyncQdrantClient(host=self._async_client_host, port=self._async_client_port)
                    dense_results = await self._async_client.query_points(
                        collection_name=self.collection_name,
                        query=dense_vector,
                        using=self.dense_vector_name,
                        limit=limit * 2,
                        query_filter=qdrant_filter,
                        with_payload=True,
                        with_vectors=False
                    )
            
            # Sparse ë²¡í„° ê²€ìƒ‰
            if sparse_vector:
                if self.use_local_storage:
                    sparse_results = await asyncio.to_thread(
                        self.client.query_points,
                        collection_name=self.collection_name,
                        query=sparse_vector,
                        using=self.sparse_vector_name,
                        limit=limit * 2,
                        query_filter=qdrant_filter,
                        with_payload=True,
                        with_vectors=False
                    )
                else:
                    if not hasattr(self, '_async_client') or self._async_client is None:
                        self._async_client = AsyncQdrantClient(host=self._async_client_host, port=self._async_client_port)
                    sparse_results = await self._async_client.query_points(
                        collection_name=self.collection_name,
                        query=sparse_vector,
                        using=self.sparse_vector_name,
                        limit=limit * 2,
                        query_filter=qdrant_filter,
                        with_payload=True,
                        with_vectors=False
                    )
            
            # ê°€ì¤‘ì¹˜ë¡œ ê²°ê³¼ ê²°í•©
            combined_results = {}
            
            # Dense ê²°ê³¼ ì²˜ë¦¬ (ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜)
            if dense_results and dense_results.points:
                for rank, point in enumerate(dense_results.points, 1):
                    point_id = str(point.id)
                    raw_distance = point.score if hasattr(point, 'score') else 0.0
                    # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                    dense_similarity = self._distance_to_similarity(raw_distance, 'dense')
                    
                    if point_id not in combined_results:
                        combined_results[point_id] = {
                            'point': point,
                            'dense_score': dense_similarity,
                            'dense_rank': rank,
                            'dense_distance': raw_distance,
                            'sparse_score': 0.0,
                            'sparse_rank': None,
                            'sparse_distance': 0.0,
                            'combined_score': 0.0
                        }
                    else:
                        combined_results[point_id]['dense_score'] = dense_similarity
                        combined_results[point_id]['dense_rank'] = rank
                        combined_results[point_id]['dense_distance'] = raw_distance
            
            # Sparse ê²°ê³¼ ì²˜ë¦¬ (ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜)
            if sparse_results and sparse_results.points:
                for rank, point in enumerate(sparse_results.points, 1):
                    point_id = str(point.id)
                    raw_distance = point.score if hasattr(point, 'score') else 0.0
                    # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                    sparse_similarity = self._distance_to_similarity(raw_distance, 'sparse')
                    
                    if point_id not in combined_results:
                        combined_results[point_id] = {
                            'point': point,
                            'dense_score': 0.0,
                            'dense_rank': None,
                            'dense_distance': 0.0,
                            'sparse_score': sparse_similarity,
                            'sparse_rank': rank,
                            'sparse_distance': raw_distance,
                            'combined_score': 0.0
                        }
                    else:
                        combined_results[point_id]['sparse_score'] = sparse_similarity
                        combined_results[point_id]['sparse_rank'] = rank
                        combined_results[point_id]['sparse_distance'] = raw_distance
            
            # ê°€ì¤‘ì¹˜ë¡œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
            for point_id, result in combined_results.items():
                dense_score = result['dense_score']
                sparse_score = result['sparse_score']
                
                # ê°€ì¤‘ì¹˜ ê²°í•©
                if dense_score > 0 and sparse_score > 0:
                    combined_score = (dense_score * dense_weight) + (sparse_score * sparse_weight)
                elif dense_score > 0:
                    combined_score = dense_score * dense_weight / (dense_weight + sparse_weight) if (dense_weight + sparse_weight) > 0 else dense_score
                elif sparse_score > 0:
                    combined_score = sparse_score * sparse_weight / (dense_weight + sparse_weight) if (dense_weight + sparse_weight) > 0 else sparse_score
                else:
                    combined_score = 0.0
                
                combined_score = max(0.0, min(1.0, combined_score))
                result['combined_score'] = combined_score
            
            # ìµœì¢… ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            sorted_results = sorted(
                combined_results.values(),
                key=lambda x: x['combined_score'],
                reverse=True
            )[:limit]
            
            # Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            docs = []
            for result in sorted_results:
                point = result['point']
                payload = point.payload or {}
                
                page_content = payload.get('page_content', '') or payload.get('content', '')
                source_file = (
                    payload.get('source_file') or 
                    payload.get('file_path') or 
                    payload.get('file_name') or 
                    ''
                )
                chunk_index = payload.get('chunk_index', 0)
                try:
                    chunk_index = int(chunk_index) if chunk_index is not None else 0
                except (ValueError, TypeError):
                    chunk_index = 0
                chunk_id = payload.get('chunk_id', '')
                
                if chunk_index == 0 and chunk_id:
                    match = re.search(r'_(\d+)$', chunk_id)
                    if match:
                        try:
                            chunk_index = int(match.group(1))
                        except (ValueError, TypeError):
                            chunk_index = 0
                
                doc_metadata = {
                    'chunk_id': chunk_id,
                    'source_file': source_file,
                    'chunk_index': chunk_index,
                    **{k: v for k, v in payload.items() if k not in ['page_content', 'content', 'chunk_id', 'source_file', 'chunk_index']}
                }
                
                doc = Document(
                    page_content=page_content,
                    metadata=doc_metadata
                )
                docs.append((doc, result['combined_score']))
            
            self.logger.info(f"âœ… ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(docs)}ê°œ ê²°ê³¼")
            self.logger.info(f"   - Dense ê°€ì¤‘ì¹˜: {dense_weight}, Sparse ê°€ì¤‘ì¹˜: {sparse_weight}")
            return docs
            
        except Exception as e:
            self.logger.error(f"ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return []
    
    def get_documents_info(self) -> List[Dict[str, Any]]:
        """ì €ì¥ëœ ë¬¸ì„œë“¤ì˜ ì •ë³´ ë°˜í™˜"""
        try:
            # ëª¨ë“  í¬ì¸íŠ¸ ì¡°íšŒ (ë©”íƒ€ë°ì´í„°ë§Œ)
            points = self.client.scroll(
                collection_name=self.collection_name,
                limit=self.max_scroll_limit,
                with_payload=True,
                with_vectors=False
            )[0]
            
            # ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”
            documents = {}
            for point in points:
                payload = point.payload
                
                # source_file ì¶”ì¶œ (payload ìµœìƒìœ„ ë ˆë²¨ ë˜ëŠ” metadata ë‚´ë¶€)
                # ì‹¤ì œ ì €ì¥ ì‹œ: payload['source_file']ì— ì§ì ‘ ì €ì¥ë¨
                source_file = (payload.get('source_file') or 
                             payload.get('metadata', {}).get('source_file') or
                             payload.get('source') or 
                             payload.get('file_path') or 
                             payload.get('file_name') or 
                             '')
                
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
                metadata = self._get_metadata(payload)
                
                # source_fileì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸° (unknown ë¬¸ì„œ ì œì™¸)
                if not source_file or source_file.strip() == '':
                    continue
                
                # íŒŒì¼ëª… ì¶”ì¶œ (ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ)
                file_name = metadata.get('file_name', '')
                if not file_name and source_file != 'unknown':
                    # source_fileì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
                    file_name = os.path.basename(source_file)
                
                if source_file not in documents:
                    documents[source_file] = {
                        'source_file': source_file,
                        'file_name': file_name,  # íŒŒì¼ëª… ì¶”ê°€
                        'total_chunks': 0,
                        'first_chunk_index': float('inf'),
                        'last_chunk_index': -1,
                        'file_type': metadata.get('file_type', ''),
                        'file_size': metadata.get('file_size', 0),
                        'upload_time': metadata.get('upload_time', ''),
                        'chunk_ids': []
                    }
                
                documents[source_file]['total_chunks'] += 1
                chunk_index = metadata.get('chunk_index', 0)
                documents[source_file]['first_chunk_index'] = min(
                    documents[source_file]['first_chunk_index'], 
                    chunk_index
                )
                documents[source_file]['last_chunk_index'] = max(
                    documents[source_file]['last_chunk_index'], 
                    chunk_index
                )
                documents[source_file]['chunk_ids'].append(point.id)
            
            # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            result = list(documents.values())
            self.logger.info(f"ë¬¸ì„œ ì •ë³´ ì¡°íšŒ ì™„ë£Œ: {len(result)}ê°œ ë¬¸ì„œ")
            return result
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def get_document_chunks(self, document_id: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ë¬¸ì„œì˜ ì²­í¬ ì •ë³´ ë°˜í™˜"""
        try:
            # ì…ë ¥ ê²€ì¦
            if not document_id or document_id.strip() == '':
                self.logger.warning("get_document_chunks: document_idê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return []
            
            if document_id == 'N/A':
                self.logger.warning("get_document_chunks: 'N/A'ëŠ” ìœ íš¨í•œ ë¬¸ì„œ IDê°€ ì•„ë‹™ë‹ˆë‹¤.")
                return []
            
            # ëª¨ë“  ì²­í¬ë¥¼ ê°€ì ¸ì˜¨ í›„ Pythonì—ì„œ í•„í„°ë§
            points = self.client.scroll(
                collection_name=self.collection_name,
                limit=self.max_scroll_limit,
                with_payload=True,
                with_vectors=False
            )[0]
            
            # íŠ¹ì • ë¬¸ì„œì˜ ì²­í¬ë“¤ë§Œ í•„í„°ë§
            filtered_points = []
            for point in points:
                metadata = self._get_metadata(point.payload)
                
                # source_fileì´ ì—¬ëŸ¬ ìœ„ì¹˜ì— ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª¨ë‘ í™•ì¸
                source_file = (
                    metadata.get('source_file') or 
                    metadata.get('file_path') or 
                    metadata.get('file_name') or 
                    ''
                )
                
                if source_file == document_id:
                    filtered_points.append(point)
            
            points = filtered_points
            
            chunks = []
            for point in points:
                payload = point.payload
                
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
                metadata = self._get_metadata(payload)
                
                chunks.append({
                    'chunk_id': point.id,
                    'chunk_index': metadata.get('chunk_index', 0),
                    'content_preview': payload.get('page_content', '')[:200] + '...',
                    'content_full': payload.get('page_content', ''),  # ì „ì²´ ë‚´ìš© ì¶”ê°€
                    'content_length': len(payload.get('page_content', '')),
                    'metadata': metadata
                })
            
            # ì²­í¬ ì¸ë±ìŠ¤ ìˆœìœ¼ë¡œ ì •ë ¬
            chunks.sort(key=lambda x: x['chunk_index'])
            
            self.logger.info(f"ë¬¸ì„œ ì²­í¬ ì¡°íšŒ ì™„ë£Œ: {document_id}, {len(chunks)}ê°œ ì²­í¬")
            return chunks
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ì²­í¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _get_all_documents_from_qdrant(self) -> List[Any]:
        """
        Qdrantì—ì„œ ëª¨ë“  ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¤ê¸° (LangChain Document í˜•ì‹)
        
        Returns:
            ëª¨ë“  ë¬¸ì„œì˜ LangChain Document ë¦¬ìŠ¤íŠ¸
        """
        from langchain_core.documents import Document
        
        try:
            if not self._check_connection():
                return []
            
            all_documents = []
            offset = None
            
            # Scrollì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  í¬ì¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
            while True:
                scroll_result = self.client.scroll(
                    collection_name=self.collection_name,
                    limit=self.max_scroll_limit,
                    offset=offset,
                    with_payload=True,
                    with_vectors=False  # ë²¡í„°ëŠ” í•„ìš” ì—†ìŒ
                )
                
                points, next_offset = scroll_result
                
                if not points:
                    break
                
                # Pointë¥¼ LangChain Documentë¡œ ë³€í™˜
                for point in points:
                    payload = point.payload or {}
                    page_content = payload.get('page_content', '')
                    
                    if page_content:  # ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                        doc = Document(
                            page_content=page_content,
                            metadata={
                                'chunk_id': payload.get('chunk_id', ''),
                                'source_file': payload.get('source_file', ''),
                                'chunk_index': payload.get('chunk_index', 0),
                                **{k: v for k, v in payload.items() 
                                   if k not in ['page_content', 'chunk_id', 'source_file', 'chunk_index']}
                            }
                        )
                        all_documents.append(doc)
                
                if next_offset is None:
                    break
                
                offset = next_offset
            
            self.logger.debug(f"Qdrantì—ì„œ ì´ {len(all_documents)}ê°œ ë¬¸ì„œ ê°€ì ¸ì˜´")
            return all_documents
            
        except Exception as e:
            self.logger.error(f"Qdrantì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")
            return []
    
    def get_collection_info(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ ì •ë³´ ë°˜í™˜"""
        try:
            collection_info = self.client.get_collection(self.collection_name)
            
            # vectorsê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (ë‹¤ì¤‘ ë²¡í„° ì§€ì›)ì™€ ë‹¨ì¼ ë²¡í„°ì¸ ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
            vectors_config = collection_info.config.params.vectors
            if isinstance(vectors_config, dict):
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°: ê¸°ë³¸ ë²¡í„°("") ë˜ëŠ” ì²« ë²ˆì§¸ ë²¡í„° ì‚¬ìš©
                if "" in vectors_config:
                    vector_params = vectors_config[""]
                else:
                    # ê¸°ë³¸ ë²¡í„°ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë²¡í„° ì‚¬ìš©
                    vector_params = next(iter(vectors_config.values()))
                vector_size = vector_params.size
                distance_metric = vector_params.distance.name
            else:
                # ë‹¨ì¼ ë²¡í„°ì¸ ê²½ìš° (ë ˆê±°ì‹œ)
                vector_size = vectors_config.size
                distance_metric = vectors_config.distance.name
            
            # Sparse ë²¡í„° ì •ë³´ í™•ì¸
            sparse_vectors_info = None
            if hasattr(collection_info.config.params, 'sparse_vectors') and \
               collection_info.config.params.sparse_vectors:
                sparse_vectors = collection_info.config.params.sparse_vectors
                if isinstance(sparse_vectors, dict):
                    sparse_vectors_info = list(sparse_vectors.keys())
            
            result = {
                'name': self.collection_name,
                'vector_size': vector_size,
                'distance_metric': distance_metric,
                'points_count': collection_info.points_count,
                'status': collection_info.status.name
            }
            
            if sparse_vectors_info:
                result['sparse_vectors'] = sparse_vectors_info
            
            return result
        except ValueError as e:
            # ì»¬ë ‰ì…˜ì´ ì—†ëŠ” ê²½ìš° (ì´ˆê¸°í™” ì‹œ ì •ìƒì ì¸ ìƒí™©)
            if "not found" in str(e).lower():
                self.logger.debug(f"ì»¬ë ‰ì…˜ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•ŠìŒ: {self.collection_name} (ì´ˆê¸°í™” ì¤‘ì¼ ìˆ˜ ìˆìŒ)")
                return {}
            else:
                self.logger.error(f"ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
                return {}
        except Exception as e:
            self.logger.error(f"ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {}
    
    def delete_collection(self) -> bool:
        """ì»¬ë ‰ì…˜ ì‚­ì œ"""
        try:
            self.client.delete_collection(self.collection_name)
            self.logger.info(f"ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ: {self.collection_name}")
            return True
        except Exception as e:
            self.logger.error(f"ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    '''ìƒ˜í”Œ ë²¡í„° DBí™•ì¸'''
    def inspect_vectors(self, sample_size: int = 3, point_ids: Optional[List[Any]] = None) -> Dict[str, Any]:
        """
        ìƒ˜í”Œ í¬ì¸íŠ¸ì˜ Denseì™€ Sparse ë²¡í„° í™•ì¸
        
        Args:
            sample_size: í™•ì¸í•  ìƒ˜í”Œ í¬ì¸íŠ¸ ìˆ˜ (point_idsê°€ Noneì¼ ë•Œ)
            point_ids: í™•ì¸í•  íŠ¹ì • í¬ì¸íŠ¸ ID ë¦¬ìŠ¤íŠ¸ (ì§€ì • ì‹œ sample_size ë¬´ì‹œ)
            
        Returns:
            ë²¡í„° ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
            collection_info = self.client.get_collection(self.collection_name)
            
            # ë²¡í„° ì„¤ì • í™•ì¸
            vectors_config = collection_info.config.params.vectors
            sparse_vectors_config = getattr(collection_info.config.params, 'sparse_vectors', None)
            
            result = {
                'collection_name': self.collection_name,
                'points_count': collection_info.points_count,
                'dense_vectors': {},
                'sparse_vectors': {},
                'samples': []
            }
            
            # Dense ë²¡í„° ì„¤ì • ì •ë³´
            if isinstance(vectors_config, dict):
                for vec_name, vec_params in vectors_config.items():
                    result['dense_vectors'][vec_name or '(default)'] = {
                        'size': vec_params.size,
                        'distance': vec_params.distance.name
                    }
            elif vectors_config:
                result['dense_vectors']['(default)'] = {
                    'size': vectors_config.size,
                    'distance': vectors_config.distance.name
                }
            
            # Sparse ë²¡í„° ì„¤ì • ì •ë³´
            if sparse_vectors_config and isinstance(sparse_vectors_config, dict):
                for sparse_name in sparse_vectors_config.keys():
                    result['sparse_vectors'][sparse_name] = {
                        'enabled': True
                    }
            
            # ìƒ˜í”Œ í¬ì¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
            if point_ids:
                # íŠ¹ì • í¬ì¸íŠ¸ IDë¡œ ì¡°íšŒ
                # Qdrant ë‹¤ì¤‘ ë²¡í„° êµ¬ì¡°ì—ì„œëŠ” with_vectorsë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì§€ì •í•´ì•¼ í•  ìˆ˜ ìˆìŒ
                try:
                    # ë°©ë²• 1: ëª¨ë“  ë²¡í„° í¬í•¨ (ë‹¤ì¤‘ ë²¡í„° êµ¬ì¡°)
                    points = self.client.retrieve(
                        collection_name=self.collection_name,
                        ids=point_ids,
                        with_payload=True,
                        with_vectors=True  # ëª¨ë“  ë²¡í„° í¬í•¨ (dense + sparse)
                    )
                except Exception as e:
                    self.logger.warning(f"retrieve with_vectors=True ì‹¤íŒ¨: {str(e)}, ë‹¤ë¥¸ ë°©ë²• ì‹œë„")
                    # ë°©ë²• 2: ëª…ì‹œì ìœ¼ë¡œ ë²¡í„° ì´ë¦„ ì§€ì •
                    points = self.client.retrieve(
                        collection_name=self.collection_name,
                        ids=point_ids,
                        with_payload=True,
                        with_vectors={self.dense_vector_name: True, self.sparse_vector_name: True}
                    )
            else:
                # ìƒ˜í”Œ í¬ì¸íŠ¸ ìŠ¤í¬ë¡¤
                try:
                    scroll_result = self.client.scroll(
                        collection_name=self.collection_name,
                        limit=sample_size,
                        with_payload=True,
                        with_vectors=True  # ëª¨ë“  ë²¡í„° í¬í•¨ (dense + sparse)
                    )
                    points = scroll_result[0]
                except Exception as e:
                    self.logger.warning(f"scroll with_vectors=True ì‹¤íŒ¨: {str(e)}, ë‹¤ë¥¸ ë°©ë²• ì‹œë„")
                    scroll_result = self.client.scroll(
                        collection_name=self.collection_name,
                        limit=sample_size,
                        with_payload=True,
                        with_vectors={self.dense_vector_name: True, self.sparse_vector_name: True}
                    )
                    points = scroll_result[0]
            
            # ìƒ˜í”Œ í¬ì¸íŠ¸ ë¶„ì„
            for point in points:
                sample_info = {
                    'point_id': str(point.id),
                    'payload': point.payload,
                    'dense_vectors': {},
                    'sparse_vectors': {}
                }
                
                # ë””ë²„ê¹…: point.vector êµ¬ì¡° í™•ì¸
                self.logger.info(f"í¬ì¸íŠ¸ {point.id} ë²¡í„° íƒ€ì…: {type(point.vector)}")
                if hasattr(point, 'vector'):
                    self.logger.info(f"í¬ì¸íŠ¸ {point.id} has vector: {point.vector is not None}")
                    if isinstance(point.vector, dict):
                        self.logger.info(f"í¬ì¸íŠ¸ {point.id} vector keys: {list(point.vector.keys())}")
                        for k, v in point.vector.items():
                            self.logger.info(f"í¬ì¸íŠ¸ {point.id} vector[{k}] íƒ€ì…: {type(v)}")
                            if hasattr(v, 'indices'):
                                self.logger.info(f"í¬ì¸íŠ¸ {point.id} vector[{k}] indices íƒ€ì…: {type(v.indices)}, ê¸¸ì´: {len(v.indices) if hasattr(v, '__len__') else 'N/A'}")
                    elif point.vector is not None:
                        self.logger.info(f"í¬ì¸íŠ¸ {point.id} vectorëŠ” ë¦¬ìŠ¤íŠ¸ íƒ€ì…, ê¸¸ì´: {len(point.vector) if hasattr(point.vector, '__len__') else 'N/A'}")
                
                # Dense ë²¡í„° í™•ì¸
                if point.vector:
                    if isinstance(point.vector, dict):
                        # ë‹¤ì¤‘ ë²¡í„°
                        for vec_name, vec_data in point.vector.items():
                            if isinstance(vec_data, list):
                                sample_info['dense_vectors'][vec_name or '(default)'] = {
                                    'size': len(vec_data),
                                    'preview': vec_data[:5] if len(vec_data) > 5 else vec_data,
                                    'has_data': True
                                }
                    elif isinstance(point.vector, list):
                        # ë‹¨ì¼ ë²¡í„°
                        sample_info['dense_vectors']['(default)'] = {
                            'size': len(point.vector),
                            'preview': point.vector[:5] if len(point.vector) > 5 else point.vector,
                            'has_data': True
                        }
                
                # Sparse ë²¡í„° í™•ì¸
                # ë°©ë²• 1: point.vector ë”•ì…”ë„ˆë¦¬ì—ì„œ sparse ë²¡í„° í™•ì¸ (Qdrant ë‹¤ì¤‘ ë²¡í„° êµ¬ì¡°)
                if point.vector and isinstance(point.vector, dict):
                    self.logger.info(f"í¬ì¸íŠ¸ {point.id} vector ë”•ì…”ë„ˆë¦¬ í™•ì¸: {list(point.vector.keys())}, sparse_vector_name={self.sparse_vector_name}")
                    for vec_name, vec_data in point.vector.items():
                        self.logger.info(f"í¬ì¸íŠ¸ {point.id} ë²¡í„° ì´ë¦„: {vec_name}, íƒ€ì…: {type(vec_data)}")
                        # Sparse ë²¡í„°ëŠ” QdrantSparseVector ê°ì²´ ë˜ëŠ” dict í˜•íƒœ
                        is_sparse = (vec_name == self.sparse_vector_name) or (hasattr(vec_data, 'indices') and hasattr(vec_data, 'values'))
                        self.logger.info(f"í¬ì¸íŠ¸ {point.id} {vec_name} is_sparse: {is_sparse}, hasattr indices: {hasattr(vec_data, 'indices')}, hasattr values: {hasattr(vec_data, 'values')}")
                        if is_sparse:
                            sparse_info = {}
                            
                            if hasattr(vec_data, 'indices') and hasattr(vec_data, 'values'):
                                indices = list(vec_data.indices)
                                values = list(vec_data.values)
                            elif isinstance(vec_data, dict) and 'indices' in vec_data and 'values' in vec_data:
                                indices = list(vec_data['indices'])
                                values = list(vec_data['values'])
                            else:
                                continue
                            
                            sparse_info = {
                                'indices_count': len(indices),
                                'values_count': len(values),
                                'indices_preview': indices[:10] if len(indices) > 10 else indices,
                                'values_preview': values[:10] if len(values) > 10 else values,
                                'has_data': True
                            }
                            
                            # í† í° ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ë‹¨ì–´ë¡œ ë³€í™˜
                            tokens_info = []
                            try:
                                if hasattr(self, 'bge_model') and self.bge_model and hasattr(self.bge_model, 'tokenizer') and self.bge_model.tokenizer:
                                    for token_idx, weight in zip(indices, values):
                                        token_info = {
                                            'token_id': token_idx,
                                            'weight': float(weight)
                                        }
                                        
                                        try:
                                            # convert_ids_to_tokens: ì„œë¸Œì›Œë“œ í† í° ë°˜í™˜
                                            tokens = self.bge_model.tokenizer.convert_ids_to_tokens([token_idx])
                                            if tokens and len(tokens) > 0:
                                                token_info['token_text'] = tokens[0]
                                            
                                            # decode: ì‹¤ì œ ë‹¨ì–´ë¡œ ë””ì½”ë”©
                                            token_word = self.bge_model.tokenizer.decode([token_idx], skip_special_tokens=True)
                                            if token_word:
                                                token_info['token_word'] = token_word.strip()
                                        except Exception as e:
                                            self.logger.debug(f"í† í° {token_idx} ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
                                        
                                        tokens_info.append(token_info)
                                    
                                    sparse_info['tokens'] = tokens_info
                                    sparse_info['tokens_preview'] = tokens_info[:10] if len(tokens_info) > 10 else tokens_info
                            except Exception as e:
                                self.logger.debug(f"í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•œ í† í° ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
                            
                            sample_info['sparse_vectors'][vec_name or self.sparse_vector_name] = sparse_info
                
                # ë°©ë²• 2: point.sparse_vectors ì†ì„± í™•ì¸ (ë ˆê±°ì‹œ)
                if hasattr(point, 'sparse_vectors') and point.sparse_vectors:
                    if isinstance(point.sparse_vectors, dict):
                        for sparse_name, sparse_data in point.sparse_vectors.items():
                            if hasattr(sparse_data, 'indices') and hasattr(sparse_data, 'values'):
                                indices = list(sparse_data.indices)
                                values = list(sparse_data.values)
                                
                                sparse_info = {
                                    'indices_count': len(indices),
                                    'values_count': len(values),
                                    'indices_preview': indices[:10] if len(indices) > 10 else indices,
                                    'values_preview': values[:10] if len(values) > 10 else values,
                                    'has_data': True
                                }
                                
                                # í† í° ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ë‹¨ì–´ë¡œ ë³€í™˜
                                tokens_info = []
                                try:
                                    if hasattr(self, 'bge_model') and self.bge_model and hasattr(self.bge_model, 'tokenizer') and self.bge_model.tokenizer:
                                        for token_idx, weight in zip(indices, values):
                                            token_info = {
                                                'token_id': token_idx,
                                                'weight': float(weight)
                                            }
                                            
                                            try:
                                                tokens = self.bge_model.tokenizer.convert_ids_to_tokens([token_idx])
                                                if tokens and len(tokens) > 0:
                                                    token_info['token_text'] = tokens[0]
                                                
                                                token_word = self.bge_model.tokenizer.decode([token_idx], skip_special_tokens=True)
                                                if token_word:
                                                    token_info['token_word'] = token_word.strip()
                                            except Exception as e:
                                                self.logger.debug(f"í† í° {token_idx} ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
                                            
                                            tokens_info.append(token_info)
                                        
                                        sparse_info['tokens'] = tokens_info
                                        sparse_info['tokens_preview'] = tokens_info[:10] if len(tokens_info) > 10 else tokens_info
                                except Exception as e:
                                    self.logger.debug(f"í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•œ í† í° ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
                                
                                sample_info['sparse_vectors'][sparse_name] = sparse_info
                
                result['samples'].append(sample_info)
            
            self.logger.info(f"ë²¡í„° í™•ì¸ ì™„ë£Œ: {len(result['samples'])}ê°œ ìƒ˜í”Œ í¬ì¸íŠ¸")
            return result
            
        except Exception as e:
            self.logger.error(f"ë²¡í„° í™•ì¸ ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {
                'error': str(e),
                'collection_name': self.collection_name
            }
    
    def get_vector_statistics(self) -> Dict[str, Any]:
        """
        ë²¡í„° í†µê³„ ì •ë³´ ë°˜í™˜
        
        Returns:
            ë²¡í„° í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        try:
            collection_info = self.client.get_collection(self.collection_name)
            
            # ë²¡í„° ì„¤ì • í™•ì¸
            vectors_config = collection_info.config.params.vectors
            sparse_vectors_config = getattr(collection_info.config.params, 'sparse_vectors', None)
            
            stats = {
                'collection_name': self.collection_name,
                'points_count': collection_info.points_count,
                'dense_vector_count': 0,
                'sparse_vector_count': 0,
                'dense_vectors_enabled': False,
                'sparse_vectors_enabled': False,
                'vector_configs': {}
            }
            
            # Dense ë²¡í„° í™•ì¸
            if vectors_config:
                stats['dense_vectors_enabled'] = True
                if isinstance(vectors_config, dict):
                    stats['dense_vector_count'] = len(vectors_config)
                    for vec_name, vec_params in vectors_config.items():
                        stats['vector_configs'][vec_name or '(default)'] = {
                            'type': 'dense',
                            'size': vec_params.size,
                            'distance': vec_params.distance.name
                        }
                else:
                    stats['dense_vector_count'] = 1
                    stats['vector_configs']['(default)'] = {
                        'type': 'dense',
                        'size': vectors_config.size,
                        'distance': vectors_config.distance.name
                    }
            
            # Sparse ë²¡í„° í™•ì¸
            if sparse_vectors_config and isinstance(sparse_vectors_config, dict):
                stats['sparse_vectors_enabled'] = True
                stats['sparse_vector_count'] = len(sparse_vectors_config)
                for sparse_name in sparse_vectors_config.keys():
                    stats['vector_configs'][sparse_name] = {
                        'type': 'sparse'
                    }
            
            return stats
            
        except Exception as e:
            self.logger.error(f"ë²¡í„° í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {
                'error': str(e),
                'collection_name': self.collection_name
            }
    
    def get_sparse_vocabulary(self, limit: Optional[int] = None, search_token: Optional[str] = None) -> Dict[str, Any]:
        """
        Qdrantì—ì„œ Sparse ë²¡í„°ë¥¼ ì¡°íšŒí•˜ì—¬ Vocabulary ì •ë³´ ì§‘ê³„ (BGE-m3 ê¸°ë°˜)
        
        Qdrantì— ì €ì¥ëœ ëª¨ë“  í¬ì¸íŠ¸ì˜ sparse ë²¡í„°ë¥¼ ì¡°íšŒí•˜ì—¬:
        - í† í° ì¸ë±ìŠ¤ì™€ ê°€ì¤‘ì¹˜ ì§‘ê³„
        - í† í°ë³„ ë¬¸ì„œ ë¹ˆë„ ê³„ì‚° (DF)
        - IDF ê°’ ê³„ì‚° (Inverse Document Frequency)
        - í†µê³„ ì •ë³´ ì œê³µ
        
        Args:
            limit: ë°˜í™˜í•  vocabulary í•­ëª© ìˆ˜ (Noneì´ë©´ ì „ì²´, ê¸°ë³¸ê°’: 1000ê°œ)
            search_token: íŠ¹ì • í† í° ê²€ìƒ‰ (í† í° ì¸ë±ìŠ¤ê°€ í¬í•¨ëœ í•­ëª©ë§Œ ë°˜í™˜)
        
        Returns:
            Vocabulary ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            result = {
                'sparse_enabled': self.sparse_enabled,
                'model_type': 'BGE-m3',
                'model_trained': True,
                'corpus_size': 0,
                'vocabulary_size': 0,
                'avgdl': 0.0,
                'vocabulary': {},
                'idf_values': {},
                'statistics': {},
                'message': ''
            }
            
            if not self.sparse_enabled:
                result['message'] = 'Sparse ë²¡í„°ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.'
                return result
            
            if not self._check_connection():
                result['message'] = 'Qdrant ì—°ê²°ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
                return result
            
            self.logger.info("Qdrantì—ì„œ sparse ë²¡í„° vocabulary ì§‘ê³„ ì‹œì‘...")
            
            # ëª¨ë“  í¬ì¸íŠ¸ì˜ sparse ë²¡í„° ìˆ˜ì§‘
            token_doc_freq: Dict[int, int] = {}  # í† í° ì¸ë±ìŠ¤ -> ë¬¸ì„œ ìˆ˜
            token_weights: Dict[int, List[float]] = {}  # í† í° ì¸ë±ìŠ¤ -> ê°€ì¤‘ì¹˜ ë¦¬ìŠ¤íŠ¸
            total_documents = 0
            offset = None
            
            # Scrollì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  í¬ì¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
            while True:
                scroll_result = self.client.scroll(
                    collection_name=self.collection_name,
                    limit=self.max_scroll_limit,
                    offset=offset,
                    with_payload=False,
                    with_vectors=True  # sparse ë²¡í„°ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ í•„ìš”
                )
                
                points, next_offset = scroll_result
                
                if not points:
                    break
                
                for point in points:
                    # Sparse ë²¡í„° ì¶”ì¶œ
                    if hasattr(point, 'vector') and point.vector:
                        vectors = point.vector
                        if isinstance(vectors, dict) and self.sparse_vector_name in vectors:
                            sparse_vec = vectors[self.sparse_vector_name]
                            if hasattr(sparse_vec, 'indices') and hasattr(sparse_vec, 'values'):
                                indices = sparse_vec.indices
                                values = sparse_vec.values
                                
                                # ê° í† í° ì¸ë±ìŠ¤ì— ëŒ€í•´ ë¬¸ì„œ ë¹ˆë„ ë° ê°€ì¤‘ì¹˜ ìˆ˜ì§‘
                                seen_tokens_in_doc = set()
                                for idx, weight in zip(indices, values):
                                    if idx not in seen_tokens_in_doc:
                                        token_doc_freq[idx] = token_doc_freq.get(idx, 0) + 1
                                        seen_tokens_in_doc.add(idx)
                                    
                                    if idx not in token_weights:
                                        token_weights[idx] = []
                                    token_weights[idx].append(float(weight))
                
                total_documents += len(points)
                
                if next_offset is None:
                    break
                
                offset = next_offset
            
            if total_documents == 0:
                result['message'] = 'Qdrantì— ì €ì¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.'
                return result
            
            # IDF ê³„ì‚°: IDF(t) = log(N / df(t))
            # N: ì „ì²´ ë¬¸ì„œ ìˆ˜, df(t): í† í° të¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì„œ ìˆ˜
            import math
            vocabulary = {}
            idf_values = {}
            
            # í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ë‹¨ì–´ë¡œ ë³€í™˜
            tokenizer_available = False
            try:
                if hasattr(self.bge_model, 'tokenizer') and self.bge_model.tokenizer is not None:
                    tokenizer_available = True
                    self.logger.info("í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ì¸ë±ìŠ¤ë¥¼ ë‹¨ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
            except Exception as e:
                self.logger.warning(f"í† í¬ë‚˜ì´ì € ì ‘ê·¼ ì‹¤íŒ¨: {str(e)}. í† í° ì¸ë±ìŠ¤ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.")
            
            for token_idx in token_doc_freq.keys():
                df = token_doc_freq[token_idx]
                idf = math.log(total_documents / df) if df > 0 else 0.0
                
                # í‰ê·  ê°€ì¤‘ì¹˜ ê³„ì‚°
                avg_weight = sum(token_weights[token_idx]) / len(token_weights[token_idx]) if token_idx in token_weights else 0.0
                max_weight = max(token_weights[token_idx]) if token_idx in token_weights else 0.0
                min_weight = min(token_weights[token_idx]) if token_idx in token_weights else 0.0
                
                # í† í° ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ë‹¨ì–´ë¡œ ë³€í™˜
                token_text = None
                token_text_decoded = None
                if tokenizer_available:
                    try:
                        # convert_ids_to_tokens: ì„œë¸Œì›Œë“œ í† í° ë°˜í™˜ (ì˜ˆ: "ì „ê¸°" -> ["ì „", "##ê¸°"])
                        tokens = self.bge_model.tokenizer.convert_ids_to_tokens([token_idx])
                        if tokens and len(tokens) > 0:
                            token_text = tokens[0]
                        
                        # decode: ì‹¤ì œ ë‹¨ì–´ë¡œ ë””ì½”ë”© (ì„œë¸Œì›Œë“œ í† í°ì„ í•©ì³ì„œ ë‹¨ì–´ë¡œ ë§Œë“¦)
                        token_text_decoded = self.bge_model.tokenizer.decode([token_idx], skip_special_tokens=True)
                        # decodeëŠ” ê³µë°±ì„ ì¶”ê°€í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ strip
                        if token_text_decoded:
                            token_text_decoded = token_text_decoded.strip()
                    except Exception as e:
                        self.logger.debug(f"í† í° {token_idx} ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
                        token_text = None
                        token_text_decoded = None
                
                vocabulary[token_idx] = {
                    'index': token_idx,
                    'document_frequency': df,
                    'avg_weight': avg_weight,
                    'max_weight': max_weight,
                    'min_weight': min_weight,
                    'total_occurrences': len(token_weights[token_idx]),
                    'token_text': token_text,  # ì„œë¸Œì›Œë“œ í† í° (ì˜ˆ: "ì „", "##ê¸°")
                    'token_word': token_text_decoded  # ì‹¤ì œ ë‹¨ì–´ (ì˜ˆ: "ì „ê¸°")
                }
                idf_values[token_idx] = idf
            
            # search_token í•„í„°ë§ (í† í° ì¸ë±ìŠ¤, í† í° í…ìŠ¤íŠ¸, ì‹¤ì œ ë‹¨ì–´ ëª¨ë‘ ê²€ìƒ‰)
            if search_token:
                search_token_lower = search_token.lower()
                filtered_vocab = {}
                filtered_idf = {}
                for token_idx, vocab_info in vocabulary.items():
                    token_str = str(token_idx)
                    token_text = vocab_info.get('token_text', '') or ''
                    token_word = vocab_info.get('token_word', '') or ''
                    
                    # í† í° ì¸ë±ìŠ¤, í† í° í…ìŠ¤íŠ¸, ì‹¤ì œ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë¼ë„ ë§¤ì¹­ë˜ë©´ í¬í•¨
                    if (search_token_lower in token_str.lower() or 
                        search_token_lower in token_text.lower() or 
                        search_token_lower in token_word.lower()):
                        filtered_vocab[token_idx] = vocab_info
                        filtered_idf[token_idx] = idf_values[token_idx]
                vocabulary = filtered_vocab
                idf_values = filtered_idf
            
            # limit ì ìš© ë° ì •ë ¬ (IDF ê°’ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
            sorted_tokens = sorted(idf_values.items(), key=lambda x: x[1], reverse=True)
            if limit:
                sorted_tokens = sorted_tokens[:limit]
            
            # ìµœì¢… vocabulary ë° idf_values êµ¬ì„±
            final_vocabulary = {}
            final_idf_values = {}
            for token_idx, idf_val in sorted_tokens:
                final_vocabulary[token_idx] = vocabulary[token_idx]
                final_idf_values[token_idx] = idf_val
            
            # í†µê³„ ì •ë³´ ê³„ì‚°
            if idf_values:
                idf_list = list(idf_values.values())
                statistics = {
                    'min_idf': min(idf_list),
                    'max_idf': max(idf_list),
                    'avg_idf': sum(idf_list) / len(idf_list),
                    'median_idf': sorted(idf_list)[len(idf_list) // 2] if idf_list else 0.0,
                    'top_tokens': [
                        {
                            'index': idx, 
                            'idf': idf_val, 
                            'df': vocabulary[idx]['document_frequency'],
                            'token_text': vocabulary[idx].get('token_text'),
                            'token_word': vocabulary[idx].get('token_word')
                        }
                        for idx, idf_val in sorted_tokens[:10]
                    ]
                }
            else:
                statistics = {}
            
            result.update({
                'corpus_size': total_documents,
                'vocabulary_size': len(final_vocabulary),
                'vocabulary': final_vocabulary,
                'idf_values': final_idf_values,
                'statistics': statistics,
                'message': f'Qdrantì—ì„œ {total_documents}ê°œ ë¬¸ì„œì˜ sparse ë²¡í„°ë¥¼ ì¡°íšŒí•˜ì—¬ {len(final_vocabulary)}ê°œ í† í°ì„ ì§‘ê³„í–ˆìŠµë‹ˆë‹¤.'
            })
            
            self.logger.info(f"Vocabulary ì§‘ê³„ ì™„ë£Œ: {total_documents}ê°œ ë¬¸ì„œ, {len(final_vocabulary)}ê°œ í† í°")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Sparse vocabulary ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {
                'error': str(e),
                'sparse_enabled': self.sparse_enabled,
                'message': f'Vocabulary ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
            }

    # ------------------------------------------------------------------
    def _preprocess_texts_for_embedding(self, texts: List[str]) -> List[str]:
        """BGE ì„ë² ë”© ì „ì— Kiwipiepy í˜•íƒœì†Œ ì „ì²˜ë¦¬ë¥¼ ì ìš©"""
        if not self.use_kiwipiepy_preprocessing or not self.kiwipiepy_preprocessor:
            return texts
        processed: List[str] = []
        for text in texts:
            try:
                processed.append(self.kiwipiepy_preprocessor.preprocess(text))
            except Exception as exc:  # pragma: no cover - ë°©ì–´ì  ì²˜ë¦¬
                self.logger.warning(f"Kiwipiepy ì „ì²˜ë¦¬ ì‹¤íŒ¨. ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©: {exc}")
                processed.append(text)
        return processed

    def _preprocess_query_text(self, text: str) -> str:
        """ê²€ìƒ‰ ì¿¼ë¦¬ ì „ì²˜ë¦¬"""
        if not self.use_kiwipiepy_preprocessing or not self.kiwipiepy_preprocessor:
            return text
        try:
            return self.kiwipiepy_preprocessor.preprocess(text)
        except Exception as exc:  # pragma: no cover
            self.logger.warning(f"Kiwipiepy ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì‹¤íŒ¨. ì›ë³¸ ì‚¬ìš©: {exc}")
            return text
    
    def analyze_sparse_quality(self) -> Dict[str, Any]:
        """
        Sparse ë²¡í„° DB í’ˆì§ˆ ë¶„ì„
        
        Returns:
            í’ˆì§ˆ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        import statistics
        from collections import Counter
        
        try:
            result = {
                'sparse_enabled': self.sparse_enabled,
                'total_points': 0,
                'points_with_sparse': 0,
                'points_without_sparse': 0,
                'empty_sparse_vectors': 0,
                'token_statistics': {},
                'weight_statistics': {},
                'weight_distribution': {},
                'vocabulary_statistics': {},
                'quality_assessment': {},
                'recommendations': [],
                'issues': []
            }
            
            if not self.sparse_enabled:
                result['message'] = 'Sparse ë²¡í„°ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.'
                return result
            
            if not self._check_connection():
                result['message'] = 'Qdrant ì—°ê²°ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
                return result
            
            self.logger.info("Sparse ë²¡í„° í’ˆì§ˆ ë¶„ì„ ì‹œì‘...")
            
            # í†µê³„ ìˆ˜ì§‘
            token_counts = []
            weight_values = []
            token_frequency = Counter()
            low_weight_tokens = 0  # < 0.1
            medium_weight_tokens = 0  # 0.1-0.5
            high_weight_tokens = 0  # >= 0.5
            
            # ëª¨ë“  í¬ì¸íŠ¸ ìŠ¤í¬ë¡¤
            offset = None
            total_points = 0
            points_with_sparse = 0
            points_without_sparse = 0
            empty_sparse_vectors = 0
            
            while True:
                scroll_result = self.client.scroll(
                    collection_name=self.collection_name,
                    limit=self.max_scroll_limit,
                    offset=offset,
                    with_payload=False,
                    with_vectors=True
                )
                
                points, next_offset = scroll_result
                
                if not points:
                    break
                
                total_points += len(points)
                
                for point in points:
                    vectors = point.vector if hasattr(point, 'vector') else {}
                    
                    if isinstance(vectors, dict):
                        sparse_vector = vectors.get(self.sparse_vector_name)
                    else:
                        sparse_vector = None
                    
                    if sparse_vector is None:
                        points_without_sparse += 1
                        continue
                    
                    points_with_sparse += 1
                    
                    # Sparse ë²¡í„° êµ¬ì¡° í™•ì¸
                    if hasattr(sparse_vector, 'indices') and hasattr(sparse_vector, 'values'):
                        indices = sparse_vector.indices
                        values = sparse_vector.values
                    elif isinstance(sparse_vector, dict):
                        indices = list(sparse_vector.keys())
                        values = list(sparse_vector.values())
                    else:
                        continue
                    
                    if not indices or not values:
                        empty_sparse_vectors += 1
                        continue
                    
                    # í† í° ìˆ˜
                    token_count = len(indices)
                    token_counts.append(token_count)
                    
                    # í† í° ë¹ˆë„
                    for token_idx in indices:
                        token_frequency[token_idx] += 1
                    
                    # ê°€ì¤‘ì¹˜ ë¶„ì„
                    for weight in values:
                        weight_values.append(float(weight))
                        
                        if weight < 0.1:
                            low_weight_tokens += 1
                        elif weight < 0.5:
                            medium_weight_tokens += 1
                        else:
                            high_weight_tokens += 1
                
                if next_offset is None:
                    break
                
                offset = next_offset
            
            # í†µê³„ ê³„ì‚°
            result['total_points'] = total_points
            result['points_with_sparse'] = points_with_sparse
            result['points_without_sparse'] = points_without_sparse
            result['empty_sparse_vectors'] = empty_sparse_vectors
            
            # í† í° í†µê³„
            if token_counts:
                result['token_statistics'] = {
                    'mean': statistics.mean(token_counts),
                    'median': statistics.median(token_counts),
                    'min': min(token_counts),
                    'max': max(token_counts),
                    'stdev': statistics.stdev(token_counts) if len(token_counts) > 1 else 0.0,
                    'q25': sorted(token_counts)[len(token_counts) // 4] if token_counts else 0,
                    'q75': sorted(token_counts)[len(token_counts) * 3 // 4] if token_counts else 0
                }
            
            # ê°€ì¤‘ì¹˜ í†µê³„
            if weight_values:
                result['weight_statistics'] = {
                    'mean': statistics.mean(weight_values),
                    'median': statistics.median(weight_values),
                    'min': min(weight_values),
                    'max': max(weight_values),
                    'stdev': statistics.stdev(weight_values) if len(weight_values) > 1 else 0.0
                }
            
            # ê°€ì¤‘ì¹˜ ë¶„í¬
            total_tokens = low_weight_tokens + medium_weight_tokens + high_weight_tokens
            if total_tokens > 0:
                result['weight_distribution'] = {
                    'low_weight_count': low_weight_tokens,
                    'low_weight_percentage': low_weight_tokens / total_tokens * 100,
                    'medium_weight_count': medium_weight_tokens,
                    'medium_weight_percentage': medium_weight_tokens / total_tokens * 100,
                    'high_weight_count': high_weight_tokens,
                    'high_weight_percentage': high_weight_tokens / total_tokens * 100
                }
            
            # Vocabulary í†µê³„
            unique_tokens = len(token_frequency)
            result['vocabulary_statistics'] = {
                'unique_tokens': unique_tokens,
                'most_common_tokens': [
                    {'token_id': token_id, 'frequency': count, 'percentage': count / points_with_sparse * 100 if points_with_sparse > 0 else 0}
                    for token_id, count in token_frequency.most_common(10)
                ]
            }
            
            # í’ˆì§ˆ í‰ê°€
            issues = []
            recommendations = []
            
            # 1. Sparse ë²¡í„° ëˆ„ë½ í™•ì¸
            if total_points > 0:
                missing_rate = points_without_sparse / total_points
                if missing_rate > 0.1:
                    issues.append(f"{missing_rate*100:.1f}%ì˜ í¬ì¸íŠ¸ì— sparse ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    recommendations.append("ë¬¸ì„œ ì¬ì²˜ë¦¬ ì‹œ sparse ë²¡í„° ìƒì„± í™•ì¸ í•„ìš”")
            
            # 2. í† í° ìˆ˜ í™•ì¸
            if token_counts:
                avg_tokens = statistics.mean(token_counts)
                if avg_tokens < 10:
                    issues.append(f"í‰ê·  í† í° ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ ({avg_tokens:.1f}ê°œ).")
                    recommendations.append("ì²­í¬ í¬ê¸° ì¦ê°€ ë˜ëŠ” í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê°œì„  ê³ ë ¤")
                elif avg_tokens > 200:
                    issues.append(f"í‰ê·  í† í° ìˆ˜ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤ ({avg_tokens:.1f}ê°œ).")
                    recommendations.append("ì²­í¬ í¬ê¸° ê°ì†Œ ë˜ëŠ” ë” ì„¸ë°€í•œ ì²­í‚¹ ê³ ë ¤")
            
            # 3. ê°€ì¤‘ì¹˜ ë¶„í¬ í™•ì¸
            if total_tokens > 0:
                low_weight_ratio = low_weight_tokens / total_tokens
                if low_weight_ratio > 0.5:
                    issues.append(f"ë‚®ì€ ê°€ì¤‘ì¹˜ í† í°ì´ {low_weight_ratio*100:.1f}%ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤.")
                    recommendations.append("BGE-m3 ëª¨ë¸ì˜ lexical weight ì„ê³„ê°’ ì¡°ì • ê³ ë ¤")
                
                high_weight_ratio = high_weight_tokens / total_tokens
                if high_weight_ratio < 0.1:
                    issues.append(f"ë†’ì€ ê°€ì¤‘ì¹˜ í† í°ì´ {high_weight_ratio*100:.1f}%ì— ë¶ˆê³¼í•©ë‹ˆë‹¤.")
                    recommendations.append("í…ìŠ¤íŠ¸ í’ˆì§ˆ ê°œì„  ë˜ëŠ” ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¡°ì • ê³ ë ¤")
            
            # 4. Vocabulary í¬ê¸° í™•ì¸
            if unique_tokens < 100:
                issues.append(f"Vocabularyê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤ ({unique_tokens}ê°œ).")
                recommendations.append("ë” ë§ì€ ë¬¸ì„œ ì¶”ê°€ ë˜ëŠ” í…ìŠ¤íŠ¸ ë‹¤ì–‘ì„± ì¦ê°€ í•„ìš”")
            elif unique_tokens > 100000:
                issues.append(f"Vocabularyê°€ ë„ˆë¬´ í½ë‹ˆë‹¤ ({unique_tokens:,}ê°œ).")
                recommendations.append("ë¶ˆí•„ìš”í•œ í† í° í•„í„°ë§ ë˜ëŠ” ì „ì²˜ë¦¬ ê°œì„  ê³ ë ¤")
            
            result['quality_assessment'] = {
                'overall_quality': 'good' if not issues else 'needs_improvement',
                'issues': issues
            }
            result['recommendations'] = recommendations
            
            self.logger.info("Sparse ë²¡í„° í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
            return result
            
        except Exception as e:
            self.logger.error(f"Sparse ë²¡í„° í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {
                'error': str(e),
                'message': f'í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {str(e)}'
            }


