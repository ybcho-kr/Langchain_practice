"""
ë²¡í„° ì €ì¥ì†Œ ëª¨ë“ˆ
LangChain-Qdrantë¥¼ ì‚¬ìš©í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬
"""

from typing import List, Dict, Any, Optional
import uuid
import json
import os
from pathlib import Path

from qdrant_client import QdrantClient
from qdrant_client import AsyncQdrantClient
from qdrant_client.models import Distance, VectorParams, SparseVectorParams, Filter, FieldCondition, MatchValue, Query, NamedSparseVector, Prefetch, SparseVector as QdrantSparseVector
from langchain_qdrant import QdrantVectorStore as LangChainQdrantVectorStore
from langchain_qdrant.qdrant import RetrievalMode
from langchain_core.documents import Document
from langchain_ollama import OllamaEmbeddings

from src.utils.logger import get_logger
from src.utils.config import get_qdrant_config, get_embedding_config
from src.modules.document_processor import DocumentChunk
# ë ˆê±°ì‹œ BM25Indexer ì œê±°ë¨ - LangChain BM25Retriever ì‚¬ìš©
# from src.modules.bm25_indexer import BM25Indexer
from src.modules.langchain_retrievers import LangChainRetrievalManager
from src.modules.langchain_embedding_wrapper import EmbeddingManagerWrapper
from src.modules.embedding_module import EmbeddingManager
from src.modules.sparse_embedding import BM25SparseEmbedding, SparseEmbeddingManager


class QdrantVectorStore:
    """LangChain-Qdrant ë²¡í„° ì €ì¥ì†Œ (ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ)"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, embeddings: Optional[Any] = None):
        """
        Args:
            config: Qdrant ì„¤ì •
            embeddings: ê¸°ì¡´ ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì , ì¤‘ë³µ ë¡œë“œ ë°©ì§€ìš©)
        """
        self.logger = get_logger()
        
        if config is None:
            config = get_qdrant_config()
        
        self.collection_name = config.collection_name
        self.vector_size = config.vector_size
        self.distance_metric = Distance.COSINE if config.distance_metric.lower() == 'cosine' else Distance.EUCLIDEAN
        self.storage_path = config.storage_path
        self.use_local_storage = config.use_local_storage
        
        # Sparse ë²¡í„° ì„¤ì •
        self.sparse_enabled = getattr(config, 'sparse_enabled', True)
        self.sparse_vector_name = getattr(config, 'sparse_vector_name', 'sparse')
        self.hybrid_search_dense_weight = getattr(config, 'hybrid_search_dense_weight', 0.7)
        self.hybrid_search_sparse_weight = getattr(config, 'hybrid_search_sparse_weight', 0.3)
        
        # ê²€ìƒ‰ ê¸°ë³¸ê°’ ì„¤ì •
        self.default_limit = config.default_limit
        self.max_scroll_limit = config.max_scroll_limit
        # score_thresholdëŠ” RAG ì„¤ì •ì—ì„œ ì°¸ì¡°í•˜ë„ë¡ ë³€ê²½ (í†µì¼í™”)
        self.connection_timeout = config.connection_timeout
        self.request_timeout = config.request_timeout
        
        # ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ì‚¬ìš©
        if self.use_local_storage:
            from pathlib import Path
            storage_path = Path(self.storage_path)
            storage_path.mkdir(parents=True, exist_ok=True)
            
            self.client = QdrantClient(path=str(storage_path))
            # ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ëŠ” ì§€ì—° ì´ˆê¸°í™” (ë¡œì»¬ ì €ì¥ì†Œ ë™ì‹œ ì ‘ê·¼ ë¬¸ì œ ë°©ì§€)
            self.async_client = None
            self._async_client_path = str(storage_path)
            self._async_client_host = None
            self._async_client_port = None
            self.logger.info(f"Qdrant ë¡œì»¬ ì €ì¥ì†Œ ì´ˆê¸°í™”: {storage_path}")
        else:
            self.client = QdrantClient(host=config.host, port=config.port)
            # ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ëŠ” ì§€ì—° ì´ˆê¸°í™”
            self.async_client = None
            self._async_client_path = None
            self._async_client_host = config.host
            self._async_client_port = config.port
            self.logger.info(f"Qdrant ì„œë²„ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”: {config.host}:{config.port}")
        
        # ì„ë² ë”© ì„¤ì •: ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš© (ì¤‘ë³µ ë¡œë“œ ë°©ì§€)
        if embeddings is not None:
            self.embeddings = embeddings
            self.logger.info("ê¸°ì¡´ ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš© (ì¤‘ë³µ ë¡œë“œ ë°©ì§€)")
        else:
            # LangChain ì„ë² ë”© ì„¤ì • (ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            from src.utils.config import get_embedding_config
            embedding_config = get_embedding_config()
            
            # providerì— ë”°ë¼ ì ì ˆí•œ ì„ë² ë”© í´ë˜ìŠ¤ ì„ íƒ
            if embedding_config.provider == "huggingface":
                from langchain_huggingface import HuggingFaceEmbeddings
                # ë””ë°”ì´ìŠ¤ ê²°ì •: CUDA ê°€ìš©ì„± í™•ì¸ í›„ ë¶ˆê°€í•˜ë©´ CPUë¡œ ê°•ì œ ì „í™˜
                resolved_device = getattr(embedding_config, 'device', 'cuda') or 'cuda'
                if resolved_device == 'cuda':
                    try:
                        import torch  # ì§€ì—° ì„í¬íŠ¸ë¡œ ì´ˆê¸°í™” ë¹„ìš© ìµœì†Œí™”
                        import torchaudio
                        import torchvision
                        
                        # PyTorch íŒ¨í‚¤ì§€ ë²„ì „ ì •ë³´ ë¡œê¹…
                        self.logger.info(f"PyTorch íŒ¨í‚¤ì§€ ë²„ì „ ì •ë³´:")
                        self.logger.info(f"  torch: {torch.__version__}")
                        self.logger.info(f"  torchaudio: {torchaudio.__version__}")
                        self.logger.info(f"  torchvision: {torchvision.__version__}")
                        
                        if not torch.cuda.is_available():
                            self.logger.warning("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                            resolved_device = 'cpu'
                    except Exception:
                        resolved_device = 'cpu'
                self.embeddings = HuggingFaceEmbeddings(
                    model_name=embedding_config.model_path or embedding_config.name,
                    model_kwargs={'device': resolved_device},
                    encode_kwargs={'normalize_embeddings': True}
                )
                self.logger.info(f"HuggingFace ì„ë² ë”© ì´ˆê¸°í™”: {(embedding_config.model_path or embedding_config.name)} (device={resolved_device})")
            else:
                self.embeddings = OllamaEmbeddings(
                    model=embedding_config.name,
                    base_url=embedding_config.base_url
                )
                self.logger.info(f"Ollama ì„ë² ë”© ì´ˆê¸°í™”: {embedding_config.name}")
        
        # Sparse ë²¡í„° ì„¤ì • ì¶”ê°€
        self.sparse_vocabulary_path = getattr(config, 'sparse_vocabulary_path', 'data/sparse_vocabulary')
        self.sparse_use_morphological = getattr(config, 'sparse_use_morphological', True)
        self.sparse_include_doc_stats = getattr(config, 'sparse_include_doc_stats', False)
        
        # Vocabulary íŒŒì¼ ê²½ë¡œ ìƒì„± (ì»¬ë ‰ì…˜ë³„ë¡œ ë¶„ë¦¬)
        vocabulary_file = f"{self.sparse_vocabulary_path}/{self.collection_name}_vocabulary.json"
        
        # Sparse ì„ë² ë”© ì´ˆê¸°í™” (sparse_enabledì¼ ë•Œë§Œ)
        self.sparse_embedding = None
        self.sparse_embedding_manager = None
        if self.sparse_enabled:
            self.sparse_embedding_manager = SparseEmbeddingManager(
                vocabulary_path=vocabulary_file,
                use_morphological=self.sparse_use_morphological
            )
            self.sparse_embedding = self.sparse_embedding_manager.get_sparse_embedding()
            if self.sparse_embedding_manager.is_fitted:
                self.logger.info(f"Sparse ì„ë² ë”© ì´ˆê¸°í™” ì™„ë£Œ (ì €ì¥ëœ Vocabulary ë¡œë“œë¨: {vocabulary_file})")
            else:
                morphological_status = "í™œì„±í™”" if self.sparse_use_morphological else "ë¹„í™œì„±í™”"
                self.logger.info(f"Sparse ì„ë² ë”© ì´ˆê¸°í™” ì™„ë£Œ (í˜•íƒœì†Œ ë¶„ì„: {morphological_status}, í•™ìŠµì€ ë¬¸ì„œ ì¶”ê°€ ì‹œ ìˆ˜í–‰)")
        
        # LangChain Qdrant ë²¡í„° ì €ì¥ì†ŒëŠ” ë‚˜ì¤‘ì— ì´ˆê¸°í™” (ì»¬ë ‰ì…˜ ìƒì„± í›„)
        self.vector_store = None
    
    def _get_async_client(self):
        """ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ì§€ì—° ì´ˆê¸°í™” (ë¡œì»¬ ì €ì¥ì†Œ ë™ì‹œ ì ‘ê·¼ ë¬¸ì œ ë°©ì§€)"""
        if self.async_client is None:
            if self.use_local_storage:
                # ë¡œì»¬ ì €ì¥ì†Œì˜ ê²½ìš° AsyncQdrantLocalì€ ë™ê¸° í´ë¼ì´ì–¸íŠ¸ì™€ ë™ì‹œ ì ‘ê·¼ ë¶ˆê°€
                # ë”°ë¼ì„œ Noneì„ ë°˜í™˜í•˜ì—¬ asyncio.to_thread ì‚¬ìš©ì„ ìœ ë„
                return None
            else:
                # ì„œë²„ ëª¨ë“œì˜ ê²½ìš° ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                self.async_client = AsyncQdrantClient(host=self._async_client_host, port=self._async_client_port)
        return self.async_client
    
    def _check_connection(self) -> bool:
        """ì—°ê²° ìƒíƒœ í™•ì¸"""
        try:
            collections = self.client.get_collections()
            self.logger.debug(f"Qdrant ì—°ê²° í™•ì¸: {len(collections.collections)}ê°œ ì»¬ë ‰ì…˜")
            return True
        except Exception as e:
            self.logger.error(f"Qdrant ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _get_metadata(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í—¬í¼ ë©”ì„œë“œ (ì¤‘ë³µ ì œê±°)"""
        if 'metadata' in payload:
            return payload['metadata']
        return payload
    
    def create_collection(self, force_recreate: bool = False) -> bool:
        """ì»¬ë ‰ì…˜ ìƒì„±"""
        try:
            self.logger.info(f"ì»¬ë ‰ì…˜ ìƒì„± ì‹œì‘: {self.collection_name}")
            
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸
            self.logger.info("ê¸°ì¡´ ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ ì¤‘...")
            collections = self.client.get_collections()
            self.logger.info(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ ìˆ˜: {len(collections.collections)}")
            
            collection_exists = any(col.name == self.collection_name for col in collections.collections)
            self.logger.info(f"ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€: {collection_exists}")
            
            if collection_exists:
                if force_recreate:
                    self.logger.info(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ: {self.collection_name}")
                    self.client.delete_collection(self.collection_name)
                else:
                    self.logger.info(f"ì»¬ë ‰ì…˜ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {self.collection_name}")
                    
                    # LangChain Qdrant ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
                    if self.vector_store is None:
                        self.logger.info("LangChain Qdrant ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘...")
                        
                        # RetrievalMode ê²°ì •
                        if self.sparse_enabled and self.sparse_embedding is not None:
                            retrieval_mode = RetrievalMode.HYBRID
                            self.logger.info(f"RetrievalMode: HYBRID (dense + sparse)")
                        else:
                            retrieval_mode = RetrievalMode.DENSE
                            self.logger.info(f"RetrievalMode: DENSE")
                        
                        self.vector_store = LangChainQdrantVectorStore(
                            client=self.client,
                            collection_name=self.collection_name,
                            embedding=self.embeddings,
                            retrieval_mode=retrieval_mode,
                            sparse_embedding=self.sparse_embedding if self.sparse_enabled else None,
                            sparse_vector_name=self.sparse_vector_name if self.sparse_enabled else None
                        )
                        self.logger.info("LangChain Qdrant ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
                    
                    return True
            
            # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
            self.logger.info(f"ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì¤‘: {self.collection_name}, ë²¡í„° í¬ê¸°: {self.vector_size}")
            
            # Dense ë²¡í„° ì„¤ì •
            vectors_config = {
                "": VectorParams(
                    size=self.vector_size,
                    distance=self.distance_metric
                )
            }
            
            # Sparse ë²¡í„° ì„¤ì • (sparse_enabledì¼ ë•Œë§Œ)
            sparse_vectors_config = None
            if self.sparse_enabled:
                sparse_vectors_config = {
                    self.sparse_vector_name: SparseVectorParams()
                }
                self.logger.info(f"Sparse ë²¡í„° ì„¤ì • ì¶”ê°€: {self.sparse_vector_name}")
            
            # ì»¬ë ‰ì…˜ ìƒì„±
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=vectors_config,
                sparse_vectors_config=sparse_vectors_config
            )
            
            self.logger.info(f"ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ: {self.collection_name}")
            
            # LangChain Qdrant ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
            self.logger.info("LangChain Qdrant ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘...")
            
            # RetrievalMode ê²°ì •
            if self.sparse_enabled and self.sparse_embedding is not None:
                retrieval_mode = RetrievalMode.HYBRID
                self.logger.info(f"RetrievalMode: HYBRID (dense + sparse)")
            else:
                retrieval_mode = RetrievalMode.DENSE
                self.logger.info(f"RetrievalMode: DENSE")
            
            self.vector_store = LangChainQdrantVectorStore(
                client=self.client,
                collection_name=self.collection_name,
                embedding=self.embeddings,
                retrieval_mode=retrieval_mode,
                sparse_embedding=self.sparse_embedding if self.sparse_enabled else None,
                sparse_vector_name=self.sparse_vector_name if self.sparse_enabled else None
            )
            self.logger.info("LangChain Qdrant ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            self.logger.error(f"ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False
    
    def add_documents(self, documents: List[DocumentChunk], force_update: bool = False) -> bool:
        """ë¬¸ì„œ ì¶”ê°€ (LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜, dense+sparse ë²¡í„° í•¨ê»˜ ì €ì¥)"""
        if not self._check_connection():
            return False
        
        if self.vector_store is None:
            self.logger.error("LangChain Qdrant ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False
        
        try:
            # DocumentChunkë¥¼ LangChain Documentë¡œ ë³€í™˜
            langchain_docs = []
            seen_chunks = set()  # ì¤‘ë³µ ì²­í¬ ë°©ì§€
            
            for doc in documents:
                # ì²­í¬ ê³ ìœ  ì‹ë³„ì ìƒì„± (íŒŒì¼ëª… + ì²­í¬ ì¸ë±ìŠ¤)
                chunk_key = f"{doc.source_file}:{doc.chunk_index}"
                    
                if chunk_key in seen_chunks:
                    self.logger.warning(f"ì¤‘ë³µ ì²­í¬ ê±´ë„ˆë›°ê¸°: {chunk_key}")
                    continue
                
                seen_chunks.add(chunk_key)
                
                langchain_doc = Document(
                    page_content=doc.content,
                    metadata={
                        'chunk_id': doc.chunk_id,
                        'source_file': doc.source_file,
                        'chunk_index': doc.chunk_index,
                        **doc.metadata
                    }
                )
                langchain_docs.append(langchain_doc)
            
            # Sparse ì„ë² ë”© ëª¨ë¸ í•™ìŠµ
            if self.sparse_enabled and self.sparse_embedding_manager:
                if not self.sparse_embedding_manager.is_fitted:
                    # ì²« ë²ˆì§¸ í•™ìŠµ: í˜„ì¬ ë¬¸ì„œë¡œ í•™ìŠµ
                    self.logger.info("Sparse ì„ë² ë”© ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ì´ˆê¸° í•™ìŠµ)...")
                    document_texts = [doc.page_content for doc in langchain_docs]
                    self.sparse_embedding_manager.fit(document_texts, include_doc_stats=self.sparse_include_doc_stats)
                    self.sparse_embedding = self.sparse_embedding_manager.get_sparse_embedding()
                    # vector_storeì˜ sparse_embeddingë„ ì—…ë°ì´íŠ¸ (LangChain QdrantVectorStore ë‚´ë¶€ ì†ì„±)
                    if self.vector_store:
                        # LangChain QdrantVectorStoreëŠ” _sparse_embeddings ì†ì„±ì— ì €ì¥
                        if hasattr(self.vector_store, '_sparse_embeddings'):
                            self.vector_store._sparse_embeddings = self.sparse_embedding
                            self.logger.info("vector_store._sparse_embeddings ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                        elif hasattr(self.vector_store, 'sparse_embedding'):
                            self.vector_store.sparse_embedding = self.sparse_embedding
                            self.logger.info("vector_store.sparse_embedding ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                    # í•™ìŠµ ìƒíƒœ í™•ì¸ ë¡œê·¸
                    if self.sparse_embedding and hasattr(self.sparse_embedding, 'corpus_size'):
                        self.logger.info(f"Sparse ì„ë² ë”© ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: corpus_size={self.sparse_embedding.corpus_size}, vocabulary_size={len(self.sparse_embedding.vocabulary)}")
                    else:
                        self.logger.info("Sparse ì„ë² ë”© ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
                else:
                    # ì¶”ê°€ ë¬¸ì„œ ì—…ë¡œë“œ ì‹œ: ì „ì²´ ë¬¸ì„œë¡œ ì¬í•™ìŠµ (vocabulary ì—…ë°ì´íŠ¸)
                    self.logger.info("ì¶”ê°€ ë¬¸ì„œ ì—…ë¡œë“œ ê°ì§€: Sparse ì„ë² ë”© ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘...")
                    try:
                        # Qdrantì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                        all_documents = self._get_all_documents_from_qdrant()
                        if all_documents:
                            # ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ ì œê±° (Vocabulary í•™ìŠµ ì •í™•ë„ í–¥ìƒ)
                            import hashlib
                            seen_content_hashes = set()
                            unique_documents = []
                            
                            for doc in all_documents:
                                content_hash = hashlib.sha256(doc.page_content.encode('utf-8')).hexdigest()
                                if content_hash not in seen_content_hashes:
                                    seen_content_hashes.add(content_hash)
                                    unique_documents.append(doc)
                            
                            # ê¸°ì¡´ ë¬¸ì„œ (ì¤‘ë³µ ì œê±°ë¨)
                            existing_texts = [doc.page_content for doc in unique_documents]
                            
                            # ìƒˆ ë¬¸ì„œë„ ì¤‘ë³µ ì²´í¬ (ê¸°ì¡´ ë¬¸ì„œì™€ì˜ ì¤‘ë³µ ì œê±°)
                            new_texts = [doc.page_content for doc in langchain_docs]
                            unique_new_texts = []
                            for text in new_texts:
                                content_hash = hashlib.sha256(text.encode('utf-8')).hexdigest()
                                if content_hash not in seen_content_hashes:
                                    seen_content_hashes.add(content_hash)
                                    unique_new_texts.append(text)
                            
                            all_texts = existing_texts + unique_new_texts
                            
                            if len(unique_new_texts) < len(new_texts):
                                self.logger.info(f"ì¤‘ë³µ ë¬¸ì„œ ì œê±°: {len(new_texts) - len(unique_new_texts)}ê°œ ì¤‘ë³µ ë¬¸ì„œ ì œì™¸")
                            
                            self.logger.info(f"ì „ì²´ ë¬¸ì„œë¡œ ì¬í•™ìŠµ: ê¸°ì¡´ {len(existing_texts)}ê°œ (ì¤‘ë³µ ì œê±°ë¨) + ìƒˆ {len(unique_new_texts)}ê°œ = ì´ {len(all_texts)}ê°œ")
                            # ì¬í•™ìŠµ (vocabulary ì—…ë°ì´íŠ¸)
                            self.sparse_embedding_manager.fit(all_texts, include_doc_stats=self.sparse_include_doc_stats)
                            # í•™ìŠµ í›„ sparse_embedding ì°¸ì¡° ì—…ë°ì´íŠ¸
                            self.sparse_embedding = self.sparse_embedding_manager.get_sparse_embedding()
                            # vector_storeì˜ sparse_embeddingë„ ì—…ë°ì´íŠ¸
                            if self.vector_store:
                                if hasattr(self.vector_store, '_sparse_embeddings'):
                                    self.vector_store._sparse_embeddings = self.sparse_embedding
                                    self.logger.info("vector_store._sparse_embeddings ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                                elif hasattr(self.vector_store, 'sparse_embedding'):
                                    self.vector_store.sparse_embedding = self.sparse_embedding
                                    self.logger.info("vector_store.sparse_embedding ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                            # í•™ìŠµ ìƒíƒœ í™•ì¸ ë¡œê·¸
                            if self.sparse_embedding and hasattr(self.sparse_embedding, 'corpus_size'):
                                self.logger.info(f"Sparse ì„ë² ë”© ëª¨ë¸ ì¬í•™ìŠµ ì™„ë£Œ: corpus_size={self.sparse_embedding.corpus_size}, vocabulary_size={len(self.sparse_embedding.vocabulary)}")
                            else:
                                self.logger.info("Sparse ì„ë² ë”© ëª¨ë¸ ì¬í•™ìŠµ ì™„ë£Œ (vocabulary ì—…ë°ì´íŠ¸ë¨)")
                        else:
                            # Qdrantì—ì„œ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìœ¼ë©´ í˜„ì¬ ë¬¸ì„œë§Œìœ¼ë¡œ ì¬í•™ìŠµ
                            self.logger.warning("Qdrantì—ì„œ ê¸°ì¡´ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ í˜„ì¬ ë¬¸ì„œë§Œìœ¼ë¡œ ì¬í•™ìŠµí•©ë‹ˆë‹¤.")
                            document_texts = [doc.page_content for doc in langchain_docs]
                            self.sparse_embedding_manager.fit(document_texts, include_doc_stats=self.sparse_include_doc_stats)
                            # í•™ìŠµ í›„ ì—…ë°ì´íŠ¸
                            self.sparse_embedding = self.sparse_embedding_manager.get_sparse_embedding()
                            if self.vector_store:
                                if hasattr(self.vector_store, '_sparse_embeddings'):
                                    self.vector_store._sparse_embeddings = self.sparse_embedding
                                elif hasattr(self.vector_store, 'sparse_embedding'):
                                    self.vector_store.sparse_embedding = self.sparse_embedding
                    except Exception as e:
                        self.logger.warning(f"Sparse ì„ë² ë”© ëª¨ë¸ ì¬í•™ìŠµ ì‹¤íŒ¨: {str(e)}. í˜„ì¬ ë¬¸ì„œë§Œìœ¼ë¡œ ì¬í•™ìŠµí•©ë‹ˆë‹¤.")
                        document_texts = [doc.page_content for doc in langchain_docs]
                        self.sparse_embedding_manager.fit(document_texts, include_doc_stats=self.sparse_include_doc_stats)
                        # í•™ìŠµ í›„ ì—…ë°ì´íŠ¸
                        self.sparse_embedding = self.sparse_embedding_manager.get_sparse_embedding()
                        if self.vector_store:
                            if hasattr(self.vector_store, '_sparse_embeddings'):
                                self.vector_store._sparse_embeddings = self.sparse_embedding
                            elif hasattr(self.vector_store, 'sparse_embedding'):
                                self.vector_store.sparse_embedding = self.sparse_embedding
            
            if force_update:
                # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ í›„ ìƒˆë¡œ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
                self.logger.info("ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ í›„ ìƒˆë¡œ ì¶”ê°€")
                # LangChain-QdrantëŠ” upsertë¥¼ ì§€ì›í•˜ë¯€ë¡œ ì¤‘ë³µ ìë™ ì²˜ë¦¬
                # HYBRID ëª¨ë“œì¼ ë•Œ ìë™ìœ¼ë¡œ dense+sparse ë²¡í„° ìƒì„± ë° ì €ì¥
                self.vector_store.add_documents(langchain_docs)
            else:
                # ì¼ë°˜ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€ ë¡œì§ ì ìš©)
                # HYBRID ëª¨ë“œì¼ ë•Œ ìë™ìœ¼ë¡œ dense+sparse ë²¡í„° ìƒì„± ë° ì €ì¥
                self.vector_store.add_documents(langchain_docs)
            
            self.logger.info(f"ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ: {len(langchain_docs)}ê°œ (ì¤‘ë³µ ì œê±°: {len(documents) - len(langchain_docs)}ê°œ)")
            if self.sparse_enabled:
                self.logger.info("Dense + Sparse ë²¡í„°ê°€ í•¨ê»˜ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
            return True
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False
    
    def replace_document_vectors(self, file_path: str, new_chunks: List[DocumentChunk]) -> bool:
        """íŠ¹ì • íŒŒì¼ì˜ ë²¡í„°ë¥¼ ì™„ì „íˆ êµì²´"""
        try:
            self.logger.info(f"íŒŒì¼ ë²¡í„° êµì²´ ì‹œì‘: {file_path}")
            
            # 1. ê¸°ì¡´ ë²¡í„° ì‚­ì œ
            delete_success = self._delete_document_vectors(file_path)
            if not delete_success:
                self.logger.warning(f"ê¸°ì¡´ ë²¡í„° ì‚­ì œ ì‹¤íŒ¨, ìƒˆ ë²¡í„°ë§Œ ì¶”ê°€: {file_path}")
            
            # 2. ìƒˆ ë²¡í„° ì¶”ê°€
            add_success = self.add_documents(new_chunks, force_update=False)
            
            if add_success:
                self.logger.info(f"íŒŒì¼ ë²¡í„° êµì²´ ì™„ë£Œ: {file_path}, ì²­í¬ ìˆ˜: {len(new_chunks)}")
                return True
            else:
                self.logger.error(f"íŒŒì¼ ë²¡í„° êµì²´ ì‹¤íŒ¨: {file_path}")
                return False
                
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ë²¡í„° êµì²´ ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {str(e)}")
            return False
    
    def _delete_document_vectors(self, file_path: str) -> bool:
        """íŠ¹ì • íŒŒì¼ì˜ ëª¨ë“  ë²¡í„° ì‚­ì œ"""
        try:
            # Qdrantì—ì„œ í•´ë‹¹ íŒŒì¼ì˜ ëª¨ë“  í¬ì¸íŠ¸ ì‚­ì œ
            result = self.client.delete(
                collection_name=self.collection_name,
                points_selector=Filter(
                    must=[
                        FieldCondition(
                            key="metadata.source_file",
                            match=MatchValue(value=file_path)
                        )
                    ]
                )
            )
            
            self.logger.info(f"íŒŒì¼ ë²¡í„° ì‚­ì œ ì™„ë£Œ: {file_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ë²¡í„° ì‚­ì œ ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {str(e)}")
            return False
    
    def search_by_table_title(self, 
                             table_title: str, 
                             limit: Optional[int] = None,
                             score_threshold: Optional[float] = None) -> List[Dict[str, Any]]:
        """
        í‘œ ì œëª©ìœ¼ë¡œ ê²€ìƒ‰ (ë ˆê±°ì‹œ í˜¸í™˜ì„± ìœ ì§€)
        
        Note: ë‚´ë¶€ì ìœ¼ë¡œ search_with_table_filterë¥¼ í˜¸ì¶œí•˜ì—¬ ì¤‘ë³µ ì½”ë“œ ì œê±°
        """
        # search_with_table_filterë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ ê¸°ëŠ¥ êµ¬í˜„
        return self.search_with_table_filter(
            query=table_title,  # í‘œ ì œëª©ì„ ì¿¼ë¦¬ë¡œ ì‚¬ìš©
            table_title=table_title,  # í•„í„°ë¡œë„ ì‚¬ìš©
            limit=limit,
            score_threshold=score_threshold
        )
    
    def search_with_table_filter(self, 
                                query: str, 
                                table_title: Optional[str] = None,
                                is_table_data: Optional[bool] = None,
                                limit: Optional[int] = None,
                                score_threshold: Optional[float] = None) -> List[Dict[str, Any]]:
        """í‘œ ê´€ë ¨ í•„í„°ì™€ í•¨ê»˜ ê²€ìƒ‰"""
        if not self._check_connection():
            return []
        
        if self.vector_store is None:
            self.logger.error("LangChain Qdrant ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return []
        
        # ê¸°ë³¸ê°’ ì ìš©
        limit = limit if limit is not None else self.default_limit
        # score_thresholdëŠ” í˜¸ì¶œí•˜ëŠ” ìª½ì—ì„œ í•­ìƒ ì „ë‹¬ë˜ë¯€ë¡œ None ì²´í¬ë§Œ ìˆ˜í–‰
        if score_threshold is None:
            # RAG ì„¤ì •ì—ì„œ ê¸°ë³¸ê°’ ê°€ì ¸ì˜¤ê¸° (í˜¸í™˜ì„±)
            from src.utils.config import get_rag_config
            rag_config = get_rag_config()
            score_threshold = rag_config.score_threshold
        
        try:
            filter_conditions = None
            
            # í•„í„° ì¡°ê±´ êµ¬ì„±
            if table_title or is_table_data is not None:
                filter_conditions = {"must": []}
                
                if table_title:
                    filter_conditions["must"].append({
                        "key": "table_title",
                        "match": {"value": table_title}
                    })
                
                if is_table_data is not None:
                    filter_conditions["must"].append({
                        "key": "is_table_data",
                        "match": {"value": is_table_data}
                    })
            
            # ê²€ìƒ‰ ëª¨ë“œ ë° ì„¤ì • í™•ì¸ ë° ë¡œê¹…
            retrieval_mode = getattr(self.vector_store, 'retrieval_mode', None)
            sparse_embedding_available = self.sparse_embedding is not None
            
            # vector_storeì—ì„œ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” sparse_embedding í™•ì¸
            vector_store_sparse_embedding = None
            if self.vector_store:
                if hasattr(self.vector_store, '_sparse_embeddings'):
                    vector_store_sparse_embedding = self.vector_store._sparse_embeddings
                elif hasattr(self.vector_store, 'sparse_embedding'):
                    vector_store_sparse_embedding = getattr(self.vector_store, 'sparse_embedding', None)
            
            # í•™ìŠµ ìƒíƒœ í™•ì¸
            sparse_model_trained = False
            corpus_size = 0
            vocabulary_size = 0
            if vector_store_sparse_embedding and hasattr(vector_store_sparse_embedding, 'corpus_size'):
                corpus_size = vector_store_sparse_embedding.corpus_size
                vocabulary_size = len(vector_store_sparse_embedding.vocabulary) if hasattr(vector_store_sparse_embedding, 'vocabulary') else 0
                sparse_model_trained = corpus_size > 0
            
            self.logger.info(f"=== Qdrant ê²€ìƒ‰ ì‹œì‘ ===")
            self.logger.info(f"ì¿¼ë¦¬: {query[:100]}...")
            self.logger.info(f"Sparse ë²¡í„° í™œì„±í™”: {self.sparse_enabled}")
            self.logger.info(f"Sparse ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥: {sparse_embedding_available}")
            self.logger.info(f"Sparse ëª¨ë¸ í•™ìŠµ ìƒíƒœ: {sparse_model_trained} (corpus_size={corpus_size}, vocabulary_size={vocabulary_size})")
            self.logger.info(f"RetrievalMode: {retrieval_mode}")
            
            if self.sparse_enabled and retrieval_mode == RetrievalMode.HYBRID:
                self.logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ëª¨ë“œ: Dense + Sparse ë²¡í„° ëª¨ë‘ ì‚¬ìš©")
                self.logger.info(f"   - Dense ê°€ì¤‘ì¹˜: {self.hybrid_search_dense_weight}")
                self.logger.info(f"   - Sparse ê°€ì¤‘ì¹˜: {self.hybrid_search_sparse_weight}")
            elif self.sparse_enabled and retrieval_mode == RetrievalMode.DENSE:
                self.logger.warning(f"âš ï¸  Sparse ë²¡í„° í™œì„±í™”ë˜ì—ˆì§€ë§Œ DENSE ëª¨ë“œë¡œ ê²€ìƒ‰ ì¤‘ (Sparse ë²¡í„° ë¯¸ì‚¬ìš©)")
            elif not self.sparse_enabled:
                self.logger.info(f"â„¹ï¸  Dense ë²¡í„°ë§Œ ì‚¬ìš© (Sparse ë²¡í„° ë¹„í™œì„±í™”)")
            else:
                self.logger.warning(f"âš ï¸  RetrievalModeë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŒ: {retrieval_mode}")
            
            if filter_conditions:
                self.logger.debug(f"í•„í„° ì¡°ê±´ ì ìš©: {filter_conditions}")
                docs = self.vector_store.similarity_search_with_relevance_scores(
                    query=query,
                    k=limit,
                    filter=filter_conditions
                )
            else:
                docs = self.vector_store.similarity_search_with_relevance_scores(
                    query=query,
                    k=limit
                )
            
            self.logger.info(f"ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ ë°˜í™˜")
            
            # ê²°ê³¼ ë³€í™˜
            results = []
            for doc, score in docs:
                similarity_score = float(score)
                
                results.append({
                    'content': doc.page_content,
                    'score': similarity_score,
                    'metadata': doc.metadata,
                    'source_file': doc.metadata.get('source_file', ''),
                    'chunk_index': doc.metadata.get('chunk_index', 0),
                    'table_title': doc.metadata.get('table_title', ''),
                    'is_table_data': doc.metadata.get('is_table_data', False)
                })
            
            # ì ìˆ˜ ì„ê³„ê°’ í•„í„°ë§
            if score_threshold > 0:
                before_filter_count = len(results)
                results = [r for r in results if r['score'] >= score_threshold]
                filtered_out = before_filter_count - len(results)
                if filtered_out > 0:
                    self.logger.info(f"ì ìˆ˜ ì„ê³„ê°’({score_threshold:.3f}) í•„í„°ë§: {before_filter_count}ê°œ â†’ {len(results)}ê°œ (ì œì™¸: {filtered_out}ê°œ)")
            
            # ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë¡œê·¸
            self.logger.info(f"=== Qdrant ê²€ìƒ‰ ì™„ë£Œ ===")
            self.logger.info(f"ìµœì¢… ê²°ê³¼: {len(results)}ê°œ (ì ìˆ˜ ì„ê³„ê°’ í•„í„°ë§ í›„)")
            if self.sparse_enabled and retrieval_mode == RetrievalMode.HYBRID:
                self.logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: Dense + Sparse ë²¡í„° í†µí•© ê²°ê³¼")
            if results:
                self.logger.info(f"ìƒìœ„ ê²°ê³¼ ìš”ì•½ (ìµœëŒ€ 3ê°œ):")
                for i, result in enumerate(results[:3], 1):
                    source_file = result.get('source_file', '')
                    filename = os.path.basename(source_file) if source_file else 'N/A'
                    chunk_idx = result.get('chunk_index', 'N/A')
                    table_title = result.get('table_title', '')
                    is_table = result.get('is_table_data', False)
                    score = result.get('score', 0.0)
                    content_preview = result.get('content', '')[:50].replace('\n', ' ') + '...' if result.get('content') else 'N/A'
                    
                    table_info = f", í‘œì œëª©: {table_title}" if table_title else ""
                    table_type = ", í‘œë°ì´í„°" if is_table else ""
                    
                    self.logger.info(
                        f"  [{i}] ì ìˆ˜: {score:.4f} | íŒŒì¼: {filename} | ì²­í¬#{chunk_idx}"
                        f"{table_info}{table_type} | ë‚´ìš©: {content_preview}"
                    )
            else:
                self.logger.info("  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ì ìˆ˜ ì„ê³„ê°’ ë¯¸ë‹¬ ë˜ëŠ” ë§¤ì¹­ ë¬¸ì„œ ì—†ìŒ)")
            
            return results
            
        except Exception as e:
            self.logger.error(f"í•„í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    # ========== ë¹„ë™ê¸° ë©”ì„œë“œ (Phase 2: ë²¡í„° ê²€ìƒ‰ ë¹„ë™ê¸°í™”) ==========
    
    async def search_with_table_filter_async(self, 
                                            query: str, 
                                            table_title: Optional[str] = None,
                                            is_table_data: Optional[bool] = None,
                                            limit: Optional[int] = None,
                                            score_threshold: Optional[float] = None,
                                            dense_weight: Optional[float] = None,
                                            sparse_weight: Optional[float] = None) -> List[Dict[str, Any]]:
        """ë¹„ë™ê¸° í‘œ ê´€ë ¨ í•„í„°ì™€ í•¨ê»˜ ê²€ìƒ‰"""
        if not self._check_connection():
            return []
        
        if self.vector_store is None:
            self.logger.error("LangChain Qdrant ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return []
        
        # ê¸°ë³¸ê°’ ì ìš©
        limit = limit if limit is not None else self.default_limit
        if score_threshold is None:
            from src.utils.config import get_rag_config
            rag_config = get_rag_config()
            score_threshold = rag_config.score_threshold
        
        try:
            filter_conditions = None
            
            # í•„í„° ì¡°ê±´ êµ¬ì„±
            if table_title or is_table_data is not None:
                filter_conditions = {"must": []}
                
                if table_title:
                    filter_conditions["must"].append({
                        "key": "table_title",
                        "match": {"value": table_title}
                    })
                
                if is_table_data is not None:
                    filter_conditions["must"].append({
                        "key": "is_table_data",
                        "match": {"value": is_table_data}
                    })
            
            # ê²€ìƒ‰ ëª¨ë“œ ë° ì„¤ì • í™•ì¸ ë° ë¡œê¹…
            retrieval_mode = getattr(self.vector_store, 'retrieval_mode', None)
            sparse_embedding_available = self.sparse_embedding is not None
            
            # vector_storeì—ì„œ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” sparse_embedding í™•ì¸
            vector_store_sparse_embedding = None
            if self.vector_store:
                if hasattr(self.vector_store, '_sparse_embeddings'):
                    vector_store_sparse_embedding = self.vector_store._sparse_embeddings
                elif hasattr(self.vector_store, 'sparse_embedding'):
                    vector_store_sparse_embedding = getattr(self.vector_store, 'sparse_embedding', None)
            
            # í•™ìŠµ ìƒíƒœ í™•ì¸
            sparse_model_trained = False
            corpus_size = 0
            vocabulary_size = 0
            if vector_store_sparse_embedding and hasattr(vector_store_sparse_embedding, 'corpus_size'):
                corpus_size = vector_store_sparse_embedding.corpus_size
                vocabulary_size = len(vector_store_sparse_embedding.vocabulary) if hasattr(vector_store_sparse_embedding, 'vocabulary') else 0
                sparse_model_trained = corpus_size > 0
            
            self.logger.info(f"=== Qdrant ë¹„ë™ê¸° ê²€ìƒ‰ ì‹œì‘ ===")
            self.logger.info(f"ì¿¼ë¦¬: {query[:100]}...")
            self.logger.info(f"Sparse ë²¡í„° í™œì„±í™”: {self.sparse_enabled}")
            self.logger.info(f"Sparse ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥: {sparse_embedding_available}")
            self.logger.info(f"Sparse ëª¨ë¸ í•™ìŠµ ìƒíƒœ: {sparse_model_trained} (corpus_size={corpus_size}, vocabulary_size={vocabulary_size})")
            self.logger.info(f"RetrievalMode: {retrieval_mode}")
            
            # ê°€ì¤‘ì¹˜ ì ìš© (APIì—ì„œ ì œê³µëœ ê²½ìš° ì‚¬ìš©, ì—†ìœ¼ë©´ config ê¸°ë³¸ê°’)
            effective_dense_weight = dense_weight if dense_weight is not None else self.hybrid_search_dense_weight
            effective_sparse_weight = sparse_weight if sparse_weight is not None else self.hybrid_search_sparse_weight
            
            if self.sparse_enabled and retrieval_mode == RetrievalMode.HYBRID:
                self.logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ëª¨ë“œ: Dense + Sparse ë²¡í„° ëª¨ë‘ ì‚¬ìš©")
                self.logger.info(f"   - Dense ê°€ì¤‘ì¹˜: {effective_dense_weight} {'(API ì œê³µ)' if dense_weight is not None else '(config ê¸°ë³¸ê°’)'}")
                self.logger.info(f"   - Sparse ê°€ì¤‘ì¹˜: {effective_sparse_weight} {'(API ì œê³µ)' if sparse_weight is not None else '(config ê¸°ë³¸ê°’)'}")
                
                # ë™ì  ê°€ì¤‘ì¹˜ ì ìš©: Qdrant í´ë¼ì´ì–¸íŠ¸ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
                if dense_weight is not None or sparse_weight is not None:
                    self.logger.info(f"ğŸ”„ ë™ì  ê°€ì¤‘ì¹˜ ì ìš©: Qdrant í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ ì‚¬ìš©")
                    docs = await self._hybrid_search_with_weights(
                        query=query,
                        limit=limit,
                        filter_conditions=filter_conditions,
                        dense_weight=effective_dense_weight,
                        sparse_weight=effective_sparse_weight
                    )
                else:
                    # ê°€ì¤‘ì¹˜ê°€ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ LangChain QdrantVectorStore ì‚¬ìš©
                    docs = await self._search_with_langchain(
                        query=query,
                        limit=limit,
                        filter_conditions=filter_conditions
                    )
            elif self.sparse_enabled and retrieval_mode == RetrievalMode.DENSE:
                self.logger.warning(f"âš ï¸  Sparse ë²¡í„° í™œì„±í™”ë˜ì—ˆì§€ë§Œ DENSE ëª¨ë“œë¡œ ê²€ìƒ‰ ì¤‘ (Sparse ë²¡í„° ë¯¸ì‚¬ìš©)")
                docs = await self._search_with_langchain(
                    query=query,
                    limit=limit,
                    filter_conditions=filter_conditions
                )
            elif not self.sparse_enabled:
                self.logger.info(f"â„¹ï¸  Dense ë²¡í„°ë§Œ ì‚¬ìš© (Sparse ë²¡í„° ë¹„í™œì„±í™”)")
                docs = await self._search_with_langchain(
                    query=query,
                    limit=limit,
                    filter_conditions=filter_conditions
                )
            else:
                self.logger.warning(f"âš ï¸  RetrievalModeë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŒ: {retrieval_mode}")
                docs = await self._search_with_langchain(
                    query=query,
                    limit=limit,
                    filter_conditions=filter_conditions
                )
            
            # ê²°ê³¼ ë³€í™˜
            results = []
            for doc, score in docs:
                similarity_score = float(score)
                
                results.append({
                    'content': doc.page_content,
                    'score': similarity_score,
                    'metadata': doc.metadata,
                    'source_file': doc.metadata.get('source_file', ''),
                    'chunk_index': doc.metadata.get('chunk_index', 0),
                    'table_title': doc.metadata.get('table_title', ''),
                    'is_table_data': doc.metadata.get('is_table_data', False)
                })
            
            # ì ìˆ˜ ì„ê³„ê°’ í•„í„°ë§
            if score_threshold > 0:
                before_filter_count = len(results)
                results = [r for r in results if r['score'] >= score_threshold]
                filtered_out = before_filter_count - len(results)
                if filtered_out > 0:
                    self.logger.info(f"ë¹„ë™ê¸° ì ìˆ˜ ì„ê³„ê°’({score_threshold:.3f}) í•„í„°ë§: {before_filter_count}ê°œ â†’ {len(results)}ê°œ (ì œì™¸: {filtered_out}ê°œ)")
            
            self.logger.info(f"=== Qdrant ë¹„ë™ê¸° ê²€ìƒ‰ ì™„ë£Œ ===")
            self.logger.info(f"ìµœì¢… ê²°ê³¼: {len(results)}ê°œ (ì ìˆ˜ ì„ê³„ê°’ í•„í„°ë§ í›„)")
            if self.sparse_enabled and retrieval_mode == RetrievalMode.HYBRID:
                self.logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: Dense + Sparse ë²¡í„° í†µí•© ê²°ê³¼")
            return results
            
        except Exception as e:
            self.logger.error(f"ë¹„ë™ê¸° í•„í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    async def search_similar_async(self, 
                                  query: str, 
                                  limit: Optional[int] = None,
                                  score_threshold: Optional[float] = None,
                                  filter_conditions: Optional[Dict[str, Any]] = None,
                                  dense_weight: Optional[float] = None,
                                  sparse_weight: Optional[float] = None) -> List[Dict[str, Any]]:
        """
        ë¹„ë™ê¸° ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            limit: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            score_threshold: ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’
            filter_conditions: í•„í„° ì¡°ê±´ (í˜„ì¬ ë¯¸ì‚¬ìš©, í˜¸í™˜ì„± ìœ ì§€)
            dense_weight: Dense ë²¡í„° ê°€ì¤‘ì¹˜ (Noneì´ë©´ config ê¸°ë³¸ê°’ ì‚¬ìš©)
            sparse_weight: Sparse ë²¡í„° ê°€ì¤‘ì¹˜ (Noneì´ë©´ config ê¸°ë³¸ê°’ ì‚¬ìš©)
        """
        return await self.search_with_table_filter_async(
            query=query,
            limit=limit,
            score_threshold=score_threshold,
            dense_weight=dense_weight,
            sparse_weight=sparse_weight
        )
    
    async def _search_with_langchain(self, query: str, limit: int, filter_conditions: Optional[Dict[str, Any]] = None):
        """LangChain QdrantVectorStoreë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰ (ê°€ì¤‘ì¹˜ ë¯¸ì§€ì›)"""
        import asyncio
        
        if self.use_local_storage:
            if filter_conditions:
                return await asyncio.to_thread(
                    self.vector_store.similarity_search_with_relevance_scores,
                    query=query,
                    k=limit,
                    filter=filter_conditions
                )
            else:
                return await asyncio.to_thread(
                    self.vector_store.similarity_search_with_relevance_scores,
                    query=query,
                    k=limit
                )
        else:
            if hasattr(self.vector_store, 'asimilarity_search_with_relevance_scores'):
                if filter_conditions:
                    return await self.vector_store.asimilarity_search_with_relevance_scores(
                        query=query,
                        k=limit,
                        filter=filter_conditions
                    )
                else:
                    return await self.vector_store.asimilarity_search_with_relevance_scores(
                        query=query,
                        k=limit
                    )
            else:
                if filter_conditions:
                    return await asyncio.to_thread(
                        self.vector_store.similarity_search_with_relevance_scores,
                        query=query,
                        k=limit,
                        filter=filter_conditions
                    )
                else:
                    return await asyncio.to_thread(
                        self.vector_store.similarity_search_with_relevance_scores,
                        query=query,
                        k=limit
                    )
    
    async def _hybrid_search_with_weights(self, 
                                         query: str, 
                                         limit: int,
                                         filter_conditions: Optional[Dict[str, Any]] = None,
                                         dense_weight: float = 0.7,
                                         sparse_weight: float = 0.3) -> List[tuple]:
        """
        Qdrant í´ë¼ì´ì–¸íŠ¸ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ë™ì  ê°€ì¤‘ì¹˜ë¡œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
        
        Returns:
            List[tuple]: (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        import asyncio
        
        try:
            # Dense ë²¡í„° ìƒì„±
            dense_vector = await asyncio.to_thread(self.embeddings.embed_query, query)
            
            # Sparse ë²¡í„° ìƒì„±
            sparse_vector_obj = None
            if self.sparse_embedding:
                sparse_vector_obj = await asyncio.to_thread(self.sparse_embedding.embed_query, query)
            
            # Qdrant Query êµ¬ì„±
            query_vector = None
            sparse_query = None
            
            if dense_vector and sparse_vector_obj:
                # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: Dense + Sparse
                query_vector = dense_vector
                # SparseVector ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                sparse_vector_dict = {
                    "indices": sparse_vector_obj.indices,
                    "values": sparse_vector_obj.values
                }
                sparse_query = NamedSparseVector(
                    name=self.sparse_vector_name,
                    vector=sparse_vector_dict
                )
            elif dense_vector:
                # Denseë§Œ ì‚¬ìš©
                query_vector = dense_vector
            elif sparse_vector_obj:
                # Sparseë§Œ ì‚¬ìš©
                # SparseVector ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                sparse_vector_dict = {
                    "indices": sparse_vector_obj.indices,
                    "values": sparse_vector_obj.values
                }
                sparse_query = NamedSparseVector(
                    name=self.sparse_vector_name,
                    vector=sparse_vector_dict
                )
            else:
                self.logger.error("Denseì™€ Sparse ë²¡í„° ëª¨ë‘ ìƒì„± ì‹¤íŒ¨")
                return []
            
            # Qdrant í´ë¼ì´ì–¸íŠ¸ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
            # Prefetchë¥¼ ì‚¬ìš©í•˜ì—¬ Denseì™€ Sparse ê²€ìƒ‰ì„ ê°ê° ìˆ˜í–‰í•œ í›„ ê°€ì¤‘ì¹˜ë¡œ ê²°í•©
            self.logger.info(f"ğŸ”„ ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (Dense={dense_weight}, Sparse={sparse_weight})")
            
            # í•„í„° êµ¬ì„±
            qdrant_filter = None
            if filter_conditions:
                must_conditions = []
                for condition in filter_conditions.get("must", []):
                    key = condition.get("key")
                    match_value = condition.get("match", {}).get("value")
                    if key and match_value is not None:
                        must_conditions.append(
                            FieldCondition(key=key, match=MatchValue(value=match_value))
                        )
                if must_conditions:
                    qdrant_filter = Filter(must=must_conditions)
            
            # Prefetchë¥¼ ì‚¬ìš©í•˜ì—¬ Denseì™€ Sparse ê²€ìƒ‰ ê°ê° ìˆ˜í–‰
            # QdrantëŠ” prefetch ê²°ê³¼ë¥¼ ìë™ìœ¼ë¡œ ê²°í•©í•˜ì§€ë§Œ, ê°€ì¤‘ì¹˜ë¥¼ ì§ì ‘ ì ìš©í•˜ë ¤ë©´
            # ê°ê° ê²€ìƒ‰ í›„ ìˆ˜ë™ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ê²°í•©í•´ì•¼ í•¨
            import asyncio
            
            # Dense ë²¡í„° ê²€ìƒ‰
            dense_results = None
            if query_vector:
                if self.use_local_storage:
                    dense_results = await asyncio.to_thread(
                        self.client.query_points,
                        collection_name=self.collection_name,
                        query=query_vector,
                        using="",  # ê¸°ë³¸ ë²¡í„° ì‚¬ìš©
                        limit=limit * 2,  # ê°€ì¤‘ì¹˜ ê²°í•©ì„ ìœ„í•´ ë” ë§ì€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                        query_filter=qdrant_filter,
                        with_payload=True,
                        with_vectors=False
                    )
                else:
                    if not hasattr(self, '_async_client') or self._async_client is None:
                        self._async_client = AsyncQdrantClient(host=self._async_client_host, port=self._async_client_port)
                    dense_results = await self._async_client.query_points(
                        collection_name=self.collection_name,
                        query=query_vector,
                        using="",
                        limit=limit * 2,
                        query_filter=qdrant_filter,
                        with_payload=True,
                        with_vectors=False
                    )
            
            # Sparse ë²¡í„° ê²€ìƒ‰
            sparse_results = None
            if sparse_vector_obj:
                sparse_vector_qdrant = QdrantSparseVector(
                    indices=sparse_vector_obj.indices,
                    values=sparse_vector_obj.values
                )
                if self.use_local_storage:
                    sparse_results = await asyncio.to_thread(
                        self.client.query_points,
                        collection_name=self.collection_name,
                        query=sparse_vector_qdrant,
                        using=self.sparse_vector_name,
                        limit=limit * 2,
                        query_filter=qdrant_filter,
                        with_payload=True,
                        with_vectors=False
                    )
                else:
                    if not hasattr(self, '_async_client') or self._async_client is None:
                        self._async_client = AsyncQdrantClient(host=self._async_client_host, port=self._async_client_port)
                    sparse_results = await self._async_client.query_points(
                        collection_name=self.collection_name,
                        query=sparse_vector_qdrant,
                        using=self.sparse_vector_name,
                        limit=limit * 2,
                        query_filter=qdrant_filter,
                        with_payload=True,
                        with_vectors=False
                    )
            
            # ê°€ì¤‘ì¹˜ë¡œ ê²°ê³¼ ê²°í•©
            combined_results = {}
            
            # Dense ê²°ê³¼ ì²˜ë¦¬
            if dense_results and dense_results.points:
                for point in dense_results.points:
                    point_id = str(point.id)
                    if point_id not in combined_results:
                        combined_results[point_id] = {
                            'point': point,
                            'dense_score': point.score if hasattr(point, 'score') else 0.0,
                            'sparse_score': 0.0,
                            'combined_score': 0.0
                        }
                    else:
                        combined_results[point_id]['dense_score'] = point.score if hasattr(point, 'score') else 0.0
            
            # Sparse ê²°ê³¼ ì²˜ë¦¬
            if sparse_results and sparse_results.points:
                for point in sparse_results.points:
                    point_id = str(point.id)
                    if point_id not in combined_results:
                        combined_results[point_id] = {
                            'point': point,
                            'dense_score': 0.0,
                            'sparse_score': point.score if hasattr(point, 'score') else 0.0,
                            'combined_score': 0.0
                        }
                    else:
                        combined_results[point_id]['sparse_score'] = point.score if hasattr(point, 'score') else 0.0
            
            # ê°€ì¤‘ì¹˜ë¡œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
            for point_id, result in combined_results.items():
                dense_score = result['dense_score']
                sparse_score = result['sparse_score']
                
                # ê°€ì¤‘ì¹˜ ê²°í•©: (Dense ì ìˆ˜ Ã— dense_weight) + (Sparse ì ìˆ˜ Ã— sparse_weight)
                # ì ìˆ˜ê°€ 0ì¸ ê²½ìš° í•´ë‹¹ ê²€ìƒ‰ì—ì„œ ë°œê²¬ë˜ì§€ ì•Šì€ ê²ƒì´ë¯€ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •
                if dense_score > 0 and sparse_score > 0:
                    # ë‘˜ ë‹¤ ë°œê²¬ëœ ê²½ìš°: ê°€ì¤‘ì¹˜ ê·¸ëŒ€ë¡œ ì ìš©
                    combined_score = (dense_score * dense_weight) + (sparse_score * sparse_weight)
                elif dense_score > 0:
                    # Denseë§Œ ë°œê²¬ëœ ê²½ìš°: Dense ê°€ì¤‘ì¹˜ë§Œ ì ìš© (ì •ê·œí™”)
                    combined_score = dense_score * dense_weight / (dense_weight + sparse_weight) if (dense_weight + sparse_weight) > 0 else dense_score
                elif sparse_score > 0:
                    # Sparseë§Œ ë°œê²¬ëœ ê²½ìš°: Sparse ê°€ì¤‘ì¹˜ë§Œ ì ìš© (ì •ê·œí™”)
                    combined_score = sparse_score * sparse_weight / (dense_weight + sparse_weight) if (dense_weight + sparse_weight) > 0 else sparse_score
                else:
                    combined_score = 0.0
                
                result['combined_score'] = combined_score
            
            # ìµœì¢… ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            sorted_results = sorted(
                combined_results.values(),
                key=lambda x: x['combined_score'],
                reverse=True
            )[:limit]
            
            # LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            docs = []
            for result in sorted_results:
                point = result['point']
                payload = point.payload or {}
                # LangChain QdrantVectorStoreëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 'page_content' í‚¤ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
                # ìš°ë¦¬ê°€ ì €ì¥í•  ë•ŒëŠ” DocumentChunkì˜ contentë¥¼ ê·¸ëŒ€ë¡œ ì €ì¥í•˜ë¯€ë¡œ
                # payloadì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜, metadataì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
                # LangChainì˜ ê¸°ë³¸ ë™ì‘ì„ ë”°ë¼ 'page_content' í‚¤ë¥¼ ë¨¼ì € í™•ì¸
                page_content = payload.get('page_content', '')
                if not page_content:
                    # 'page_content'ê°€ ì—†ìœ¼ë©´ payloadì˜ ë‹¤ë¥¸ í‚¤ë“¤ì„ í™•ì¸
                    # ë˜ëŠ” metadataì—ì„œ ê°€ì ¸ì˜¤ê¸°
                    for key in ['content', 'text', 'body']:
                        if key in payload:
                            page_content = payload[key]
                            break
                
                doc = Document(
                    page_content=page_content,
                    metadata={
                        'chunk_id': payload.get('chunk_id', ''),
                        'source_file': payload.get('source_file', ''),
                        'chunk_index': payload.get('chunk_index', 0),
                        'table_title': payload.get('table_title', ''),
                        'is_table_data': payload.get('is_table_data', False),
                        **{k: v for k, v in payload.items() 
                           if k not in ['page_content', 'chunk_id', 'source_file', 'chunk_index', 'table_title', 'is_table_data']}
                    }
                )
                # ê²°í•©ëœ ì ìˆ˜ ì‚¬ìš©
                docs.append((doc, result['combined_score']))
            
            self.logger.info(f"âœ… ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(docs)}ê°œ ê²°ê³¼")
            self.logger.info(f"   - Dense ê°€ì¤‘ì¹˜: {dense_weight}, Sparse ê°€ì¤‘ì¹˜: {sparse_weight}")
            if docs:
                self.logger.debug(f"   - ìƒìœ„ ê²°ê³¼ ì ìˆ˜ ë²”ìœ„: {docs[0][1]:.4f} ~ {docs[-1][1]:.4f}")
            return docs
            
        except Exception as e:
            self.logger.error(f"Qdrant ì§ì ‘ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            # í´ë°±: LangChain ì‚¬ìš©
            self.logger.warning("LangChain QdrantVectorStoreë¡œ í´ë°±")
            return await self._search_with_langchain(query, limit, filter_conditions)
    
    def search_similar(self, 
                      query: str, 
                      limit: Optional[int] = None,
                      score_threshold: Optional[float] = None,
                      filter_conditions: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            limit: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            score_threshold: ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’
            filter_conditions: í•„í„° ì¡°ê±´ (í˜„ì¬ ë¯¸ì‚¬ìš©, í˜¸í™˜ì„± ìœ ì§€)
        
        Note: filter_conditionsëŠ” í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ.
             í•„í„°ê°€ í•„ìš”í•œ ê²½ìš° search_with_table_filterë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
        """
        return self.search_with_table_filter(
            query=query,
            limit=limit,
            score_threshold=score_threshold
        )
    
    def get_documents_info(self) -> List[Dict[str, Any]]:
        """ì €ì¥ëœ ë¬¸ì„œë“¤ì˜ ì •ë³´ ë°˜í™˜"""
        try:
            # ëª¨ë“  í¬ì¸íŠ¸ ì¡°íšŒ (ë©”íƒ€ë°ì´í„°ë§Œ)
            points = self.client.scroll(
                collection_name=self.collection_name,
                limit=self.max_scroll_limit,
                with_payload=True,
                with_vectors=False
            )[0]
            
            # ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”
            documents = {}
            for point in points:
                payload = point.payload
                
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
                metadata = self._get_metadata(payload)
                
                # ë‹¤ì–‘í•œ ê°€ëŠ¥í•œ í‚¤ ì´ë¦„ ì‹œë„
                source_file = (metadata.get('source_file') or 
                             metadata.get('source') or 
                             metadata.get('file_path') or 
                             metadata.get('file_name') or 
                             'unknown')
                
                # íŒŒì¼ëª… ì¶”ì¶œ (ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ)
                file_name = metadata.get('file_name', '')
                if not file_name and source_file != 'unknown':
                    # source_fileì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
                    file_name = os.path.basename(source_file)
                
                if source_file not in documents:
                    documents[source_file] = {
                        'source_file': source_file,
                        'file_name': file_name,  # íŒŒì¼ëª… ì¶”ê°€
                        'total_chunks': 0,
                        'first_chunk_index': float('inf'),
                        'last_chunk_index': -1,
                        'file_type': metadata.get('file_type', ''),
                        'file_size': metadata.get('file_size', 0),
                        'upload_time': metadata.get('upload_time', ''),
                        'chunk_ids': []
                    }
                
                documents[source_file]['total_chunks'] += 1
                chunk_index = metadata.get('chunk_index', 0)
                documents[source_file]['first_chunk_index'] = min(
                    documents[source_file]['first_chunk_index'], 
                    chunk_index
                )
                documents[source_file]['last_chunk_index'] = max(
                    documents[source_file]['last_chunk_index'], 
                    chunk_index
                )
                documents[source_file]['chunk_ids'].append(point.id)
            
            # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            result = list(documents.values())
            self.logger.info(f"ë¬¸ì„œ ì •ë³´ ì¡°íšŒ ì™„ë£Œ: {len(result)}ê°œ ë¬¸ì„œ")
            return result
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def get_document_chunks(self, document_id: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ë¬¸ì„œì˜ ì²­í¬ ì •ë³´ ë°˜í™˜"""
        try:
            # ëª¨ë“  ì²­í¬ë¥¼ ê°€ì ¸ì˜¨ í›„ Pythonì—ì„œ í•„í„°ë§
            points = self.client.scroll(
                collection_name=self.collection_name,
                limit=self.max_scroll_limit,
                with_payload=True,
                with_vectors=False
            )[0]
            
            # íŠ¹ì • ë¬¸ì„œì˜ ì²­í¬ë“¤ë§Œ í•„í„°ë§
            filtered_points = []
            for point in points:
                metadata = self._get_metadata(point.payload)
                
                if metadata.get('source_file') == document_id:
                    filtered_points.append(point)
            
            points = filtered_points
            
            chunks = []
            for point in points:
                payload = point.payload
                
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
                metadata = self._get_metadata(payload)
                
                chunks.append({
                    'chunk_id': point.id,
                    'chunk_index': metadata.get('chunk_index', 0),
                    'content_preview': payload.get('page_content', '')[:200] + '...',
                    'content_full': payload.get('page_content', ''),  # ì „ì²´ ë‚´ìš© ì¶”ê°€
                    'content_length': len(payload.get('page_content', '')),
                    'metadata': metadata
                })
            
            # ì²­í¬ ì¸ë±ìŠ¤ ìˆœìœ¼ë¡œ ì •ë ¬
            chunks.sort(key=lambda x: x['chunk_index'])
            
            self.logger.info(f"ë¬¸ì„œ ì²­í¬ ì¡°íšŒ ì™„ë£Œ: {document_id}, {len(chunks)}ê°œ ì²­í¬")
            return chunks
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ì²­í¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _get_all_documents_from_qdrant(self) -> List[Document]:
        """
        Qdrantì—ì„œ ëª¨ë“  ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¤ê¸° (LangChain Document í˜•ì‹)
        
        Returns:
            ëª¨ë“  ë¬¸ì„œì˜ LangChain Document ë¦¬ìŠ¤íŠ¸
        """
        try:
            if not self._check_connection():
                return []
            
            all_documents = []
            offset = None
            
            # Scrollì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  í¬ì¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
            while True:
                scroll_result = self.client.scroll(
                    collection_name=self.collection_name,
                    limit=self.max_scroll_limit,
                    offset=offset,
                    with_payload=True,
                    with_vectors=False  # ë²¡í„°ëŠ” í•„ìš” ì—†ìŒ
                )
                
                points, next_offset = scroll_result
                
                if not points:
                    break
                
                # Pointë¥¼ LangChain Documentë¡œ ë³€í™˜
                for point in points:
                    payload = point.payload or {}
                    page_content = payload.get('page_content', '')
                    
                    if page_content:  # ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                        doc = Document(
                            page_content=page_content,
                            metadata={
                                'chunk_id': payload.get('chunk_id', ''),
                                'source_file': payload.get('source_file', ''),
                                'chunk_index': payload.get('chunk_index', 0),
                                **{k: v for k, v in payload.items() 
                                   if k not in ['page_content', 'chunk_id', 'source_file', 'chunk_index']}
                            }
                        )
                        all_documents.append(doc)
                
                if next_offset is None:
                    break
                
                offset = next_offset
            
            self.logger.debug(f"Qdrantì—ì„œ ì´ {len(all_documents)}ê°œ ë¬¸ì„œ ê°€ì ¸ì˜´")
            return all_documents
            
        except Exception as e:
            self.logger.error(f"Qdrantì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")
            return []
    
    def get_collection_info(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ ì •ë³´ ë°˜í™˜"""
        try:
            collection_info = self.client.get_collection(self.collection_name)
            
            # vectorsê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (ë‹¤ì¤‘ ë²¡í„° ì§€ì›)ì™€ ë‹¨ì¼ ë²¡í„°ì¸ ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
            vectors_config = collection_info.config.params.vectors
            if isinstance(vectors_config, dict):
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°: ê¸°ë³¸ ë²¡í„°("") ë˜ëŠ” ì²« ë²ˆì§¸ ë²¡í„° ì‚¬ìš©
                if "" in vectors_config:
                    vector_params = vectors_config[""]
                else:
                    # ê¸°ë³¸ ë²¡í„°ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë²¡í„° ì‚¬ìš©
                    vector_params = next(iter(vectors_config.values()))
                vector_size = vector_params.size
                distance_metric = vector_params.distance.name
            else:
                # ë‹¨ì¼ ë²¡í„°ì¸ ê²½ìš° (ë ˆê±°ì‹œ)
                vector_size = vectors_config.size
                distance_metric = vectors_config.distance.name
            
            # Sparse ë²¡í„° ì •ë³´ í™•ì¸
            sparse_vectors_info = None
            if hasattr(collection_info.config.params, 'sparse_vectors') and \
               collection_info.config.params.sparse_vectors:
                sparse_vectors = collection_info.config.params.sparse_vectors
                if isinstance(sparse_vectors, dict):
                    sparse_vectors_info = list(sparse_vectors.keys())
            
            result = {
                'name': self.collection_name,
                'vector_size': vector_size,
                'distance_metric': distance_metric,
                'points_count': collection_info.points_count,
                'status': collection_info.status.name
            }
            
            if sparse_vectors_info:
                result['sparse_vectors'] = sparse_vectors_info
            
            return result
        except ValueError as e:
            # ì»¬ë ‰ì…˜ì´ ì—†ëŠ” ê²½ìš° (ì´ˆê¸°í™” ì‹œ ì •ìƒì ì¸ ìƒí™©)
            if "not found" in str(e).lower():
                self.logger.debug(f"ì»¬ë ‰ì…˜ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•ŠìŒ: {self.collection_name} (ì´ˆê¸°í™” ì¤‘ì¼ ìˆ˜ ìˆìŒ)")
                return {}
            else:
                self.logger.error(f"ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
                return {}
        except Exception as e:
            self.logger.error(f"ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {}
    
    def delete_collection(self) -> bool:
        """ì»¬ë ‰ì…˜ ì‚­ì œ"""
        try:
            self.client.delete_collection(self.collection_name)
            self.logger.info(f"ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ: {self.collection_name}")
            return True
        except Exception as e:
            self.logger.error(f"ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    '''ìƒ˜í”Œ ë²¡í„° DBí™•ì¸'''
    def inspect_vectors(self, sample_size: int = 3, point_ids: Optional[List[Any]] = None) -> Dict[str, Any]:
        """
        ìƒ˜í”Œ í¬ì¸íŠ¸ì˜ Denseì™€ Sparse ë²¡í„° í™•ì¸
        
        Args:
            sample_size: í™•ì¸í•  ìƒ˜í”Œ í¬ì¸íŠ¸ ìˆ˜ (point_idsê°€ Noneì¼ ë•Œ)
            point_ids: í™•ì¸í•  íŠ¹ì • í¬ì¸íŠ¸ ID ë¦¬ìŠ¤íŠ¸ (ì§€ì • ì‹œ sample_size ë¬´ì‹œ)
            
        Returns:
            ë²¡í„° ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
            collection_info = self.client.get_collection(self.collection_name)
            
            # ë²¡í„° ì„¤ì • í™•ì¸
            vectors_config = collection_info.config.params.vectors
            sparse_vectors_config = getattr(collection_info.config.params, 'sparse_vectors', None)
            
            result = {
                'collection_name': self.collection_name,
                'points_count': collection_info.points_count,
                'dense_vectors': {},
                'sparse_vectors': {},
                'samples': []
            }
            
            # Dense ë²¡í„° ì„¤ì • ì •ë³´
            if isinstance(vectors_config, dict):
                for vec_name, vec_params in vectors_config.items():
                    result['dense_vectors'][vec_name or '(default)'] = {
                        'size': vec_params.size,
                        'distance': vec_params.distance.name
                    }
            elif vectors_config:
                result['dense_vectors']['(default)'] = {
                    'size': vectors_config.size,
                    'distance': vectors_config.distance.name
                }
            
            # Sparse ë²¡í„° ì„¤ì • ì •ë³´
            if sparse_vectors_config and isinstance(sparse_vectors_config, dict):
                for sparse_name in sparse_vectors_config.keys():
                    result['sparse_vectors'][sparse_name] = {
                        'enabled': True
                    }
            
            # ìƒ˜í”Œ í¬ì¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
            if point_ids:
                # íŠ¹ì • í¬ì¸íŠ¸ IDë¡œ ì¡°íšŒ
                points = self.client.retrieve(
                    collection_name=self.collection_name,
                    ids=point_ids,
                    with_payload=True,
                    with_vectors=True  # ë²¡í„° ë°ì´í„° í¬í•¨
                )
            else:
                # ìƒ˜í”Œ í¬ì¸íŠ¸ ìŠ¤í¬ë¡¤
                scroll_result = self.client.scroll(
                    collection_name=self.collection_name,
                    limit=sample_size,
                    with_payload=True,
                    with_vectors=True  # ë²¡í„° ë°ì´í„° í¬í•¨
                )
                points = scroll_result[0]
            
            # ìƒ˜í”Œ í¬ì¸íŠ¸ ë¶„ì„
            for point in points:
                sample_info = {
                    'point_id': str(point.id),
                    'payload': point.payload,
                    'dense_vectors': {},
                    'sparse_vectors': {}
                }
                
                # Dense ë²¡í„° í™•ì¸
                if point.vector:
                    if isinstance(point.vector, dict):
                        # ë‹¤ì¤‘ ë²¡í„°
                        for vec_name, vec_data in point.vector.items():
                            if isinstance(vec_data, list):
                                sample_info['dense_vectors'][vec_name or '(default)'] = {
                                    'size': len(vec_data),
                                    'preview': vec_data[:5] if len(vec_data) > 5 else vec_data,
                                    'has_data': True
                                }
                    elif isinstance(point.vector, list):
                        # ë‹¨ì¼ ë²¡í„°
                        sample_info['dense_vectors']['(default)'] = {
                            'size': len(point.vector),
                            'preview': point.vector[:5] if len(point.vector) > 5 else point.vector,
                            'has_data': True
                        }
                
                # Sparse ë²¡í„° í™•ì¸
                if hasattr(point, 'sparse_vectors') and point.sparse_vectors:
                    if isinstance(point.sparse_vectors, dict):
                        for sparse_name, sparse_data in point.sparse_vectors.items():
                            if hasattr(sparse_data, 'indices') and hasattr(sparse_data, 'values'):
                                sample_info['sparse_vectors'][sparse_name] = {
                                    'indices_count': len(sparse_data.indices),
                                    'values_count': len(sparse_data.values),
                                    'indices_preview': list(sparse_data.indices[:10]) if len(sparse_data.indices) > 10 else list(sparse_data.indices),
                                    'values_preview': list(sparse_data.values[:10]) if len(sparse_data.values) > 10 else list(sparse_data.values),
                                    'has_data': True
                                }
                
                result['samples'].append(sample_info)
            
            self.logger.info(f"ë²¡í„° í™•ì¸ ì™„ë£Œ: {len(result['samples'])}ê°œ ìƒ˜í”Œ í¬ì¸íŠ¸")
            return result
            
        except Exception as e:
            self.logger.error(f"ë²¡í„° í™•ì¸ ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {
                'error': str(e),
                'collection_name': self.collection_name
            }
    
    def get_vector_statistics(self) -> Dict[str, Any]:
        """
        ë²¡í„° í†µê³„ ì •ë³´ ë°˜í™˜
        
        Returns:
            ë²¡í„° í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        try:
            collection_info = self.client.get_collection(self.collection_name)
            
            # ë²¡í„° ì„¤ì • í™•ì¸
            vectors_config = collection_info.config.params.vectors
            sparse_vectors_config = getattr(collection_info.config.params, 'sparse_vectors', None)
            
            stats = {
                'collection_name': self.collection_name,
                'points_count': collection_info.points_count,
                'dense_vector_count': 0,
                'sparse_vector_count': 0,
                'dense_vectors_enabled': False,
                'sparse_vectors_enabled': False,
                'vector_configs': {}
            }
            
            # Dense ë²¡í„° í™•ì¸
            if vectors_config:
                stats['dense_vectors_enabled'] = True
                if isinstance(vectors_config, dict):
                    stats['dense_vector_count'] = len(vectors_config)
                    for vec_name, vec_params in vectors_config.items():
                        stats['vector_configs'][vec_name or '(default)'] = {
                            'type': 'dense',
                            'size': vec_params.size,
                            'distance': vec_params.distance.name
                        }
                else:
                    stats['dense_vector_count'] = 1
                    stats['vector_configs']['(default)'] = {
                        'type': 'dense',
                        'size': vectors_config.size,
                        'distance': vectors_config.distance.name
                    }
            
            # Sparse ë²¡í„° í™•ì¸
            if sparse_vectors_config and isinstance(sparse_vectors_config, dict):
                stats['sparse_vectors_enabled'] = True
                stats['sparse_vector_count'] = len(sparse_vectors_config)
                for sparse_name in sparse_vectors_config.keys():
                    stats['vector_configs'][sparse_name] = {
                        'type': 'sparse'
                    }
            
            return stats
            
        except Exception as e:
            self.logger.error(f"ë²¡í„° í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {
                'error': str(e),
                'collection_name': self.collection_name
            }
    
    def get_sparse_vocabulary(self, limit: Optional[int] = None, search_token: Optional[str] = None) -> Dict[str, Any]:
        """
        Sparse ë²¡í„°ì˜ Vocabulary ì •ë³´ ë°˜í™˜
        
        Args:
            limit: ë°˜í™˜í•  vocabulary í•­ëª© ìˆ˜ (Noneì´ë©´ ì „ì²´, ìµœëŒ€ 1000ê°œ)
            search_token: íŠ¹ì • í† í° ê²€ìƒ‰ (í† í°ì´ í¬í•¨ëœ í•­ëª©ë§Œ ë°˜í™˜)
        
        Returns:
            Vocabulary ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            result = {
                'sparse_enabled': self.sparse_enabled,
                'model_trained': False,
                'corpus_size': 0,
                'vocabulary_size': 0,
                'avgdl': 0.0,
                'vocabulary': {},
                'idf_values': {},
                'statistics': {}
            }
            
            if not self.sparse_enabled:
                result['message'] = 'Sparse ë²¡í„°ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.'
                return result
            
            # Sparse ì„ë² ë”© ëª¨ë¸ í™•ì¸
            sparse_embedding = None
            if self.sparse_embedding:
                sparse_embedding = self.sparse_embedding
            elif self.sparse_embedding_manager:
                sparse_embedding = self.sparse_embedding_manager.get_sparse_embedding()
            
            if not sparse_embedding:
                result['message'] = 'Sparse ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
                return result
            
            # í•™ìŠµ ìƒíƒœ í™•ì¸
            if not hasattr(sparse_embedding, 'corpus_size') or sparse_embedding.corpus_size == 0:
                result['message'] = 'Sparse ì„ë² ë”© ëª¨ë¸ì´ ì•„ì§ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.'
                return result
            
            result['model_trained'] = True
            result['corpus_size'] = sparse_embedding.corpus_size
            result['avgdl'] = getattr(sparse_embedding, 'avgdl', 0.0)
            
            # Vocabulary ì •ë³´ ì¶”ì¶œ
            vocabulary = getattr(sparse_embedding, 'vocabulary', {})
            idf = getattr(sparse_embedding, 'idf', {})
            vocabulary_reverse = getattr(sparse_embedding, 'vocabulary_reverse', {})
            
            result['vocabulary_size'] = len(vocabulary)
            
            # Vocabulary í•­ëª© ì¤€ë¹„
            vocab_items = []
            for token, idx in vocabulary.items():
                idf_value = idf.get(token, 0.0)
                vocab_items.append({
                    'token': token,
                    'index': idx,
                    'idf': float(idf_value)
                })
            
            # ê²€ìƒ‰ í•„í„°ë§
            if search_token:
                search_token_lower = search_token.lower()
                vocab_items = [
                    item for item in vocab_items 
                    if search_token_lower in item['token'].lower()
                ]
                result['search_token'] = search_token
                result['filtered_count'] = len(vocab_items)
            
            # ì •ë ¬ (IDF ê°’ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
            vocab_items.sort(key=lambda x: x['idf'], reverse=True)
            
            # ì œí•œ ì ìš©
            if limit is not None:
                vocab_items = vocab_items[:limit]
                result['limit_applied'] = limit
            else:
                # ê¸°ë³¸ê°’: ìµœëŒ€ 1000ê°œ
                if len(vocab_items) > 1000:
                    vocab_items = vocab_items[:1000]
                    result['limit_applied'] = 1000
                    result['message'] = f'Vocabularyê°€ ë„ˆë¬´ ì»¤ì„œ ìƒìœ„ 1000ê°œë§Œ ë°˜í™˜í•©ë‹ˆë‹¤. (ì „ì²´: {len(vocabulary)}ê°œ)'
            
            # ê²°ê³¼ êµ¬ì„±
            result['vocabulary'] = {item['token']: item['index'] for item in vocab_items}
            result['idf_values'] = {item['token']: item['idf'] for item in vocab_items}
            
            # í†µê³„ ì •ë³´
            if vocab_items:
                idf_values_list = [item['idf'] for item in vocab_items]
                result['statistics'] = {
                    'total_vocabulary_size': len(vocabulary),
                    'returned_count': len(vocab_items),
                    'idf_min': float(min(idf_values_list)),
                    'idf_max': float(max(idf_values_list)),
                    'idf_mean': float(sum(idf_values_list) / len(idf_values_list)),
                    'top_tokens': [item['token'] for item in vocab_items[:10]]  # ìƒìœ„ 10ê°œ í† í°
                }
            
            self.logger.info(f"Sparse vocabulary ì¡°íšŒ ì™„ë£Œ: {len(vocab_items)}ê°œ í•­ëª© ë°˜í™˜ (ì „ì²´: {len(vocabulary)}ê°œ)")
            return result
            
        except Exception as e:
            self.logger.error(f"Sparse vocabulary ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {
                'error': str(e),
                'sparse_enabled': self.sparse_enabled,
                'message': f'Vocabulary ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
            }


class VectorStoreManager:
    """ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ì (LangChain ê¸°ë°˜ ë˜ëŠ” Qdrant ë ˆê±°ì‹œ)"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, embedding_manager: Optional[EmbeddingManager] = None):
        """
        Args:
            config: Qdrant ì„¤ì •
            embedding_manager: ê¸°ì¡´ EmbeddingManager ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì , ì¤‘ë³µ ë¡œë“œ ë°©ì§€ìš©)
        """
        self.logger = get_logger()
        
        # ì„¤ì • ë¡œë“œ
        if config is None:
            from src.utils.config import get_qdrant_config
            qdrant_config = get_qdrant_config()
        else:
            qdrant_config = config
        
        # EmbeddingManager ìƒì„± ë˜ëŠ” ì¬ì‚¬ìš© (ì¤‘ë³µ ë¡œë“œ ë°©ì§€)
        if embedding_manager is None:
            embedding_manager = EmbeddingManager()
            self.logger.info("EmbeddingManager ìƒˆë¡œ ìƒì„±")
        else:
            self.logger.info("ê¸°ì¡´ EmbeddingManager ì¬ì‚¬ìš© (ì¤‘ë³µ ë¡œë“œ ë°©ì§€)")
        
        # EmbeddingManagerë¥¼ LangChain Embeddingsë¡œ ë˜í•‘
        langchain_embeddings = EmbeddingManagerWrapper(embedding_manager)
        
        # Qdrant ë ˆê±°ì‹œ ì§€ì› (í•„ìš”ì‹œ) - ê¸°ì¡´ ì„ë² ë”© ì¬ì‚¬ìš©
        self.store = QdrantVectorStore(qdrant_config, embeddings=langchain_embeddings)
        
        # LangChain Retrieval Manager ì´ˆê¸°í™”
        try:
            # ì„¤ì •ì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
            if hasattr(qdrant_config, 'faiss_storage_path'):
                faiss_storage_path = qdrant_config.faiss_storage_path
            elif isinstance(qdrant_config, dict):
                faiss_storage_path = qdrant_config.get('faiss_storage_path', 'data/faiss_index')
            else:
                faiss_storage_path = 'data/faiss_index'
            
            # BM25 ì €ì¥ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
            if hasattr(qdrant_config, 'bm25_storage_path'):
                bm25_storage_path = qdrant_config.bm25_storage_path
            elif isinstance(qdrant_config, dict):
                bm25_storage_path = qdrant_config.get('bm25_storage_path', 'data/bm25_index')
            else:
                bm25_storage_path = 'data/bm25_index'
            
            # FAISS GPU ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            if hasattr(qdrant_config, 'faiss_use_gpu'):
                faiss_use_gpu = qdrant_config.faiss_use_gpu
            elif isinstance(qdrant_config, dict):
                faiss_use_gpu = qdrant_config.get('faiss_use_gpu', True)
            else:
                faiss_use_gpu = True  # ê¸°ë³¸ê°’
            
            self.langchain_retrieval_manager = LangChainRetrievalManager(
                embedding_function=langchain_embeddings,
                faiss_storage_path=faiss_storage_path,
                bm25_storage_path=bm25_storage_path,
                faiss_use_gpu=faiss_use_gpu
            )
            
            # ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
            faiss_loaded = self.langchain_retrieval_manager.load_faiss_index()
            if faiss_loaded:
                self.logger.info("FAISS ì¸ë±ìŠ¤ ìë™ ë¡œë“œ ì™„ë£Œ")
            else:
                self.logger.debug("FAISS ì¸ë±ìŠ¤ ì—†ìŒ (ë¬¸ì„œ ì—…ë¡œë“œ ì‹œ ìƒì„±ë¨)")
            
            # ê¸°ì¡´ BM25 ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
            bm25_loaded = self.langchain_retrieval_manager.load_bm25_index()
            if bm25_loaded:
                self.logger.info("BM25 ì¸ë±ìŠ¤ ìë™ ë¡œë“œ ì™„ë£Œ")
            else:
                self.logger.debug("BM25 ì¸ë±ìŠ¤ ì—†ìŒ (ë¬¸ì„œ ì—…ë¡œë“œ ì‹œ ìƒì„±ë¨)")
            
            self.logger.info("LangChain Retrieval Manager ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"LangChain Retrieval Manager ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}. ë ˆê±°ì‹œ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
            self.langchain_retrieval_manager = None
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í™œì„±í™” ì—¬ë¶€ í™•ì¸ (ë ˆê±°ì‹œ BM25Indexer ì œê±°ë¨)
        if hasattr(qdrant_config, 'hybrid_search_enabled'):
            hybrid_search_enabled = qdrant_config.hybrid_search_enabled
        elif isinstance(qdrant_config, dict):
            hybrid_search_enabled = qdrant_config.get('hybrid_search_enabled', True)
        else:
            hybrid_search_enabled = True
        
        self.hybrid_search_enabled = hybrid_search_enabled
    
    def setup_collection(self, force_recreate: bool = False) -> bool:
        """ì»¬ë ‰ì…˜ ì„¤ì •"""
        return self.store.create_collection(force_recreate)
    
    def add_chunks(self, chunks: List[DocumentChunk], force_update: bool = False) -> bool:
        """ì²­í¬ë¥¼ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€ (Qdrant + FAISS + BM25)"""
        # Qdrantì— ì¶”ê°€ (ë ˆê±°ì‹œ í˜¸í™˜)
        qdrant_success = self.store.add_documents(chunks, force_update)
        
        # LangChain FAISS ë° BM25ì— ì¶”ê°€ (ë¬¸ì„œ ì—…ë¡œë“œ ì‹œ)
        # ì£¼ì˜: add_chunksëŠ” ì—…ë¡œë“œ ì‹œ í˜¸ì¶œë˜ì§€ë§Œ, FAISSëŠ” ì „ì²´ ì²­í¬ë¡œ ì´ˆê¸° ìƒì„±í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì 
        # ë”°ë¼ì„œ FAISS ì¸ë±ìŠ¤ëŠ” rag_system.pyì˜ process_and_store_documentsì—ì„œ ì´ˆê¸°í™”ë¨
        # ì—¬ê¸°ì„œëŠ” FAISS ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•  ë•Œë§Œ ë¬¸ì„œ ì¶”ê°€ ì‹œë„
        if self.langchain_retrieval_manager:
            try:
                # FAISS ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬¸ì„œ ì¶”ê°€
                if self.langchain_retrieval_manager.faiss_store is not None:
                    faiss_success = self.langchain_retrieval_manager.add_documents_to_faiss(chunks)
                    if faiss_success:
                        self.logger.debug(f"FAISSì— {len(chunks)}ê°œ ì²­í¬ ì¶”ê°€ ì™„ë£Œ")
                # FAISS ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ì´ˆê¸° ìƒì„±ì€ rag_systemì—ì„œ ì²˜ë¦¬
                
                # BM25 ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬¸ì„œ ì¶”ê°€
                if self.langchain_retrieval_manager.bm25_retriever is not None:
                    bm25_success = self.langchain_retrieval_manager.add_documents_to_bm25(chunks)
                    if bm25_success:
                        self.logger.debug(f"BM25ì— {len(chunks)}ê°œ ì²­í¬ ì¶”ê°€ ì™„ë£Œ")
                # BM25 ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ì´ˆê¸° ìƒì„±ì€ rag_systemì—ì„œ ì²˜ë¦¬
                
            except Exception as e:
                self.logger.warning(f"FAISS/BM25 ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
        
        return qdrant_success
    
    def replace_document_vectors(self, file_path: str, new_chunks: List[DocumentChunk]) -> bool:
        """íŠ¹ì • íŒŒì¼ì˜ ë²¡í„°ë¥¼ ì™„ì „íˆ êµì²´ (Qdrant + FAISS + BM25)"""
        # Qdrantì—ì„œ êµì²´
        qdrant_success = self.store.replace_document_vectors(file_path, new_chunks)
        
        if not qdrant_success:
            return False
        
        # FAISS ë° BM25ì—ì„œë„ êµì²´
        if self.langchain_retrieval_manager:
            try:
                # FAISSì—ì„œ ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ í›„ ìƒˆ ë¬¸ì„œ ì¶”ê°€
                # FAISSëŠ” ì§ì ‘ ì‚­ì œë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì „ì²´ ì¬êµ¬ì¶•ì´ í•„ìš”í•˜ê±°ë‚˜
                # ì¼ë‹¨ ìƒˆ ë¬¸ì„œë¥¼ ì¶”ê°€í•˜ê³  ë‚˜ì¤‘ì— ì¬êµ¬ì¶•í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©
                if self.langchain_retrieval_manager.faiss_store is not None:
                    # ìƒˆ ë¬¸ì„œ ì¶”ê°€
                    faiss_success = self.langchain_retrieval_manager.add_documents_to_faiss(new_chunks)
                    if faiss_success:
                        self.logger.info(f"FAISSì— ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ: {file_path} ({len(new_chunks)}ê°œ)")
                    else:
                        self.logger.warning(f"FAISS ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {file_path}")
                        # FAISS ì¬êµ¬ì¶• ê¶Œì¥
                        self.logger.warning("FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì¶•ì„ ê¶Œì¥í•©ë‹ˆë‹¤. /rebuild-indexes APIë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
                
                # BM25ì—ì„œ ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ í›„ ìƒˆ ë¬¸ì„œ ì¶”ê°€
                if self.langchain_retrieval_manager.bm25_retriever is not None:
                    # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ
                    delete_success = self.langchain_retrieval_manager.delete_documents_by_source(file_path)
                    if delete_success:
                        self.logger.debug(f"BM25ì—ì„œ ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {file_path}")
                    
                    # ìƒˆ ë¬¸ì„œ ì¶”ê°€
                    bm25_success = self.langchain_retrieval_manager.add_documents_to_bm25(new_chunks)
                    if bm25_success:
                        self.logger.info(f"BM25ì— ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ: {file_path} ({len(new_chunks)}ê°œ)")
                    else:
                        self.logger.warning(f"BM25 ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {file_path}")
                elif self.langchain_retrieval_manager.faiss_store is not None:
                    # BM25 ì¸ë±ìŠ¤ê°€ ì—†ì§€ë§Œ FAISSëŠ” ìˆëŠ” ê²½ìš°, BM25ë§Œ ì´ˆê¸°í™”
                    self.logger.info(f"BM25 ì¸ë±ìŠ¤ê°€ ì—†ì–´ ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤: {file_path}")
                    # Qdrantì—ì„œ ì „ì²´ ë¬¸ì„œ ê°€ì ¸ì™€ì„œ BM25 êµ¬ì¶•í•˜ëŠ” ê²ƒì€ ë³„ë„ ë¡œì§ í•„ìš”
                    # ì¼ë‹¨ ê²½ê³ ë§Œ ë‚¨ê¸°ê³ , ë‚˜ì¤‘ì— ì¬êµ¬ì¶•í•˜ë„ë¡ ì•ˆë‚´
                    self.logger.warning("BM25 ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•˜ë ¤ë©´ ì „ì²´ ë¬¸ì„œë¡œ ì¬êµ¬ì¶•ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                
            except Exception as e:
                self.logger.error(f"FAISS/BM25 ë¬¸ì„œ êµì²´ ì‹¤íŒ¨: {str(e)}")
                # QdrantëŠ” ì„±ê³µí–ˆìœ¼ë¯€ë¡œ True ë°˜í™˜ (FAISS/BM25ëŠ” ê²½ê³ ë§Œ)
        
        return qdrant_success
    
    def search_similar(self, 
                      query: str, 
                      limit: Optional[int] = None,
                      score_threshold: Optional[float] = None,
                      filter_conditions: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (FAISS ìš°ì„ , ì—†ìœ¼ë©´ Qdrant)"""
        # LangChain FAISS ì‚¬ìš© ê°€ëŠ¥ ì‹œ ìš°ì„  ì‚¬ìš©
        if self.langchain_retrieval_manager and self.langchain_retrieval_manager.faiss_store:
            try:
                results = self.langchain_retrieval_manager.search_with_faiss_only(
                    query=query,
                    k=limit or 10,
                    score_threshold=score_threshold
                )
                if results:
                    self.logger.debug("FAISS ê²€ìƒ‰ ì‚¬ìš©")
                    return results
            except Exception as e:
                self.logger.warning(f"FAISS ê²€ìƒ‰ ì‹¤íŒ¨, Qdrant ì‚¬ìš©: {str(e)}")
        
        # ë ˆê±°ì‹œ Qdrant ê²€ìƒ‰
        return self.store.search_similar(query, limit, score_threshold, filter_conditions)
    
    def search_by_table_title(self, 
                             table_title: str, 
                             limit: Optional[int] = None,
                             score_threshold: Optional[float] = None) -> List[Dict[str, Any]]:
        """í‘œ ì œëª©ìœ¼ë¡œ ê²€ìƒ‰"""
        return self.store.search_by_table_title(table_title, limit, score_threshold)
    
    def search_with_table_filter(self, 
                                query: str, 
                                table_title: Optional[str] = None,
                                is_table_data: Optional[bool] = None,
                                limit: Optional[int] = None,
                                score_threshold: Optional[float] = None) -> List[Dict[str, Any]]:
        """í‘œ ê´€ë ¨ í•„í„°ì™€ í•¨ê»˜ ê²€ìƒ‰"""
        return self.store.search_with_table_filter(
            query, table_title, is_table_data, limit, score_threshold
        )
    
    def get_collection_info(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ ì •ë³´ ë°˜í™˜"""
        return self.store.get_collection_info()
    
    def get_stats(self) -> Dict[str, Any]:
        """ì €ì¥ì†Œ í†µê³„ ë°˜í™˜ (get_collection_infoì˜ ë³„ì¹­)"""
        return self.get_collection_info()
    
    def inspect_vectors(self, sample_size: int = 3, point_ids: Optional[List[Any]] = None) -> Dict[str, Any]:
        """
        ìƒ˜í”Œ í¬ì¸íŠ¸ì˜ Denseì™€ Sparse ë²¡í„° í™•ì¸
        
        Args:
            sample_size: í™•ì¸í•  ìƒ˜í”Œ í¬ì¸íŠ¸ ìˆ˜ (point_idsê°€ Noneì¼ ë•Œ)
            point_ids: í™•ì¸í•  íŠ¹ì • í¬ì¸íŠ¸ ID ë¦¬ìŠ¤íŠ¸ (ì§€ì • ì‹œ sample_size ë¬´ì‹œ)
            
        Returns:
            ë²¡í„° ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        return self.store.inspect_vectors(sample_size, point_ids)
    
    def get_vector_statistics(self) -> Dict[str, Any]:
        """
        ë²¡í„° í†µê³„ ì •ë³´ ë°˜í™˜
        
        Returns:
            ë²¡í„° í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        return self.store.get_vector_statistics()
    
    def get_sparse_vocabulary(self, limit: Optional[int] = None, search_token: Optional[str] = None) -> Dict[str, Any]:
        """
        Sparse ë²¡í„°ì˜ Vocabulary ì •ë³´ ë°˜í™˜
        
        Args:
            limit: ë°˜í™˜í•  vocabulary í•­ëª© ìˆ˜ (Noneì´ë©´ ì „ì²´)
            search_token: íŠ¹ì • í† í° ê²€ìƒ‰ (í† í°ì´ í¬í•¨ëœ í•­ëª©ë§Œ ë°˜í™˜)
        
        Returns:
            Vocabulary ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        return self.store.get_sparse_vocabulary(limit, search_token)
    
    def get_documents_info(self) -> List[Dict[str, Any]]:
        """ì €ì¥ëœ ë¬¸ì„œë“¤ì˜ ì •ë³´ ë°˜í™˜"""
        return self.store.get_documents_info()
    
    def get_document_chunks(self, document_id: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ë¬¸ì„œì˜ ì²­í¬ ì •ë³´ ë°˜í™˜"""
        return self.store.get_document_chunks(document_id)
    
    def delete_document(self, source_file: str) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ì‚­ì œ (Qdrant + FAISS + BM25)
        
        Args:
            source_file: ì‚­ì œí•  ë¬¸ì„œì˜ source_file ê²½ë¡œ
            
        Returns:
            ì‚­ì œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        result = {
            'success': False,
            'deleted_chunks_count': 0,
            'qdrant_deleted': False,
            'qdrant_success': False,  # í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
            'faiss_deleted': False,
            'faiss_handled': False,  # FAISS ì²˜ë¦¬ ì—¬ë¶€
            'bm25_deleted': False,
            'bm25_success': False,  # í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
            'message': '',
            'warnings': []
        }
        
        try:
            # 1. Qdrantì—ì„œ ì‚­ì œ ì „ì— ì²­í¬ ìˆ˜ í™•ì¸
            deleted_chunks_count = self.store.get_document_chunks_count(source_file)
            
            # 2. Qdrantì—ì„œ ì‚­ì œ
            qdrant_success = self.store._delete_document_vectors(source_file)
            result['qdrant_deleted'] = qdrant_success
            result['qdrant_success'] = qdrant_success
            result['deleted_chunks_count'] = deleted_chunks_count
            
            if not qdrant_success:
                result['message'] = f"Qdrantì—ì„œ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {source_file}"
                return result
            
            # 3. BM25ì—ì„œ ì‚­ì œ
            if self.langchain_retrieval_manager:
                try:
                    bm25_success = self.langchain_retrieval_manager.delete_documents_by_source(source_file)
                    result['bm25_deleted'] = bm25_success
                    result['bm25_success'] = bm25_success
                    if bm25_success:
                        self.logger.info(f"BM25ì—ì„œ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {source_file}")
                    else:
                        result['warnings'].append("BM25ì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì´ë¯¸ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    self.logger.error(f"BM25 ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
                    result['warnings'].append(f"BM25 ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
                
                # 4. FAISSì—ì„œ ì‚­ì œ (FAISSëŠ” ì§ì ‘ ì‚­ì œë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì¬êµ¬ì¶• í•„ìš”)
                if self.langchain_retrieval_manager.faiss_store is not None:
                    result['faiss_handled'] = True
                    result['warnings'].append(
                        "FAISSëŠ” ì§ì ‘ ì‚­ì œë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
                        "ì¸ë±ìŠ¤ ì¬êµ¬ì¶•ì„ ê¶Œì¥í•©ë‹ˆë‹¤. /rebuild-indexes APIë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
                    )
                    # FAISS ì¬êµ¬ì¶•ì€ ì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ í•´ì•¼ í•¨
            
            result['success'] = True
            result['message'] = f"ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {source_file}"
            
            return result
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            result['message'] = f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}"
            return result
    
    def build_bm25_index(self, chunks: List[DocumentChunk]) -> bool:
        """BM25 ì¸ë±ìŠ¤ êµ¬ì¶• (LangChain BM25Retrieverë§Œ ì‚¬ìš©)"""
        if not self.langchain_retrieval_manager:
            self.logger.warning("LangChain Retrieval Managerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. BM25 ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            success = self.langchain_retrieval_manager.initialize_bm25_from_chunks(chunks)
            if success:
                self.logger.info("LangChain BM25Retriever ì´ˆê¸°í™” ì™„ë£Œ")
            return success
        except Exception as e:
            self.logger.error(f"LangChain BM25Retriever ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False
    
    def hybrid_search(self,
                     query: str,
                     limit: int = 10,
                     score_threshold: Optional[float] = None,
                     vector_weight: Optional[float] = None,
                     bm25_weight: Optional[float] = None,
                     rrf_k: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (EnsembleRetriever ë˜ëŠ” ë ˆê±°ì‹œ RRF)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            limit: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            score_threshold: ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’
            vector_weight: ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (Noneì´ë©´ ì„¤ì •ê°’ ì‚¬ìš©)
            bm25_weight: BM25 ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (Noneì´ë©´ ì„¤ì •ê°’ ì‚¬ìš©)
            rrf_k: RRF ì•Œê³ ë¦¬ì¦˜ ìƒìˆ˜ (Noneì´ë©´ ì„¤ì •ê°’ ì‚¬ìš©)
            
        Returns:
            í†µí•©ëœ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self.hybrid_search_enabled:
            self.logger.warning("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            return self.search_similar(query, limit, score_threshold)
        
        # LangChain EnsembleRetriever ì‚¬ìš© ì‹œë„
        if self.langchain_retrieval_manager:
            try:
                # ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°
                from src.utils.config import get_qdrant_config
                qdrant_config = get_qdrant_config()
                
                faiss_weight = vector_weight if vector_weight is not None else (qdrant_config.hybrid_search_vector_weight if hasattr(qdrant_config, 'hybrid_search_vector_weight') else 0.7)
                bm25_weight_val = bm25_weight if bm25_weight is not None else (qdrant_config.hybrid_search_bm25_weight if hasattr(qdrant_config, 'hybrid_search_bm25_weight') else 0.3)
                rrf_c = rrf_k if rrf_k is not None else (qdrant_config.hybrid_search_rrf_k if hasattr(qdrant_config, 'hybrid_search_rrf_k') else 60)
                
                # EnsembleRetriever ìƒì„± (ì—†ìœ¼ë©´ ìƒì„±)
                if self.langchain_retrieval_manager.ensemble_retriever is None:
                    self.langchain_retrieval_manager.create_ensemble_retriever(
                        faiss_weight=faiss_weight,
                        bm25_weight=bm25_weight_val,
                        c=rrf_c,
                        k=limit
                    )
                
                # EnsembleRetriever ê²€ìƒ‰
                if self.langchain_retrieval_manager.ensemble_retriever:
                    results = self.langchain_retrieval_manager.search_with_ensemble(
                        query=query,
                        k=limit,
                        score_threshold=score_threshold
                    )
                    if results:
                        self.logger.info(f"EnsembleRetriever ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
                        return results
                
            except Exception as e:
                self.logger.error(f"EnsembleRetriever ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                import traceback
                self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        
        # EnsembleRetrieverê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©
        self.logger.warning("EnsembleRetrieverë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë²¡í„° ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        return self.search_similar(query, limit, score_threshold)
    
    # ========== ë¹„ë™ê¸° ë©”ì„œë“œ (Phase 2: ë²¡í„° ê²€ìƒ‰ ë¹„ë™ê¸°í™”) ==========
    
    async def search_similar_async(self, 
                                  query: str, 
                                  limit: Optional[int] = None,
                                  score_threshold: Optional[float] = None,
                                  filter_conditions: Optional[Dict[str, Any]] = None,
                                  dense_weight: Optional[float] = None,
                                  sparse_weight: Optional[float] = None) -> List[Dict[str, Any]]:
        """ë¹„ë™ê¸° ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (FAISS ìš°ì„ , ì—†ìœ¼ë©´ Qdrant)"""
        # LangChain FAISS ì‚¬ìš© ê°€ëŠ¥ ì‹œ ìš°ì„  ì‚¬ìš© (ë™ê¸° ìœ ì§€ - FAISSëŠ” CPU/GPU ì—°ì‚°)
        if self.langchain_retrieval_manager and self.langchain_retrieval_manager.faiss_store:
            try:
                # FAISSëŠ” CPU/GPU ì—°ì‚°ì´ë¯€ë¡œ asyncio.to_threadë¡œ ë¹„ë™ê¸°í™”
                import asyncio
                results = await asyncio.to_thread(
                    self.langchain_retrieval_manager.search_with_faiss_only,
                    query=query,
                    k=limit or 10,
                    score_threshold=score_threshold
                )
                if results:
                    self.logger.debug("ë¹„ë™ê¸° FAISS ê²€ìƒ‰ ì‚¬ìš©")
                    return results
            except Exception as e:
                self.logger.warning(f"ë¹„ë™ê¸° FAISS ê²€ìƒ‰ ì‹¤íŒ¨, Qdrant ì‚¬ìš©: {str(e)}")
        
        # Qdrant ë¹„ë™ê¸° ê²€ìƒ‰
        return await self.store.search_similar_async(
            query, limit, score_threshold, filter_conditions, dense_weight, sparse_weight
        )
    
    async def hybrid_search_async(self,
                                 query: str,
                                 limit: int = 10,
                                 score_threshold: Optional[float] = None,
                                 vector_weight: Optional[float] = None,
                                 bm25_weight: Optional[float] = None,
                                 rrf_k: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        ë¹„ë™ê¸° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (EnsembleRetriever ë˜ëŠ” ë ˆê±°ì‹œ RRF)
        """
        if not self.hybrid_search_enabled:
            self.logger.warning("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë²¡í„° ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            return await self.search_similar_async(query, limit, score_threshold)
        
        # LangChain EnsembleRetriever ì‚¬ìš© ì‹œë„ (ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰)
        if self.langchain_retrieval_manager:
            try:
                from src.utils.config import get_qdrant_config
                qdrant_config = get_qdrant_config()
                
                faiss_weight = vector_weight if vector_weight is not None else (qdrant_config.hybrid_search_vector_weight if hasattr(qdrant_config, 'hybrid_search_vector_weight') else 0.7)
                bm25_weight_val = bm25_weight if bm25_weight is not None else (qdrant_config.hybrid_search_bm25_weight if hasattr(qdrant_config, 'hybrid_search_bm25_weight') else 0.3)
                rrf_c = rrf_k if rrf_k is not None else (qdrant_config.hybrid_search_rrf_k if hasattr(qdrant_config, 'hybrid_search_rrf_k') else 60)
                
                # EnsembleRetriever ìƒì„± (ì—†ìœ¼ë©´ ìƒì„±)
                if self.langchain_retrieval_manager.ensemble_retriever is None:
                    self.langchain_retrieval_manager.create_ensemble_retriever(
                        faiss_weight=faiss_weight,
                        bm25_weight=bm25_weight_val,
                        c=rrf_c,
                        k=limit
                    )
                
                # EnsembleRetriever ê²€ìƒ‰ (ë¹„ë™ê¸°ë¡œ ì‹¤í–‰)
                if self.langchain_retrieval_manager.ensemble_retriever:
                    import asyncio
                    results = await asyncio.to_thread(
                        self.langchain_retrieval_manager.search_with_ensemble,
                        query=query,
                        k=limit,
                        score_threshold=score_threshold
                    )
                    if results:
                        self.logger.info(f"ë¹„ë™ê¸° EnsembleRetriever ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
                        return results
                
            except Exception as e:
                self.logger.error(f"ë¹„ë™ê¸° EnsembleRetriever ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                import traceback
                self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        
        # EnsembleRetrieverê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©
        self.logger.warning("EnsembleRetrieverë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë²¡í„° ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        return await self.search_similar_async(query, limit, score_threshold)
    
    def _merge_with_rrf(self,
                       vector_results: List[Dict[str, Any]],
                       bm25_results: List[Dict[str, Any]],
                       rrf_k: int = 60,
                       limit: int = 10,
                       score_threshold: Optional[float] = None) -> List[Dict[str, Any]]:
        """
        Reciprocal Rank Fusion (RRF) ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ í†µí•© (ë ˆê±°ì‹œ - ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
        
        ì£¼ì˜: ì´ ë©”ì„œë“œëŠ” ë ˆê±°ì‹œ BM25Indexerì™€ í•¨ê»˜ ì‚¬ìš©ë˜ì—ˆì§€ë§Œ,
        í˜„ì¬ëŠ” LangChain EnsembleRetrieverê°€ RRFë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        
        RRF ì ìˆ˜ = Î£ 1 / (k + rank)
        - k: ìƒìˆ˜ (ì¼ë°˜ì ìœ¼ë¡œ 60)
        - rank: ê° ê²€ìƒ‰ ë°©ë²•ì—ì„œì˜ ìˆœìœ„
        """
        # ì²­í¬ IDë¥¼ í‚¤ë¡œ í•˜ëŠ” RRF ì ìˆ˜ ë”•ì…”ë„ˆë¦¬
        rrf_scores: Dict[str, float] = {}
        result_data: Dict[str, Dict[str, Any]] = {}
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì ìˆ˜ ì¶”ê°€
        for rank, result in enumerate(vector_results, 1):
            # chunk_id ì¶”ì¶œ (ì—¬ëŸ¬ ê°€ëŠ¥í•œ í‚¤ ì‹œë„)
            chunk_id = (result.get('chunk_id') or 
                       result.get('id') or 
                       result.get('metadata', {}).get('chunk_id'))
            
            if chunk_id:
                rrf_scores[chunk_id] = rrf_scores.get(chunk_id, 0) + 1 / (rrf_k + rank)
                result_data[chunk_id] = result
        
        # BM25 ê²€ìƒ‰ ê²°ê³¼ ì ìˆ˜ ì¶”ê°€
        for rank, result in enumerate(bm25_results, 1):
            chunk_id = result.get('chunk_id')
            if chunk_id:
                rrf_scores[chunk_id] = rrf_scores.get(chunk_id, 0) + 1 / (rrf_k + rank)
                
                # ë²¡í„° ê²€ìƒ‰ì— ì—†ë˜ ê²°ê³¼ì´ë©´ ë°ì´í„° ì¶”ê°€
                if chunk_id not in result_data:
                    # BM25 ê²°ê³¼ë¥¼ ë²¡í„° ê²€ìƒ‰ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    result_data[chunk_id] = {
                        'content': result.get('content', ''),
                        'score': result.get('score', 0),
                        'metadata': result.get('metadata', {}),
                        'source_file': result.get('source_file', ''),
                        'chunk_index': result.get('chunk_index', 0),
                        'chunk_id': chunk_id,
                        'rrf_score': rrf_scores[chunk_id]
                    }
        
        # RRF ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        sorted_chunk_ids = sorted(
            rrf_scores.keys(),
            key=lambda x: rrf_scores[x],
            reverse=True
        )
        
        # ìƒìœ„ Kê°œ ê²°ê³¼ ë°˜í™˜
        results = []
        for chunk_id in sorted_chunk_ids:
            result = result_data.get(chunk_id)
            if not result:
                continue
            
            # RRF ì ìˆ˜ë¥¼ ìµœì¢… ì ìˆ˜ë¡œ ì‚¬ìš©
            result['score'] = rrf_scores[chunk_id]
            result['rrf_score'] = rrf_scores[chunk_id]
            result['vector_score'] = result.get('score', 0) if chunk_id in [r.get('chunk_id') for r in vector_results] else None
            result['bm25_score'] = next((r.get('score') for r in bm25_results if r.get('chunk_id') == chunk_id), None)
            
            # ì ìˆ˜ ì„ê³„ê°’ í•„í„°ë§
            if score_threshold is not None and result['score'] < score_threshold:
                continue
            
            results.append(result)
            
            if len(results) >= limit:
                break
        
        self.logger.info(
            f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: ë²¡í„°={len(vector_results)}ê°œ, "
            f"BM25={len(bm25_results)}ê°œ, í†µí•©={len(results)}ê°œ"
        )
        
        return results


def create_vector_store_manager(config: Optional[Dict[str, Any]] = None) -> VectorStoreManager:
    """ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ì ìƒì„±"""
    return VectorStoreManager(config)


def setup_vector_store(config: Optional[Dict[str, Any]] = None, force_recreate: bool = False) -> bool:
    """ë²¡í„° ì €ì¥ì†Œ ì„¤ì •"""
    manager = create_vector_store_manager(config)
    return manager.setup_collection(force_recreate)
