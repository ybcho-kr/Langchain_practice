"""
RAG ì‹œìŠ¤í…œ ëª¨ë“ˆ
ë¬¸ì„œ ì²˜ë¦¬, ì„ë² ë”©, ë²¡í„° ê²€ìƒ‰, ë‹µë³€ ìƒì„±ì„ í†µí•©í•œ RAG ì‹œìŠ¤í…œ
"""

from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import time

from src.utils.logger import get_logger
from src.utils.config import get_config, get_rag_config
from src.utils.helpers import is_general_question
from src.modules.document_processor import DocumentProcessor, DocumentChunk
from src.modules.vector_store import QdrantVectorStore
from src.models.llm_models import OllamaLLMClient
from src.modules.reranker_module import CrossEncoderReranker


@dataclass
class RAGResponse:
    """RAG ì‘ë‹µ"""
    answer: str
    sources: List[Dict[str, Any]]
    confidence: float
    processing_time: float
    query: str
    model_used: str
    is_general_answer: bool = False  # ì¼ë°˜ ë‹µë³€ ì—¬ë¶€
    is_rag_answer: bool = True  # RAG ë‹µë³€ ì—¬ë¶€ (ê¸°ë³¸ê°’ True)


class RAGSystem:
    """RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.logger = get_logger()
        
        if config is None:
            config = get_config()
        
        self.config = config
        self.rag_config = get_rag_config()
        
        print(f"  ì„ë² ë”© ëª¨ë¸: {self.config.model.get('embedding', {}).name if 'embedding' in config.model else 'N/A'}")
        print(f"  LLM ëª¨ë¸: {self.config.model.get('llm', {}).name if 'llm' in config.model else 'N/A'}")
        # ëª¨ë“ˆ ì´ˆê¸°í™”
        self.document_processor = DocumentProcessor()
        # QdrantVectorStore ì§ì ‘ ì‚¬ìš©
        self.vector_store = QdrantVectorStore(config.qdrant, bge_model=None)
        
        # ë¦¬ë­ì»¤ ì´ˆê¸°í™” (ì„¤ì • ê¸°ë°˜)
        self.reranker: Optional[CrossEncoderReranker] = None
        try:
            reranker_cfg = getattr(self.config, 'reranker', None)
            if reranker_cfg is None:
                self.logger.info("ë¦¬ë­ì»¤ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. ë¦¬ë­ì»¤ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
                self.reranker = None
            else:
                # Pydantic ëª¨ë¸ ë˜ëŠ” dict ëª¨ë‘ ì§€ì›
                enabled = (reranker_cfg.enabled if hasattr(reranker_cfg, 'enabled') 
                          else reranker_cfg.get('enabled', False)) if reranker_cfg else False
                
                if enabled:
                    # ì„¤ì • íŒŒì‹± ë°©ì–´ì½”ë“œ (Pydantic ëª¨ë¸/dict í˜¼ìš©)
                    model_path = (reranker_cfg.model_path if hasattr(reranker_cfg, 'model_path') 
                                 else reranker_cfg.get('model_path', '')) if reranker_cfg else ''
                    device = (reranker_cfg.device if hasattr(reranker_cfg, 'device') 
                             else reranker_cfg.get('device', 'cuda')) if reranker_cfg else 'cuda'
                    batch_size = (reranker_cfg.batch_size if hasattr(reranker_cfg, 'batch_size') 
                                 else reranker_cfg.get('batch_size', 32)) if reranker_cfg else 32
                    
                    self.logger.info(
                        f"ë¦¬ë­ì»¤ ì„¤ì • í™•ì¸: enabled={enabled}, model_path={model_path}, "
                        f"device={device}, batch_size={batch_size}"
                    )
                    
                    if model_path:
                        try:
                            self.reranker = CrossEncoderReranker(
                                model_path=model_path,
                                device=device,
                                batch_size=batch_size,
                            )
                            self.logger.info(
                                f"âœ… ë¦¬ë­ì»¤ ì´ˆê¸°í™” ì™„ë£Œ: path={model_path}, device={self.reranker.device}, batch_size={batch_size}"
                            )
                        except Exception as reranker_error:
                            self.logger.error(f"ë¦¬ë­ì»¤ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(reranker_error)}")
                            self.reranker = None
                    else:
                        self.logger.warning("ë¦¬ë­ì»¤ê°€ í™œì„±í™”ë˜ì–´ ìˆì§€ë§Œ model_pathê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë¦¬ë­ì»¤ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
                        self.reranker = None
                else:
                    self.logger.info("ë¦¬ë­ì»¤ ë¹„í™œì„±í™” ìƒíƒœ (ì„¤ì • enabled=False)")
                    self.reranker = None
        except Exception as e:
            self.logger.error(f"ë¦¬ë­ì»¤ ì´ˆê¸°í™” ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}", exc_info=True)
            self.reranker = None
        
        # LLM í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        llm_config = config.model.get('llm')
        if isinstance(llm_config, dict):
            self.llm_client = OllamaLLMClient(llm_config)
        else:
            self.llm_client = OllamaLLMClient(llm_config)
        
        # Qdrant ë²¡í„° ì €ì¥ì†Œ ìƒíƒœ í™•ì¸
        qdrant_stats = self.vector_store.get_collection_info()
        if qdrant_stats:
            points_count = qdrant_stats.get('points_count', 0)
            self.logger.info(f"Qdrant ì»¬ë ‰ì…˜ ì¤€ë¹„ ì™„ë£Œ (í¬ì¸íŠ¸ {points_count}ê°œ)")
        
        self.logger.info("RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    def _release_gpu_memory(self):
        """GPU ë©”ëª¨ë¦¬ í•´ì œ (ëª¨ë¸ ì–¸ë¡œë“œ)"""
        import torch
        import gc
        
        self.logger.info("GPU ë©”ëª¨ë¦¬ í•´ì œ ì‹œì‘...")
        
        # BGE-m3 ëª¨ë¸ í•´ì œ
        if hasattr(self, 'vector_store') and self.vector_store:
            if hasattr(self.vector_store, 'bge_model') and self.vector_store.bge_model:
                try:
                    del self.vector_store.bge_model
                    self.logger.info("BGE-m3 ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"BGE-m3 ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì‹¤íŒ¨: {str(e)}")
        
        # ë¦¬ë­ì»¤ ëª¨ë¸ í•´ì œ
        if self.reranker and hasattr(self.reranker, 'model'):
            try:
                del self.reranker.model
                self.logger.info("ë¦¬ë­ì»¤ ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"ë¦¬ë­ì»¤ ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì‹¤íŒ¨: {str(e)}")
        
        # PyTorch ìºì‹œ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            self.logger.info("PyTorch CUDA ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        self.logger.info("GPU ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
    
    def reload_embedding_model(self, config: Optional[Dict[str, Any]] = None):
        """BGE-m3 ëª¨ë¸ ë™ì  ì¬ë¡œë“œ"""
        self.logger.info("BGE-m3 ëª¨ë¸ ì¬ë¡œë“œ ì‹œì‘...")
        
        # ê¸°ì¡´ ëª¨ë¸ í•´ì œ
        if hasattr(self, 'vector_store') and self.vector_store:
            if hasattr(self.vector_store, 'bge_model') and self.vector_store.bge_model:
                del self.vector_store.bge_model
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
        # ìƒˆ ëª¨ë¸ ë¡œë“œ
        try:
            from FlagEmbedding import BGEM3FlagModel
            from src.utils.config import get_embedding_config
            
            if config is None:
                embedding_config = get_embedding_config()
            else:
                embedding_config = config
            
            model_path = embedding_config.model_path or embedding_config.name
            if not model_path:
                raise ValueError("BGE-m3 ëª¨ë¸ ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            use_fp16 = True
            try:
                import torch
                if not torch.cuda.is_available():
                    use_fp16 = False
            except ImportError:
                use_fp16 = False
            
            # ìƒˆ BGE-m3 ëª¨ë¸ ë¡œë“œ
            new_bge_model = BGEM3FlagModel(model_path, use_fp16=use_fp16)
            
            # QdrantVectorStoreì˜ BGE-m3 ëª¨ë¸ ì—…ë°ì´íŠ¸
            self.vector_store.bge_model = new_bge_model
            
            self.logger.info("BGE-m3 ëª¨ë¸ ì¬ë¡œë“œ ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.error(f"BGE-m3 ëª¨ë¸ ì¬ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def reload_reranker(self, config: Optional[Dict[str, Any]] = None):
        """ë¦¬ë­ì»¤ ëª¨ë¸ ë™ì  ì¬ë¡œë“œ"""
        self.logger.info("ë¦¬ë­ì»¤ ëª¨ë¸ ì¬ë¡œë“œ ì‹œì‘...")
        
        # ê¸°ì¡´ ë¦¬ë­ì»¤ í•´ì œ
        if self.reranker and hasattr(self.reranker, 'model'):
            del self.reranker.model
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            self.reranker = None
        
        # ìƒˆ ë¦¬ë­ì»¤ ë¡œë“œ
        try:
            if config is None:
                reranker_cfg = getattr(self.config, 'reranker', None)
            else:
                reranker_cfg = config
            
            if reranker_cfg:
                enabled = (reranker_cfg.enabled if hasattr(reranker_cfg, 'enabled') 
                          else reranker_cfg.get('enabled', False)) if reranker_cfg else False
                
                if enabled:
                    model_path = (reranker_cfg.model_path if hasattr(reranker_cfg, 'model_path') 
                                 else reranker_cfg.get('model_path', '')) if reranker_cfg else ''
                    device = (reranker_cfg.device if hasattr(reranker_cfg, 'device') 
                             else reranker_cfg.get('device', 'cuda')) if reranker_cfg else 'cuda'
                    batch_size = (reranker_cfg.batch_size if hasattr(reranker_cfg, 'batch_size') 
                                 else reranker_cfg.get('batch_size', 32)) if reranker_cfg else 32
                    
                    if model_path:
                        self.reranker = CrossEncoderReranker(
                            model_path=model_path,
                            device=device,
                            batch_size=batch_size,
                        )
                        self.logger.info("ë¦¬ë­ì»¤ ëª¨ë¸ ì¬ë¡œë“œ ì™„ë£Œ")
                        return True
            
            self.logger.info("ë¦¬ë­ì»¤ ë¹„í™œì„±í™”ë¨")
            return True
        except Exception as e:
            self.logger.error(f"ë¦¬ë­ì»¤ ëª¨ë¸ ì¬ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def delete_document(self, source_file: str) -> Dict[str, Any]:
        """
        íŠ¹ì • ë¬¸ì„œë¥¼ Qdrantì—ì„œ ì‚­ì œ (FAISS/BM25 ì œê±°ë¨)
        
        Args:
            source_file: ì‚­ì œí•  ë¬¸ì„œì˜ ì†ŒìŠ¤ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì‚­ì œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            self.logger.info(f"ë¬¸ì„œ ì‚­ì œ ì‹œì‘: {source_file}")
            
            # Qdrantì—ì„œ ì‚­ì œ (ì‚­ì œëœ ì²­í¬ ìˆ˜ ë°˜í™˜)
            qdrant_success, deleted_chunks_count = self.vector_store._delete_document_vectors(source_file)
            
            if qdrant_success:
                self.logger.info(f"ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {source_file}, {deleted_chunks_count}ê°œ ì²­í¬ ì‚­ì œë¨")
            else:
                self.logger.error(f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {source_file}")
            
            return {
                'success': qdrant_success,
                'qdrant_success': qdrant_success,
                'qdrant_deleted': qdrant_success,
                'deleted_chunks_count': deleted_chunks_count
            }
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {source_file}, ì˜¤ë¥˜: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {
                'success': False,
                'qdrant_success': False,
                'qdrant_deleted': False,
                'deleted_chunks_count': 0,
                'warnings': [f"ì‚­ì œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}"]
            }
    
    def process_and_store_documents(self, input_dir: str, force_update: bool = False, replace_mode: bool = False) -> bool:
        """ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ì¥"""
        try:
            self.logger.info(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {input_dir}")
            
            # 1. ë¬¸ì„œ ì²˜ë¦¬
            chunks = self.document_processor.process_directory(input_dir, force_update)
            if not chunks:
                self.logger.error("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            self.logger.info(f"ë¬¸ì„œ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
            
            # 2. ë²¡í„° ì €ì¥ì†Œì— ì €ì¥ (êµì²´ ëª¨ë“œ ë˜ëŠ” ì¼ë°˜ ëª¨ë“œ)
            # Sparse ë²¡í„°ëŠ” QdrantVectorStore.add_documentsì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨
            if replace_mode:
                # êµì²´ ëª¨ë“œ: íŒŒì¼ë³„ë¡œ ì™„ì „ êµì²´
                success = self._process_chunks_in_replace_mode(chunks)
            else:
                # ì¼ë°˜ ëª¨ë“œ: Qdrantì— ì €ì¥ (sparse_enabledì´ë©´ ìë™ìœ¼ë¡œ dense+sparse ë²¡í„° í•¨ê»˜ ì €ì¥)
                success = self.vector_store.add_documents(chunks, force_update)
            
            if not success:
                self.logger.error("ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨")
                return False
            
            # FAISS/BM25 ì¸ë±ìŠ¤ëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (Qdrant Dense+Sparseë§Œ ì‚¬ìš©)
            
            self.logger.info("ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ì¥ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def process_and_store_documents_async(self, input_dir: str, force_update: bool = False, replace_mode: bool = False) -> bool:
        """ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ì¥ (ë¹„ë™ê¸°)"""
        import asyncio
        
        try:
            self.logger.info(f"ë¹„ë™ê¸° ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {input_dir}")
            
            # 1. ë¬¸ì„œ ì²˜ë¦¬ (I/O ì‘ì—… - ë¹„ë™ê¸°í™”)
            chunks = await asyncio.to_thread(
                self.document_processor.process_directory,
                input_dir,
                force_update
            )
            if not chunks:
                self.logger.error("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            self.logger.info(f"ë¬¸ì„œ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
            
            # 2. ë²¡í„° ì €ì¥ì†Œì— ì €ì¥ (I/O ì‘ì—… - ë¹„ë™ê¸°í™”)
            if replace_mode:
                # êµì²´ ëª¨ë“œ: íŒŒì¼ë³„ë¡œ ì™„ì „ êµì²´
                success = await asyncio.to_thread(
                    self._process_chunks_in_replace_mode,
                    chunks
                )
            else:
                # ì¼ë°˜ ëª¨ë“œ: Qdrantì— ì €ì¥ (sparse_enabledì´ë©´ ìë™ìœ¼ë¡œ dense+sparse ë²¡í„° í•¨ê»˜ ì €ì¥)
                success = await asyncio.to_thread(
                    self.vector_store.add_documents,
                    chunks,
                    force_update
                )
            
            if not success:
                self.logger.error("ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨")
                return False
            
            # FAISS/BM25 ì¸ë±ìŠ¤ëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (Qdrant Dense+Sparseë§Œ ì‚¬ìš©)
            
            self.logger.info("ë¹„ë™ê¸° ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ì¥ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"ë¹„ë™ê¸° ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _process_chunks_in_replace_mode(self, chunks: List[DocumentChunk]) -> bool:
        """êµì²´ ëª¨ë“œì—ì„œ ì²­í¬ ì²˜ë¦¬ (íŒŒì¼ë³„ë¡œ ì™„ì „ êµì²´)"""
        try:
            import time
            total_start_time = time.time()
            
            # íŒŒì¼ë³„ë¡œ ì²­í¬ ê·¸ë£¹í™”
            file_chunks = {}
            for chunk in chunks:
                file_path = chunk.source_file
                if file_path not in file_chunks:
                    file_chunks[file_path] = []
                file_chunks[file_path].append(chunk)
            
            self.logger.info(f"êµì²´ ëª¨ë“œ ì²˜ë¦¬ ì‹œì‘: ì´ {len(file_chunks)}ê°œ íŒŒì¼, {len(chunks)}ê°œ ì²­í¬")
            
            # ê° íŒŒì¼ë³„ë¡œ ì™„ì „ êµì²´
            success_count = 0
            for idx, (file_path, file_chunk_list) in enumerate(file_chunks.items(), 1):
                filename = file_path.split('\\')[-1] if '\\' in file_path else file_path
                filename = filename.split('/')[-1] if '/' in filename else filename
                
                self.logger.info(
                    f"íŒŒì¼ ì²˜ë¦¬ ì¤‘: {idx}/{len(file_chunks)} | "
                    f"íŒŒì¼: {filename} | ì²­í¬ ìˆ˜: {len(file_chunk_list)}"
                )
                
                file_start_time = time.time()
                success = self.vector_store.replace_document_vectors(file_path, file_chunk_list)
                file_time = time.time() - file_start_time
                
                if success:
                    success_count += 1
                    self.logger.info(
                        f"íŒŒì¼ êµì²´ ì™„ë£Œ: {filename} | ì²­í¬ ìˆ˜: {len(file_chunk_list)} | "
                        f"ì²˜ë¦¬ ì‹œê°„: {file_time:.2f}ì´ˆ"
                    )
                else:
                    self.logger.error(f"íŒŒì¼ êµì²´ ì‹¤íŒ¨: {filename}")
            
            total_time = time.time() - total_start_time
            self.logger.info(
                f"êµì²´ ëª¨ë“œ ì²˜ë¦¬ ì™„ë£Œ: {success_count}/{len(file_chunks)}ê°œ íŒŒì¼ ì„±ê³µ | "
                f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ"
            )
            
            return success_count == len(file_chunks)
            
        except Exception as e:
            self.logger.error(f"êµì²´ ëª¨ë“œ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            import traceback
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False
    
    # ========== ë¹„ë™ê¸° ë©”ì„œë“œ (Phase 1: LLM í˜¸ì¶œ ë¹„ë™ê¸°í™”) ==========
    
    async def query_async(self, question: str, max_sources: Optional[int] = None, score_threshold: Optional[float] = None, model_name: Optional[str] = None, retrievers: Optional[Dict[str, bool]] = None, session_id: Optional[str] = None, dense_weight: Optional[float] = None, sparse_weight: Optional[float] = None) -> RAGResponse:
        """
        ë¹„ë™ê¸° ì§ˆì˜ì‘ë‹µ (LLM í˜¸ì¶œ, ê²€ìƒ‰, ë¦¬ë­í‚¹ ëª¨ë‘ ë¹„ë™ê¸° - Phase 3 ì™„ë£Œ)
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            max_sources: ìµœëŒ€ ì†ŒìŠ¤ ìˆ˜
            score_threshold: ì ìˆ˜ ì„ê³„ê°’
            model_name: ì‚¬ìš©í•  LLM ëª¨ë¸ëª…
            retrievers: ê²€ìƒ‰ê¸° ì„ íƒ ì •ë³´
            session_id: ì„¸ì…˜ ID (ì„ íƒì , ê¸°ë³¸ RAGì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ API í˜¸í™˜ì„±ì„ ìœ„í•´ ìˆ˜ë½)
        """
        start_time = time.time()
        
        # session_idëŠ” ê¸°ë³¸ RAG ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, API í˜¸í™˜ì„±ì„ ìœ„í•´ ìˆ˜ë½
        if session_id:
            self.logger.debug(f"ì„¸ì…˜ ID ìˆ˜ì‹ : {session_id} (ê¸°ë³¸ RAGì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)")
        
        try:
            self.logger.info(f"ë¹„ë™ê¸° ì§ˆì˜ ì²˜ë¦¬ ì‹œì‘: {question[:50]}...")
            
            # ê²€ìƒ‰ì— ì‚¬ìš©í•  ì§ˆë¬¸ (ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
            search_question = question
            
            # ê¸°ë³¸ê°’ ì ìš©
            max_sources = max_sources if max_sources is not None else self.rag_config.default_max_sources
            base_threshold = score_threshold if score_threshold is not None else self.rag_config.score_threshold
            
            # ë™ì  ì„ê³„ê°’ ì¡°ì •
            score_threshold = self._calculate_dynamic_threshold(
                question=search_question,
                base_threshold=base_threshold,
                max_sources=max_sources
            )
            
            self.logger.info(f"ë¬¸ì„œ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: max_sources={max_sources}, score_threshold={score_threshold:.3f} (ê¸°ë³¸ê°’: {base_threshold:.3f})")
            
            # ëª¨ë¸ ë³€ê²½ ì²˜ë¦¬
            if model_name and model_name != self.llm_client.model_name:
                self.logger.info(f"ëª¨ë¸ ë³€ê²½: {self.llm_client.model_name} -> {model_name}")
                llm_config = self.config.model.get('llm')
                model_config = {
                    'name': model_name,
                    'base_url': llm_config.base_url if hasattr(llm_config, 'base_url') else 'http://localhost:11434',
                    'max_tokens': llm_config.max_tokens if hasattr(llm_config, 'max_tokens') else 1000,
                    'temperature': llm_config.temperature if hasattr(llm_config, 'temperature') else 0.1,
                    'top_p': llm_config.top_p if hasattr(llm_config, 'top_p') else 0.9
                }
                self.llm_client = OllamaLLMClient(model_config)
            
            # ì¼ë°˜ì ì¸ ì§ˆë¬¸ì¸ì§€ í™•ì¸
            is_general = is_general_question(search_question)
            self.logger.debug(f"ì§ˆë¬¸ '{search_question}' ì¼ë°˜ ì§ˆë¬¸ íŒë³„ ê²°ê³¼: {is_general}")
            
            if is_general:
                # ì¼ë°˜ ì§ˆë¬¸ì€ ë²¡í„° ê²€ìƒ‰ ì—†ì´ ë°”ë¡œ LLMì— ì§ˆë¬¸ (ë¹„ë™ê¸°)
                self.logger.info(f"ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨: ë²¡í„° ê²€ìƒ‰ ê±´ë„ˆë›°ê¸° (ì§ˆë¬¸: '{search_question}')")
                llm_response = await self.llm_client.generate_answer_async(search_question, context="")
                answer = llm_response.text if llm_response else "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                is_general_flag = llm_response.is_general if llm_response else True
                
                return RAGResponse(
                    answer=answer,
                    sources=[],
                    confidence=1.0,
                    processing_time=time.time() - start_time,
                    query=question,
                    model_used=self.llm_client.model_name,
                    is_general_answer=is_general_flag,
                    is_rag_answer=False
                )
            
            # ì „ë¬¸ ì§ˆë¬¸ì´ë¯€ë¡œ ê²€ìƒ‰ ìˆ˜í–‰ (ë¹„ë™ê¸° - Phase 2)
            self.logger.info(f"ì „ë¬¸ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨: ê²€ìƒ‰ ìˆ˜í–‰ (ì§ˆë¬¸: '{search_question}')")
            
            # ê²€ìƒ‰ê¸° ì„ íƒì´ ì œê³µëœ ê²½ìš°
            if retrievers is not None:
                self.logger.info(f"ê²€ìƒ‰ê¸° ì„ íƒ ì‚¬ìš©: {retrievers}")
                
                # Qdrantë§Œ ì‚¬ìš© (FAISS/BM25 ì œê±°ë¨)
                selected_count = 1 if retrievers.get('use_qdrant', False) else 0
                
                search_limit = max_sources if selected_count == 1 else max_sources * 2
                self.logger.debug(f"ê²€ìƒ‰ê¸° ê°œìˆ˜: {selected_count}, ê²€ìƒ‰ ì œí•œ: {search_limit}")
                
                all_results = []
                
                # Qdrant ê²€ìƒ‰ (ë¹„ë™ê¸°)
                if retrievers.get('use_qdrant', False):
                    try:
                        # Dense/Sparse ê°€ì¤‘ì¹˜ ì¶”ì¶œ (retrievers > íŒŒë¼ë¯¸í„° > config ê¸°ë³¸ê°’)
                        from src.utils.config import get_qdrant_config
                        qdrant_config = get_qdrant_config()
                        config_dense_weight = getattr(qdrant_config, 'hybrid_search_dense_weight', 0.7)
                        config_sparse_weight = getattr(qdrant_config, 'hybrid_search_sparse_weight', 0.3)
                        
                        # ìš°ì„ ìˆœìœ„: retrievers > íŒŒë¼ë¯¸í„° > config ê¸°ë³¸ê°’
                        effective_dense_weight = retrievers.get('dense_weight')
                        if effective_dense_weight is None:
                            effective_dense_weight = dense_weight
                        if effective_dense_weight is None:
                            effective_dense_weight = config_dense_weight
                        
                        effective_sparse_weight = retrievers.get('sparse_weight')
                        if effective_sparse_weight is None:
                            effective_sparse_weight = sparse_weight
                        if effective_sparse_weight is None:
                            effective_sparse_weight = config_sparse_weight
                        
                        self.logger.debug(f"Qdrant ê²€ìƒ‰ ê°€ì¤‘ì¹˜: dense={effective_dense_weight:.2f}, sparse={effective_sparse_weight:.2f}")
                        
                        qdrant_results = await self.vector_store.search_similar_async(
                            query=search_question,
                            limit=search_limit,
                            score_threshold=score_threshold,
                            dense_weight=effective_dense_weight,
                            sparse_weight=effective_sparse_weight
                        )
                        if qdrant_results:
                            all_results.append(('qdrant', qdrant_results))
                            self.logger.debug(f"Qdrant ê²€ìƒ‰ ê²°ê³¼: {len(qdrant_results)}ê°œ")
                    except Exception as e:
                        self.logger.warning(f"Qdrant ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                
                # FAISS/BM25 ê²€ìƒ‰ì€ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (Qdrant Dense+Sparseë§Œ ì‚¬ìš©)
                
                if not all_results:
                    self.logger.warning("ì„ íƒëœ ê²€ìƒ‰ê¸°ì—ì„œ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    similar_docs = []
                elif selected_count == 1:
                    raw_results = all_results[0][1][:max_sources]
                    similar_docs = []
                    for item in raw_results:
                        if 'score' in item:
                            item['score'] = float(item['score'])
                        similar_docs.append(item)
                    self.logger.info(f"ë‹¨ì¼ ê²€ìƒ‰ê¸° ì‚¬ìš©: {all_results[0][0]}, ê²°ê³¼ {len(similar_docs)}ê°œ")
                else:
                    # ë‹¤ì¤‘ ê²€ìƒ‰ê¸°: RRF í†µí•© (Qdrantë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ë‹¨ì¼ ê²€ìƒ‰ê¸°ì™€ ë™ì¼)
                    weights = retrievers.get('weights') or {'qdrant': 1.0}
                    name_to_weight = {
                        'qdrant': float(weights.get('qdrant', 1.0)),
                    }
                    results_list = [results for _, results in all_results]
                    retriever_names = [name for name, _ in all_results]
                    rrf_scores: Dict[str, float] = {}
                    data_map: Dict[str, Dict[str, Any]] = {}
                    K = 60
                    for idx, results in enumerate(results_list):
                        name = retriever_names[idx]
                        w = name_to_weight.get(name, 0.0)
                        if not results or w <= 0:
                            continue
                        for rank, res in enumerate(results, 1):
                            chunk_id = (
                                res.get('chunk_id') or
                                res.get('metadata', {}).get('chunk_id') or
                                res.get('id', '')
                            )
                            if not chunk_id:
                                content = res.get('content', res.get('page_content', ''))
                                import hashlib
                                chunk_id = hashlib.md5(content.encode()).hexdigest()
                            contrib = w * (1.0 / (K + rank))
                            rrf_scores[chunk_id] = rrf_scores.get(chunk_id, 0.0) + contrib
                            if chunk_id not in data_map:
                                data_map[chunk_id] = res.copy()
                    sorted_ids = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)
                    merged = []
                    for cid in sorted_ids[:max_sources]:
                        item = data_map[cid].copy()
                        original_score = item.get('score', 0.0)
                        item['score'] = float(original_score)
                        item['rrf_score'] = float(rrf_scores[cid])
                        merged.append(item)
                    similar_docs = merged
                    self.logger.info(f"ê²€ìƒ‰ê¸° í†µí•© ì™„ë£Œ: {len(similar_docs)}ê°œ ê²°ê³¼")
                
                # ë¦¬ë­í‚¹ ì ìš© (ë™ê¸°)
                use_reranker = bool(retrievers.get('use_reranker', True)) if retrievers else True
                if similar_docs and use_reranker and self.reranker:
                    try:
                        reranker_cfg = getattr(self.config, 'reranker', {})
                        default_alpha = (getattr(reranker_cfg, 'alpha', 0.7) if not isinstance(reranker_cfg, dict) else reranker_cfg.get('alpha', 0.7))
                        default_top_k = (getattr(reranker_cfg, 'top_k', max_sources) if not isinstance(reranker_cfg, dict) else reranker_cfg.get('top_k', max_sources))
                        alpha = float(retrievers.get('reranker_alpha', default_alpha))
                        for d in similar_docs:
                            if not d.get('content') and d.get('page_content'):
                                d['content'] = d.get('page_content')
                        reranker_top_k_value = retrievers.get('reranker_top_k')
                        if reranker_top_k_value is None:
                            fallback_top_k = default_top_k if default_top_k is not None else (max_sources if max_sources is not None else 10)
                            requested_top_k = fallback_top_k
                        else:
                            requested_top_k = reranker_top_k_value
                        requested_top_k = int(requested_top_k)
                        safe_max_sources = max_sources if max_sources is not None else len(similar_docs)
                        top_k = max(1, min(requested_top_k, safe_max_sources, len(similar_docs)))
                        self.logger.info(f"ë¦¬ë­ì»¤ í˜¸ì¶œ ì‹œì‘: docs={len(similar_docs)}, top_k={top_k}, alpha={alpha}")
                        reranked_docs = await self.reranker.rerank_async(question, similar_docs, top_k=top_k)
                        if not reranked_docs:
                            self.logger.warning("ë¦¬ë­ì»¤ê°€ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                            reranked_docs = similar_docs
                        similar_docs = reranked_docs
                        for d in similar_docs:
                            base_score = float(d.get('score', 0.0))
                            rr_score = float(d.get('reranker_score', 0.0))
                            d['score'] = alpha * rr_score + (1.0 - alpha) * base_score
                        similar_docs.sort(key=lambda x: x.get('score', 0.0), reverse=True)
                        similar_docs = similar_docs[:max_sources]
                    except Exception as e:
                        self.logger.warning(f"ë¦¬ë­í‚¹ ì ìš© ì‹¤íŒ¨: {str(e)}")
            else:
                # retrieversê°€ Noneì¼ ë•Œ: ê¸°ë³¸ Qdrant ê²€ìƒ‰ ì‚¬ìš© (dense/sparse ê°€ì¤‘ì¹˜ ì§€ì›)
                from src.utils.config import get_qdrant_config
                qdrant_config = get_qdrant_config()
                
                # Dense/Sparse ê°€ì¤‘ì¹˜ ê²°ì • (íŒŒë¼ë¯¸í„° > retrievers > config ê¸°ë³¸ê°’)
                effective_dense_weight = dense_weight
                effective_sparse_weight = sparse_weight
                
                if effective_dense_weight is None or effective_sparse_weight is None:
                    # configì—ì„œ ê¸°ë³¸ê°’ ê°€ì ¸ì˜¤ê¸°
                    config_dense_weight = getattr(qdrant_config, 'hybrid_search_dense_weight', 0.7)
                    config_sparse_weight = getattr(qdrant_config, 'hybrid_search_sparse_weight', 0.3)
                    
                    if effective_dense_weight is None:
                        effective_dense_weight = config_dense_weight
                    if effective_sparse_weight is None:
                        effective_sparse_weight = config_sparse_weight
                
                self.logger.info(f"ê¸°ë³¸ Qdrant ê²€ìƒ‰ ì‚¬ìš©: dense_weight={effective_dense_weight:.2f}, sparse_weight={effective_sparse_weight:.2f}")
                
                # Qdrant ê²€ìƒ‰ (dense/sparse ê°€ì¤‘ì¹˜ ì „ë‹¬)
                similar_docs = await self.vector_store.search_similar_async(
                    query=search_question,
                    limit=max_sources,
                    score_threshold=score_threshold,
                    dense_weight=effective_dense_weight,
                    sparse_weight=effective_sparse_weight
                )
                
                # ì„¤ì • ê¸°ë°˜ ë¦¬ë­í‚¹ (ë¹„ë™ê¸° - Phase 3)
                if similar_docs and self.reranker:
                    try:
                        reranker_cfg = getattr(self.config, 'reranker', {})
                        enabled = (getattr(reranker_cfg, 'enabled', False) if not isinstance(reranker_cfg, dict) else reranker_cfg.get('enabled', False))
                        if enabled:
                            alpha = (getattr(reranker_cfg, 'alpha', 0.7) if not isinstance(reranker_cfg, dict) else reranker_cfg.get('alpha', 0.7))
                            configured_top_k = (getattr(reranker_cfg, 'top_k', max_sources) if not isinstance(reranker_cfg, dict) else reranker_cfg.get('top_k', max_sources))
                            if configured_top_k is None:
                                fallback_top_k = max_sources if max_sources is not None else 10
                            else:
                                fallback_top_k = configured_top_k
                            safe_max_sources = max_sources if max_sources is not None else len(similar_docs)
                            top_k = max(1, min(int(fallback_top_k), safe_max_sources, len(similar_docs)))
                            self.logger.info(f"ë¦¬ë­ì»¤ í˜¸ì¶œ ì‹œì‘ (ì„¤ì • ê¸°ë°˜): docs={len(similar_docs)}, top_k={top_k}, alpha={alpha}")
                            for d in similar_docs:
                                if not d.get('content') and d.get('page_content'):
                                    d['content'] = d.get('page_content')
                            reranked_docs = await self.reranker.rerank_async(question, similar_docs, top_k=top_k)
                            if not reranked_docs:
                                self.logger.warning("ë¦¬ë­ì»¤ê°€ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                                reranked_docs = similar_docs
                            for d in similar_docs:
                                base_score = float(d.get('score', 0.0))
                                rr_score = float(d.get('reranker_score', 0.0))
                                d['score'] = float(alpha) * rr_score + (1.0 - float(alpha)) * base_score
                            similar_docs.sort(key=lambda x: x.get('score', 0.0), reverse=True)
                            similar_docs = similar_docs[:max_sources]
                    except Exception as e:
                        self.logger.warning(f"ë¦¬ë­í‚¹ ì ìš© ì‹¤íŒ¨: {str(e)}")
            
            # ê²€ìƒ‰ ê²°ê³¼ ì •ë ¬
            similar_docs.sort(key=lambda x: x.get('score', 0), reverse=True)
            
            # ê²€ìƒ‰ ê²°ê³¼ ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬ (ë¹„ë™ê¸° LLM í˜¸ì¶œ)
            if similar_docs:
                max_score = max(doc.get('score', 0) for doc in similar_docs)
                avg_score = sum(doc.get('score', 0) for doc in similar_docs) / len(similar_docs)
                low_score_threshold = self.rag_config.low_score_general_threshold
                
                if max_score < low_score_threshold and avg_score < low_score_threshold:
                    self.logger.info(f"ê²€ìƒ‰ ì ìˆ˜ê°€ ë‚®ì•„ ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ì „í™˜: ìµœê³ ì ìˆ˜={max_score:.3f}, í‰ê· ì ìˆ˜={avg_score:.3f}")
                    llm_response = await self.llm_client.generate_answer_async(question, context="")
                    answer = llm_response.text if llm_response else "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    
                    return RAGResponse(
                        answer=answer,
                        sources=[],
                        confidence=0.3,
                        processing_time=time.time() - start_time,
                        query=question,
                        model_used=self.llm_client.model_name,
                        is_general_answer=True,
                        is_rag_answer=False
                    )
            elif not similar_docs:
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬ (ë¹„ë™ê¸° LLM í˜¸ì¶œ)
                self.logger.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬")
                llm_response = await self.llm_client.generate_answer_async(question, context="")
                answer = llm_response.text if llm_response else "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
                return RAGResponse(
                    answer=answer,
                    sources=[],
                    confidence=0.2,
                    processing_time=time.time() - start_time,
                    query=question,
                    model_used=self.llm_client.model_name,
                    is_general_answer=True,
                    is_rag_answer=False
                )
            
            # ê²€ìƒ‰ ê²°ê³¼ ë¡œê¹…
            self.logger.info(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ì‹œì‘: ì´ {len(similar_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
            for idx, doc in enumerate(similar_docs, 1):
                source_file = doc.get('source_file', 'N/A')
                chunk_index = doc.get('chunk_index', 'N/A')
                score = doc.get('score', 0.0)
                self.logger.info(f"  [{idx}] íŒŒì¼: {source_file}, ì²­í¬: {chunk_index}, ì ìˆ˜: {score:.4f}")
            
            # ì¤‘ë³µ ì²­í¬ ì œê±°
            unique_docs = []
            seen_chunks = set()
            for doc in similar_docs:
                chunk_key = f"{doc.get('source_file', '')}:{doc.get('chunk_index', '')}"
                if chunk_key not in seen_chunks:
                    seen_chunks.add(chunk_key)
                    unique_docs.append(doc)
            removed_duplicates = len(similar_docs) - len(unique_docs)
            if removed_duplicates > 0:
                self.logger.info(f"ğŸ”„ ì¤‘ë³µ ì²­í¬ ì œê±°: {removed_duplicates}ê°œ ì œê±°ë¨ (ë‚¨ì€ ë¬¸ì„œ: {len(unique_docs)}ê°œ)")
            similar_docs = unique_docs
            
            # í‘œ ë°ì´í„° ì¤‘ë³µ ì œê±°
            if len(similar_docs) > 1:
                table_docs = [doc for doc in similar_docs if 'í‘œ ë°ì´í„°' in doc.get('content', '')]
                if len(table_docs) > 1:
                    table_docs.sort(key=lambda x: x.get('score', 0), reverse=True)
                    removed_table_duplicates = 0
                    for table_doc in table_docs[1:]:
                        if table_doc in similar_docs:
                            similar_docs.remove(table_doc)
                            removed_table_duplicates += 1
                    if removed_table_duplicates > 0:
                        self.logger.info(f"ğŸ”„ í‘œ ë°ì´í„° ì¤‘ë³µ ì œê±°: {removed_table_duplicates}ê°œ ì œê±°ë¨ (ë‚¨ì€ ë¬¸ì„œ: {len(similar_docs)}ê°œ)")
            
            if not similar_docs:
                return RAGResponse(
                    answer="ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    sources=[],
                    confidence=0.0,
                    processing_time=time.time() - start_time,
                    query=question,
                    model_used=""
                )
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (í† í° ì œí•œ ìë™ ì¡°ì •)
            context = self._build_context(similar_docs, max_tokens=None)
            
            # LLMì„ í†µí•œ ë‹µë³€ ìƒì„± (ë¹„ë™ê¸°) - ì •ì œëœ ì§ˆë¬¸ ì‚¬ìš©
            llm_response = await self.llm_client.generate_answer_async(search_question, context)
            answer = llm_response.text if llm_response else "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            is_general = llm_response.is_general if llm_response else False
            has_rag_context = llm_response.has_rag_context if llm_response else True
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence(similar_docs, answer)
            
            # ì†ŒìŠ¤ ì •ë³´ ì •ë¦¬
            sources = self._format_sources(similar_docs)
            self.logger.info(f"ğŸ“š ìµœì¢… ì°¸ì¡° ë¬¸ì„œ: {len(sources)}ê°œ")
            for idx, source in enumerate(sources, 1):
                source_file = source.get('source_file', 'N/A')
                chunk_index = source.get('chunk_index', 'N/A')
                relevance_score = source.get('relevance_score', 0.0)
                self.logger.info(f"  [{idx}] íŒŒì¼: {source_file}, ì²­í¬: {chunk_index}, ê´€ë ¨ë„: {relevance_score:.4f}")
            
            processing_time = time.time() - start_time
            
            self.logger.info(f"ë¹„ë™ê¸° ì§ˆì˜ ì²˜ë¦¬ ì™„ë£Œ: {processing_time:.2f}ì´ˆ, ì‹ ë¢°ë„: {confidence:.2f}, ì¼ë°˜ë‹µë³€={is_general}, RAGë‹µë³€={has_rag_context}")
            
            return RAGResponse(
                answer=answer,
                sources=sources,
                confidence=confidence,
                processing_time=processing_time,
                query=question,
                model_used=self.llm_client.model_name,
                is_general_answer=is_general,
                is_rag_answer=has_rag_context
            )
            
        except Exception as e:
            self.logger.error(f"ë¹„ë™ê¸° ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return RAGResponse(
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                sources=[],
                confidence=0.0,
                processing_time=time.time() - start_time,
                query=question,
                model_used=""
            )
    
    def _calculate_dynamic_threshold(self, question: str, base_threshold: float, max_sources: int) -> float:
        """
        ë™ì  ì„ê³„ê°’ ê³„ì‚°
        
        ì§ˆë¬¸ ìœ í˜•ê³¼ ìš”ì²­ëœ ë¬¸ì„œ ìˆ˜ì— ë”°ë¼ ì„ê³„ê°’ì„ ì¡°ì •í•©ë‹ˆë‹¤.
        ê¸°ë³¸ ì„ê³„ê°’ì´ ë‚®ì•„ì¡Œìœ¼ë¯€ë¡œ ì¡°ì • í­ì„ ì¤„ì—¬ ë” ê´€ëŒ€í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            base_threshold: ê¸°ë³¸ ì„ê³„ê°’
            max_sources: ìš”ì²­ëœ ìµœëŒ€ ì†ŒìŠ¤ ìˆ˜
            
        Returns:
            ì¡°ì •ëœ ì„ê³„ê°’
        """
        threshold = base_threshold
        
        # 1. ì§ˆë¬¸ ê¸¸ì´ì— ë”°ë¥¸ ì¡°ì • (ë” ê´€ëŒ€í•˜ê²Œ)
        question_length = len(question.strip())
        if question_length < 10:
            # ë§¤ìš° ì§§ì€ ì§ˆë¬¸: ì„ê³„ê°’ ì•½ê°„ ì¦ê°€
            threshold += 0.05
        elif question_length > 50:
            # ê¸´ ì§ˆë¬¸: ì„ê³„ê°’ ê°ì†Œ (ë” ë§ì€ ê²°ê³¼ í¬í•¨)
            threshold -= 0.05
        
        # 2. ìš”ì²­ëœ ë¬¸ì„œ ìˆ˜ì— ë”°ë¥¸ ì¡°ì • (ë” ê´€ëŒ€í•˜ê²Œ)
        if max_sources <= 3:
            # ì ì€ ìˆ˜ì˜ ë¬¸ì„œ ìš”ì²­: ì„ê³„ê°’ ì•½ê°„ ì¦ê°€
            threshold += 0.05
        elif max_sources >= 10:
            # ë§ì€ ìˆ˜ì˜ ë¬¸ì„œ ìš”ì²­: ì„ê³„ê°’ ê°ì†Œ (ë” ë„“ì€ ë²”ìœ„)
            threshold -= 0.05
        
        # 3. ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ì¡°ì •
        question_lower = question.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì§ˆë¬¸ (ì˜ˆ: "ë³€ì••ê¸° ì§„ë‹¨ ê¸°ì¤€")
        if any(keyword in question_lower for keyword in ['ê¸°ì¤€', 'ë°©ë²•', 'ì ˆì°¨', 'ê³¼ì •', 'ì›ë¦¬']):
            # êµ¬ì²´ì ì¸ ì •ë³´ ìš”ì²­: ì„ê³„ê°’ ì•½ê°„ ê°ì†Œ
            threshold -= 0.03
        
        # ë¹„êµ/ë¶„ì„ ì§ˆë¬¸ (ì˜ˆ: "ì°¨ì´ì ", "ë¹„êµ")
        if any(keyword in question_lower for keyword in ['ì°¨ì´', 'ë¹„êµ', 'ë¶„ì„', 'ëŒ€ë¹„']):
            # ì—¬ëŸ¬ ë¬¸ì„œ ë¹„êµ í•„ìš”: ì„ê³„ê°’ ê°ì†Œ
            threshold -= 0.05
        
        # í‘œ/ë°ì´í„° ì§ˆë¬¸
        if any(keyword in question_lower for keyword in ['í‘œ', 'table', 'ë°ì´í„°', 'ìˆ˜ì¹˜']):
            # í‘œ ë°ì´í„°ëŠ” ì •í™•í•œ ë§¤ì¹­ í•„ìš”: ì„ê³„ê°’ ì•½ê°„ ì¦ê°€
            threshold += 0.03
        
        # ì„ê³„ê°’ ë²”ìœ„ ì œí•œ (0.0 ~ 1.0)
        # ìµœì†Œ ì„ê³„ê°’ì„ 0.2ë¡œ ì„¤ì •í•˜ì—¬ ë„ˆë¬´ ë‚®ì€ ì ìˆ˜ëŠ” ì œì™¸
        threshold = max(0.2, min(1.0, threshold))
        
        return threshold
    
    def _estimate_tokens(self, text: str) -> int:
        """
        í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ì¶”ì • (í•œêµ­ì–´ ê¸°ì¤€)
        
        í•œêµ­ì–´ì˜ ê²½ìš° ì¼ë°˜ì ìœ¼ë¡œ 1í† í° â‰ˆ 2-3ì ì •ë„ì…ë‹ˆë‹¤.
        ë³´ìˆ˜ì ìœ¼ë¡œ 1í† í° = 2.5ìë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            text: í† í° ìˆ˜ë¥¼ ì¶”ì •í•  í…ìŠ¤íŠ¸
            
        Returns:
            ì¶”ì •ëœ í† í° ìˆ˜
        """
        # í•œêµ­ì–´ ê¸°ì¤€: 1í† í° â‰ˆ 2.5ì
        # ì˜ì–´ ê¸°ì¤€: 1í† í° â‰ˆ 4ì
        # í˜¼í•© í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•˜ì—¬ í‰ê· ê°’ ì‚¬ìš©
        return int(len(text) / 2.5)
    
    def _build_context(self, similar_docs: List[Dict[str, Any]], max_tokens: Optional[int] = None) -> str:
        """
        ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (í† í° ì œí•œ ìë™ ì¡°ì •)
        
        Args:
            similar_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
            
        Returns:
            êµ¬ì„±ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        context_parts = []
        total_tokens = 0
        
        # LLMì˜ max_tokens ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’: 1000)
        if max_tokens is None:
            llm_max_tokens = getattr(self.llm_client, 'max_tokens', 1000)
            # ì»¨í…ìŠ¤íŠ¸ëŠ” max_tokensì˜ 70% ì •ë„ ì‚¬ìš© (ë‚˜ë¨¸ì§€ëŠ” ë‹µë³€ ìƒì„±ìš©)
            max_tokens = int(llm_max_tokens * 0.7)
        
        for i, doc in enumerate(similar_docs, 1):
            # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ (ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ)
            filename = doc['source_file'].split('\\')[-1] if '\\' in doc['source_file'] else doc['source_file']
            filename = filename.split('/')[-1] if '/' in filename else filename
            
            # ê´€ë ¨ë„ ì ìˆ˜ í¬í•¨
            relevance_score = doc.get('score', 0)
            source_info = f"[ë¬¸ì„œ {i}] ì¶œì²˜: {filename} (ê´€ë ¨ë„: {relevance_score:.3f})"
            
            # ì²­í¬ ì¸ë±ìŠ¤ ì •ë³´ ì¶”ê°€
            chunk_info = f"ì²­í¬ ì¸ë±ìŠ¤: {doc.get('chunk_index', 'N/A')}"
            
            content = doc['content']
            
            # ì»¨í…ìŠ¤íŠ¸ ë¶€ë¶„ êµ¬ì„±
            context_part = f"{source_info}\n{chunk_info}\n{content}"
            part_tokens = self._estimate_tokens(context_part)
            
            # í† í° ì œí•œ í™•ì¸
            if max_tokens and total_tokens + part_tokens > max_tokens:
                # í† í° ì œí•œ ì´ˆê³¼ ì‹œ í˜„ì¬ ë¬¸ì„œì˜ ë‚´ìš©ì„ ìë¦„
                remaining_tokens = max_tokens - total_tokens - self._estimate_tokens(f"{source_info}\n{chunk_info}\n")
                if remaining_tokens > 0:
                    # ë‚¨ì€ í† í° ìˆ˜ì— ë§ì¶° ë‚´ìš© ìë¥´ê¸°
                    max_chars = int(remaining_tokens * 2.5)  # í† í° â†’ ë¬¸ì ë³€í™˜
                    truncated_content = content[:max_chars] + "..."
                    context_part = f"{source_info}\n{chunk_info}\n{truncated_content}"
                    context_parts.append(context_part)
                    self.logger.warning(
                        f"í† í° ì œí•œìœ¼ë¡œ ì¸í•´ ë¬¸ì„œ {i}ì˜ ë‚´ìš©ì´ ì˜ë ¸ìŠµë‹ˆë‹¤. "
                        f"(ì´ í† í°: {total_tokens + self._estimate_tokens(context_part)}/{max_tokens})"
                    )
                break
            
            context_parts.append(context_part)
            total_tokens += part_tokens
        
        if max_tokens and total_tokens > 0:
            self.logger.debug(f"ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {len(context_parts)}ê°œ ë¬¸ì„œ, {total_tokens}í† í° (ì œí•œ: {max_tokens}í† í°)")
        
        return "\n\n".join(context_parts)
    
    def _calculate_confidence(self, similar_docs: List[Dict[str, Any]], answer: str) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        if not similar_docs:
            return 0.0
        
        # í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
        avg_score = sum(doc['score'] for doc in similar_docs) / len(similar_docs)
        
        # ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        confidence = min(avg_score, 1.0)
        
        return confidence
    
    def _format_sources(self, similar_docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì†ŒìŠ¤ ì •ë³´ í¬ë§·íŒ…"""
        sources = []
        preview_length = self.rag_config.content_preview_length
        
        # ì ìˆ˜ ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€/ìµœì†Œ ì ìˆ˜ ê³„ì‚°
        if similar_docs:
            scores = [doc.get('score', 0.0) for doc in similar_docs]
            if not scores:
                max_score = 1.0
                min_score = 0.0
                score_range = 1.0
            else:
                max_score = max(scores)
                min_score = min(scores)
                score_range = max_score - min_score if max_score > min_score else 1.0
                
                # ëª¨ë“  ì ìˆ˜ê°€ ê°™ì„ ë•Œ ì²˜ë¦¬ (ì •ê·œí™” ë¶ˆê°€ëŠ¥)
                if score_range == 0.0:
                    # ëª¨ë“  ì ìˆ˜ê°€ ê°™ìœ¼ë©´ ì›ë³¸ ì ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì •ê·œí™” ì—†ìŒ)
                    self.logger.warning(
                        f"ëª¨ë“  ì ìˆ˜ê°€ ë™ì¼í•¨ ({max_score:.4f}). "
                        f"ì •ê·œí™” ì—†ì´ ì›ë³¸ ì ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. "
                        f"ë¬¸ì„œ ìˆ˜: {len(similar_docs)}ê°œ"
                    )
                    score_range = 1.0  # ë‚˜ëˆ—ì…ˆ ì˜¤ë¥˜ ë°©ì§€
                else:
                    # ì •ê·œí™” ê°€ëŠ¥í•œ ê²½ìš° ì ìˆ˜ ë²”ìœ„ ë¡œê·¸
                    self.logger.debug(
                        f"ì ìˆ˜ ì •ê·œí™”: ë²”ìœ„={min_score:.4f}~{max_score:.4f}, "
                        f"ë¬¸ì„œ ìˆ˜={len(similar_docs)}ê°œ"
                    )
        else:
            max_score = 1.0
            min_score = 0.0
            score_range = 1.0
            self.logger.warning("ì •ê·œí™”í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©")
        
        for doc in similar_docs:
            content = doc['content']
            preview = content[:preview_length] + "..." if len(content) > preview_length else content
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ê³„ì¸µ ì •ë³´ ì¶”ì¶œ
            metadata = doc.get('metadata', {})
            source_parts = []
            
            # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ (ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ)
            source_file = doc.get('source_file', '')
            filename = source_file.split('\\')[-1] if '\\' in source_file else source_file
            filename = filename.split('/')[-1] if '/' in filename else filename
            source_parts.append(filename)
            
            # heading, sub-heading, sub-sub-heading ìˆœì„œë¡œ ì¶”ê°€
            if metadata.get('heading'):
                source_parts.append(metadata.get('heading'))
            if metadata.get('sub-heading'):
                source_parts.append(metadata.get('sub-heading'))
            if metadata.get('sub-sub-heading'):
                source_parts.append(metadata.get('sub-sub-heading'))

            # í‘œ ë°ì´í„°ì¸ ê²½ìš° í‘œ ì œëª©ì„ ì¶œì²˜ ê²½ë¡œ ë§ˆì§€ë§‰ì— ì¶”ê°€
            is_table_data = bool(metadata.get('is_table_data')) or ('í‘œ ë°ì´í„°' in content)
            if is_table_data:
                table_title = metadata.get('table_title')
                if not table_title:
                    # ì»¨í…ì¸ ì—ì„œ í‘œ ì œëª© ì¶”ì¶œ ì‹œë„ (ì˜ˆ: "í‘œ 5-18, ...")
                    import re
                    m = re.search(r'(í‘œ\s*\d+[\-.]?\d*[,\.:]?\s*[^\n]+)', content)
                    if m:
                        table_title = m.group(1).strip()
                if table_title:
                    source_parts.append(table_title)
            
            # ì¶œì²˜ ê²½ë¡œ ìƒì„± ("> "ë¡œ êµ¬ë¶„)
            source_path = " > ".join(source_parts) if source_parts else filename
            
            # ì ìˆ˜ ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
            raw_score = doc.get('score', 0.0)
            
            # ì›ë³¸ ì ìˆ˜ ê²€ì¦
            if raw_score < 0.0 or raw_score > 1.0:
                self.logger.warning(
                    f"ì›ë³¸ ì ìˆ˜ê°€ 0-1 ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨: {raw_score:.4f}. "
                    f"ìë™ í´ë¦¬í•‘ ì ìš©"
                )
                raw_score = max(0.0, min(1.0, raw_score))
            
            # ëª¨ë“  ì ìˆ˜ê°€ ê°™ì„ ë•ŒëŠ” ì›ë³¸ ì ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if max_score == min_score:
                # ëª¨ë“  ì ìˆ˜ê°€ ê°™ìœ¼ë©´ ì›ë³¸ ì ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì •ê·œí™” ì—†ìŒ)
                normalized_score = raw_score
            else:
                # ê°œì„ ëœ ì •ê·œí™”: ìµœì†Œê°’ì„ 0ìœ¼ë¡œ ë§Œë“¤ì§€ ì•Šê³ , ìµœëŒ€ê°’ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ì  ì ìˆ˜ ê³„ì‚°
                # ì´ë ‡ê²Œ í•˜ë©´ ìµœì†Œê°’ ë¬¸ì„œë„ 0ì´ ì•„ë‹Œ ê°’ì„ ê°€ì§
                # ë°©ë²•: score / max (ìµœëŒ€ê°’ ê¸°ì¤€ ì •ê·œí™”)
                # ì˜ˆ: ì ìˆ˜ 0.6491 / ìµœëŒ€ê°’ 0.7318 = 0.887 (0ì´ ì•„ë‹Œ ê°’)
                if max_score > 0:
                    # ìµœëŒ€ê°’ ê¸°ì¤€ ì •ê·œí™” (ì›ë³¸ ì ìˆ˜ì˜ ìƒëŒ€ì  ë¹„ìœ¨ ìœ ì§€)
                    # ëª¨ë“  ë¬¸ì„œê°€ ìµœëŒ€ê°’ ëŒ€ë¹„ ìƒëŒ€ì  ì ìˆ˜ë¥¼ ê°€ì§€ë¯€ë¡œ, ìµœì†Œê°’ë„ 0ì´ ì•„ë‹˜
                    normalized_score = raw_score / max_score
                    self.logger.debug(
                        f"ìµœëŒ€ê°’ ê¸°ì¤€ ì •ê·œí™”: {raw_score:.4f} / {max_score:.4f} = {normalized_score:.4f}"
                    )
                else:
                    # max_scoreê°€ 0ì´ë©´ ì›ë³¸ ì ìˆ˜ ì‚¬ìš©
                    self.logger.warning(f"max_scoreê°€ 0ì…ë‹ˆë‹¤. ì›ë³¸ ì ìˆ˜ ì‚¬ìš©: {raw_score:.4f}")
                    normalized_score = raw_score
            
            # 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
            normalized_score = max(0.0, min(1.0, normalized_score))
            
            # ì •ê·œí™” ê²°ê³¼ ê²€ì¦
            if normalized_score < 0.0 or normalized_score > 1.0:
                self.logger.error(
                    f"ì •ê·œí™” í›„ ì ìˆ˜ê°€ 0-1 ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨: {normalized_score:.4f}. "
                    f"ì›ë³¸ ì ìˆ˜: {raw_score:.4f}, ë²”ìœ„: {min_score:.4f}~{max_score:.4f}"
                )
                normalized_score = max(0.0, min(1.0, normalized_score))
            
            source = {
                'content': preview,
                'source_file': doc['source_file'],
                'source_path': source_path,  # ê³„ì¸µ í˜•ì‹ ì¶œì²˜ ê²½ë¡œ ì¶”ê°€
                'relevance_score': normalized_score,  # ì •ê·œí™”ëœ ì ìˆ˜ (0-1 ë²”ìœ„)
                'raw_score': raw_score,  # ì›ë³¸ ì ìˆ˜ë„ ë³´ì¡´ (ë””ë²„ê¹…ìš©)
                'chunk_index': doc['chunk_index'],
                'metadata': metadata  # ë©”íƒ€ë°ì´í„° ì „ì²´ë„ í¬í•¨
            }
            sources.append(source)
        
        return sources
    
    def get_system_stats(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í†µê³„ ë°˜í™˜"""
        try:
            # ë²¡í„° ì €ì¥ì†Œ í†µê³„
            vector_stats = self.vector_store.get_collection_info()
            
            # ì„ë² ë”© ìºì‹œ í†µê³„
            embedding_stats = {
                'cache_size': 0,  # BGE-m3ëŠ” ìºì‹œë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                'model_name': self.config.model.get('embedding', {}).name if 'embedding' in self.config.model else 'unknown',
                'dimension': self.config.model.get('embedding', {}).dimension if 'embedding' in self.config.model else 1024
            }
            
            return {
                'vector_store_stats': vector_stats,
                'embedding_cache_stats': embedding_stats,
                'llm_model': self.config.model.get('llm', {}).name if 'llm' in self.config.model else 'unknown'
            }
            
        except Exception as e:
            self.logger.error(f"ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {
                'vector_store_stats': {},
                'embedding_cache_stats': {'cache_size': 0, 'model_name': 'unknown', 'dimension': 1024},
                'llm_model': 'unknown'
            }
    
    def get_documents_info(self) -> List[Dict[str, Any]]:
        """ì €ì¥ëœ ë¬¸ì„œë“¤ì˜ ì •ë³´ ë°˜í™˜"""
        try:
            return self.vector_store.get_documents_info()
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def get_document_chunks(self, document_id: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ë¬¸ì„œì˜ ì²­í¬ ì •ë³´ ë°˜í™˜"""
        try:
            return self.vector_store.get_document_chunks(document_id)
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ì²­í¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def query_by_table_title(self, 
                            table_title: str, 
                            question: str = "",
                            max_sources: Optional[int] = None, 
                            score_threshold: Optional[float] = None, 
                            model_name: Optional[str] = None) -> RAGResponse:
        """í‘œ ì œëª©ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ì§ˆì˜ì‘ë‹µ"""
        start_time = time.time()
        
        try:
            self.logger.info(f"í‘œ ì œëª© ê²€ìƒ‰ ì‹œì‘: {table_title}")
            
            # ê¸°ë³¸ê°’ ì ìš©
            max_sources = max_sources if max_sources is not None else self.rag_config.default_max_sources_table
            score_threshold = score_threshold if score_threshold is not None else self.rag_config.score_threshold
            
            # ëª¨ë¸ ë³€ê²½ ì²˜ë¦¬
            if model_name and model_name != self.llm_client.model_name:
                self.logger.info(f"ëª¨ë¸ ë³€ê²½: {self.llm_client.model_name} -> {model_name}")
                llm_config = self.config.model.get('llm')
                model_config = {
                    'name': model_name,
                    'base_url': llm_config.base_url if hasattr(llm_config, 'base_url') else 'http://localhost:11434',
                    'max_tokens': llm_config.max_tokens if hasattr(llm_config, 'max_tokens') else 1000,
                    'temperature': llm_config.temperature if hasattr(llm_config, 'temperature') else 0.1,
                    'top_p': llm_config.top_p if hasattr(llm_config, 'top_p') else 0.9
                }
                self.llm_client = OllamaLLMClient(model_config)
            
            # 1. í‘œ ì œëª©ìœ¼ë¡œ ê²€ìƒ‰ (ë¹„ë™ê¸° ë©”ì„œë“œ ì‚¬ìš©)
            import asyncio
            similar_docs = asyncio.run(
                self.vector_store.search_with_table_filter_async(
                    query=table_title,
                    table_title=table_title,
                    limit=max_sources,
                    score_threshold=score_threshold
                )
            )
            
            if not similar_docs:
                return RAGResponse(
                    answer=f"'{table_title}' í‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    sources=[],
                    confidence=0.0,
                    processing_time=time.time() - start_time,
                    query=f"í‘œ ì œëª©: {table_title}",
                    model_used=self.llm_client.model_name
                )
            
            # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (í† í° ì œí•œ ìë™ ì¡°ì •)
            context = self._build_context(similar_docs, max_tokens=None)
            
            # 3. ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ë‹µë³€ ìƒì„±, ì—†ìœ¼ë©´ í‘œ ë‚´ìš© ìš”ì•½
            if question.strip():
                llm_response = self.llm_client.generate_answer_with_metadata(question, context)
            else:
                llm_response = self.llm_client.generate_answer_with_metadata(
                    f"'{table_title}' í‘œì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.", 
                    context
                )
            
            answer = llm_response.text if llm_response else "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            is_general = llm_response.is_general if llm_response else False
            has_rag_context = llm_response.has_rag_context if llm_response else True
            
            # 4. ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence(similar_docs, answer)
            
            # 5. ì†ŒìŠ¤ ì •ë³´ ì •ë¦¬
            sources = self._format_sources(similar_docs)
            
            processing_time = time.time() - start_time
            
            self.logger.info(f"í‘œ ì œëª© ê²€ìƒ‰ ì™„ë£Œ: {processing_time:.2f}ì´ˆ, ì‹ ë¢°ë„: {confidence:.2f}, ì¼ë°˜ë‹µë³€={is_general}, RAGë‹µë³€={has_rag_context}")
            
            return RAGResponse(
                answer=answer,
                sources=sources,
                confidence=confidence,
                processing_time=processing_time,
                query=f"í‘œ ì œëª©: {table_title}",
                model_used=self.llm_client.model_name,
                is_general_answer=is_general,
                is_rag_answer=has_rag_context
            )
            
        except Exception as e:
            self.logger.error(f"í‘œ ì œëª© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return RAGResponse(
                answer=f"í‘œ ì œëª© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                sources=[],
                confidence=0.0,
                processing_time=time.time() - start_time,
                query=f"í‘œ ì œëª©: {table_title}",
                model_used=""
            )
    
    def query_with_table_filter(self, 
                               question: str, 
                               table_title: Optional[str] = None,
                               is_table_data: Optional[bool] = None,
                               max_sources: Optional[int] = None, 
                               score_threshold: Optional[float] = None, 
                               model_name: Optional[str] = None) -> RAGResponse:
        """í‘œ ê´€ë ¨ í•„í„°ì™€ í•¨ê»˜ ì§ˆì˜ì‘ë‹µ"""
        start_time = time.time()
        
        try:
            self.logger.info(f"í•„í„° ê²€ìƒ‰ ì‹œì‘: {question[:50]}...")
            
            # ê¸°ë³¸ê°’ ì ìš© (í†µì¼ëœ ì„ê³„ê°’ ì‚¬ìš©)
            max_sources = max_sources if max_sources is not None else self.rag_config.default_max_sources_table
            score_threshold = score_threshold if score_threshold is not None else self.rag_config.score_threshold
            
            self.logger.info(f"í•„í„° ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: max_sources={max_sources}, score_threshold={score_threshold:.3f}")
            
            # ëª¨ë¸ ë³€ê²½ ì²˜ë¦¬
            if model_name and model_name != self.llm_client.model_name:
                self.logger.info(f"ëª¨ë¸ ë³€ê²½: {self.llm_client.model_name} -> {model_name}")
                # ì„¤ì •ì—ì„œ LLM ì„¤ì • ê°€ì ¸ì˜¤ê¸°
                llm_config = self.config.model.get('llm')
                model_config = {
                    'name': model_name,
                    'base_url': llm_config.base_url if hasattr(llm_config, 'base_url') else 'http://localhost:11434',
                    'max_tokens': llm_config.max_tokens if hasattr(llm_config, 'max_tokens') else 1000,
                    'temperature': llm_config.temperature if hasattr(llm_config, 'temperature') else 0.1,
                    'top_p': llm_config.top_p if hasattr(llm_config, 'top_p') else 0.9
                }
                self.llm_client = OllamaLLMClient(model_config)
            
            # 1. í•„í„°ì™€ í•¨ê»˜ ê²€ìƒ‰ (ë¹„ë™ê¸° ë©”ì„œë“œ ì‚¬ìš©)
            import asyncio
            similar_docs = asyncio.run(
                self.vector_store.search_with_table_filter_async(
                    query=question,
                    table_title=table_title,
                    is_table_data=is_table_data,
                    limit=max_sources,
                    score_threshold=score_threshold
                )
            )
            
            if not similar_docs:
                filter_info = []
                if table_title:
                    filter_info.append(f"í‘œ ì œëª©: {table_title}")
                if is_table_data is not None:
                    filter_info.append(f"í‘œ ë°ì´í„°: {is_table_data}")
                
                filter_str = ", ".join(filter_info) if filter_info else "í•„í„° ì—†ìŒ"
                return RAGResponse(
                    answer=f"ì¡°ê±´ì— ë§ëŠ” ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í•„í„°: {filter_str})",
                    sources=[],
                    confidence=0.0,
                    processing_time=time.time() - start_time,
                    query=question,
                    model_used=self.llm_client.model_name
                )
            
            # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (í† í° ì œí•œ ìë™ ì¡°ì •)
            context = self._build_context(similar_docs, max_tokens=None)
            
            # 3. LLMì„ í†µí•œ ë‹µë³€ ìƒì„± (ë©”íƒ€ë°ì´í„° í¬í•¨)
            llm_response = self.llm_client.generate_answer_with_metadata(question, context)
            answer = llm_response.text if llm_response else "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            is_general = llm_response.is_general if llm_response else False
            has_rag_context = llm_response.has_rag_context if llm_response else True
            
            # 4. ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence(similar_docs, answer)
            
            # 5. ì†ŒìŠ¤ ì •ë³´ ì •ë¦¬
            sources = self._format_sources(similar_docs)
            
            processing_time = time.time() - start_time
            
            self.logger.info(f"í•„í„° ê²€ìƒ‰ ì™„ë£Œ: {processing_time:.2f}ì´ˆ, ì‹ ë¢°ë„: {confidence:.2f}, ì¼ë°˜ë‹µë³€={is_general}, RAGë‹µë³€={has_rag_context}")
            
            return RAGResponse(
                answer=answer,
                sources=sources,
                confidence=confidence,
                processing_time=processing_time,
                query=question,
                model_used=self.llm_client.model_name,
                is_general_answer=is_general,
                is_rag_answer=has_rag_context
            )
            
        except Exception as e:
            self.logger.error(f"í•„í„° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return RAGResponse(
                answer=f"í•„í„° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                sources=[],
                confidence=0.0,
                processing_time=time.time() - start_time,
                query=question,
                model_used=""
            )
    
    def _merge_search_results_with_rrf(
        self,
        results_list: List[List[Dict[str, Any]]],
        k: int = 60,
        rrf_k: int = 60
    ) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ê²€ìƒ‰ê¸° ê²°ê³¼ë¥¼ RRF (Reciprocal Rank Fusion)ë¡œ í†µí•©
        
        Args:
            results_list: ê²€ìƒ‰ê¸°ë³„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
            k: ë°˜í™˜í•  ìµœì¢… ê²°ê³¼ ìˆ˜
            rrf_k: RRF ì•Œê³ ë¦¬ì¦˜ ìƒìˆ˜
            
        Returns:
            í†µí•©ëœ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not results_list:
            return []
        
        # RRF ì ìˆ˜ ê³„ì‚°: RRF ì ìˆ˜ = Î£ 1 / (k + rank)
        rrf_scores: Dict[str, float] = {}
        chunk_data_map: Dict[str, Dict[str, Any]] = {}
        
        # ê° ê²€ìƒ‰ê¸° ê²°ê³¼ì— ëŒ€í•´ RRF ì ìˆ˜ ê³„ì‚°
        for result_list in results_list:
            if not result_list:
                continue
            
            for rank, result in enumerate(result_list, 1):
                # ì²­í¬ ID ì¶”ì¶œ (ì—¬ëŸ¬ ê°€ëŠ¥í•œ ìœ„ì¹˜ í™•ì¸)
                chunk_id = (
                    result.get('chunk_id') or
                    result.get('metadata', {}).get('chunk_id') or
                    result.get('id', '')
                )
                
                if not chunk_id:
                    # chunk_idê°€ ì—†ìœ¼ë©´ content í•´ì‹œë¡œ ëŒ€ì²´
                    content = result.get('content', result.get('page_content', ''))
                    import hashlib
                    chunk_id = hashlib.md5(content.encode()).hexdigest()
                
                # RRF ì ìˆ˜ ëˆ„ì 
                rrf_score = 1.0 / (rrf_k + rank)
                rrf_scores[chunk_id] = rrf_scores.get(chunk_id, 0.0) + rrf_score
                
                # ì²­í¬ ë°ì´í„° ì €ì¥ (ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ê²°ê³¼ ì‚¬ìš©)
                if chunk_id not in chunk_data_map:
                    chunk_data_map[chunk_id] = result.copy()
                elif rrf_score > rrf_scores.get(chunk_id, 0.0):
                    # ê°™ì€ ì²­í¬ì˜ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë” ë†’ì€ ì ìˆ˜ë©´ ì—…ë°ì´íŠ¸
                    chunk_data_map[chunk_id] = result.copy()
        
        # RRF ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_chunk_ids = sorted(
            rrf_scores.keys(),
            key=lambda x: rrf_scores[x],
            reverse=True
        )
        
        # ìµœì¢… ê²°ê³¼ ìƒì„±
        merged_results = []
        for chunk_id in sorted_chunk_ids[:k]:
            result = chunk_data_map[chunk_id].copy()
            # RRF ì ìˆ˜ë¥¼ ìµœì¢… ì ìˆ˜ë¡œ ì‚¬ìš© (0~1 ë²”ìœ„ë¡œ ì •ê·œí™”)
            max_rrf_score = max(rrf_scores.values()) if rrf_scores else 1.0
            result['score'] = rrf_scores[chunk_id] / max_rrf_score if max_rrf_score > 0 else rrf_scores[chunk_id]
            result['rrf_score'] = rrf_scores[chunk_id]
            merged_results.append(result)
        
        return merged_results


def create_rag_system(config: Optional[Dict[str, Any]] = None) -> RAGSystem:
    """RAG ì‹œìŠ¤í…œ ìƒì„±"""
    return RAGSystem(config)


def setup_rag_system(input_dir: str, config: Optional[Dict[str, Any]] = None) -> bool:
    """RAG ì‹œìŠ¤í…œ ì„¤ì • ë° ë¬¸ì„œ ì²˜ë¦¬"""
    logger = get_logger()
    
    try:
        logger.info(f"RAG ì‹œìŠ¤í…œ ì„¤ì • ì‹œì‘: {input_dir}")
        
        # RAG ì‹œìŠ¤í…œ ìƒì„±
        rag_system = create_rag_system(config)
        logger.info("RAG ì‹œìŠ¤í…œ ìƒì„± ì™„ë£Œ")
        
        # ë²¡í„° ì €ì¥ì†Œ ì„¤ì • (ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
        logger.info("ë²¡í„° ì €ì¥ì†Œ ì»¬ë ‰ì…˜ ì„¤ì • ì¤‘...")
        if not rag_system.vector_store.create_collection(force_recreate=False):
            logger.error("ì»¬ë ‰ì…˜ ì„¤ì • ì‹¤íŒ¨")
            return False
        logger.info("ë²¡í„° ì €ì¥ì†Œ ì»¬ë ‰ì…˜ ì„¤ì • ì™„ë£Œ")
        
        # ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ì¥
        logger.info(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {input_dir}")
        success = rag_system.process_and_store_documents(input_dir)
        
        if success:
            logger.info("RAG ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ")
        else:
            logger.error("ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨")
            
        return success
        
    except Exception as e:
        logger.error(f"RAG ì‹œìŠ¤í…œ ì„¤ì • ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False
